[
  {
    "objectID": "insights.html",
    "href": "insights.html",
    "title": "Insights",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nConvolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForward propagation:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-Nearest Neighbors (KNN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-means with multi-dimensional data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-nearest neighbour\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression python multi-dimensional data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic regression multi-dimensional data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy Practice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machines (SVMs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCS 224N Lecture 3: Word Window Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes theorem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariables and Data types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Measures of Central Tendency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasures of Variability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParametric Tests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZ-Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT-Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChi-square test for Independence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect Size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Ames, Iowa House Price Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPermutation and Combination\n\n\n\n\n\n\n\n\nJul 27, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "insights/StatisticsandProbability/16. Effect Size and Statistical Power.html",
    "href": "insights/StatisticsandProbability/16. Effect Size and Statistical Power.html",
    "title": "Effect Size",
    "section": "",
    "text": "Quantifying the difference between two groups can be achieved by using an effect size. A p-value provides information about statistical significance of the difference between the groups, but it doesn’t give an insight into the magnitude of the difference. Larger sample sizes often result in a higher likelihood of finding a statistically significant difference, even if the real-world effect is small. Hence, it’s crucial to consider effect sizes in addition to p-values, as they provide a clearer picture of the true difference between the groups and are more valuable in practical applications\nThere are different measures for effect sizes. The most common effect sizes are Cohen’s d and Pearson’s r.\nCohen’s d measures the size of the difference between two groups while Pearson’s r measures the strength of the relationship between two variables.\n\nCohen’s d - Standardized Mean Difference\nCohen’s d is designed for comparing two groups. It takes the difference between two means and expresses it in standard deviation units. It tells you how many standard deviations lie between the two means.\n\\[ d =\\frac{ \\overline x_1 - \\overline x_2 }{S}\\]\nwhere \\(\\overline x_1\\) and \\(\\overline x_2\\) are mean of group 1 and group 2 respectively. \\(S\\) is standard deviation.\nThe choice of standard deviation in the equation depends on your research design. We can use: + pooled standard deviation that is based on data from both groups, + standard deviation from a control group. + the standard deviation from the pretest data or posttest.\n\n\nPearson’s r - Correlation Coefficient\nPearson’s \\(r\\), or the correlation coefficient, measures the extent of a linear relationship between two variables.\nThe formula is rather complex, so it’s best to use a statistical software to calculate Pearson’s r accurately from the raw data.\n\\[ r_{xy} = \\frac{n\\sum x_i y_i -\\sum x_i \\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}{\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}}\\]\nThe main idea of the formula is to compute how much of the variability of one variable is determined by the variability of the other variable. Pearson’s r is a standardized scale to measure correlations between variables that makes it unit-free. You can directly compare the strengths of all correlations with each other.\n\n\nInterpreting Values\n\nCohen’s \\(d\\) can take on any number between 0 and infinity, In general the greater the Cohen’s d, the larger the effect size\nPearson’s \\(r\\) ranges between -1 and 1. The closer the value is to 0, the smaller the effect size. A value closer to -1 or 1 indicates a higher effect size.\n\nGeneral Rule of thumb to quantify whether an effect size is small, medium or large:\nCohen’s D:\n\nA d of 0.2 or smaller is considered to be a small effect size.\nA d of 0.5 is considered to be a medium effect size.\nA d of 0.8 or larger is considered to be a large effect size.\n\nPearson Correlation Coefficient:\n\nAn absolute value of r around 0.1 is considered a low effect size.\nAn absolute value of r around 0.3 is considered a medium effect size.\nAn absolute value of r greater than .5 is considered to be a large effect size.\n\n\n\nStatistical Power\nStatistical power, or sensitivity, is the likelihood of a significance test detecting an effect when there actually is one. In other words, power is the probability that we will correctly reject the null hypothesis.\nLet’s look at an example to understand this concept. Suppose we have two distributions with minimal overlap, as shown in the first picture below. If we collect a small set of samples from both the green and red distributions and compare their means using hypothesis testing, we might get a small p-value, say 0.0004. This would cause us to correctly reject the null hypothesis that both sample sets came from the same distribution. In other words, if the blue distribution says that all data points came from it, we would reject that hypothesis.\nIf we keep repeating this experiment multiple times, there’s a high probability that each statistical test will correctly give us a small p-value. In other words, there is a high probability that the null hypothesis that all the data came from the same distribution will be correctly rejected.\nHowever, occasionally, we might get a trial like in the second picture below, where the two sample sets appear to come from the same distribution due to overlapping sample points, resulting in a high p-value, like 0.08. This means that even though we know that the data came from two different distributions, we cannot correctly reject the null hypothesis that all the data came from the same distribution. Since these two distributions are far apart and have very little overlap, the probability of correctly rejecting the null hypothesis is high. Thus, power, being the probability that we will correctly reject the null hypothesis, is high in this example.\nIn summary, when distributions have minimal overlap, the statistical power is high, meaning there is a high likelihood of correctly rejecting the null hypothesis.\n\n\n\n\n\n\n\n\nStatistical Power: Overlapping Distributions\nNow, let’s consider a different scenario where we have a large overlap in the distributions, as shown in the first picture below. Most of the time, when we compare the means of these two distributions, we get a high p-value and fail to reject the null hypothesis that the data comes from the same distribution.\nHowever, occasionally, when the sample data points are from the far extremes of the distributions, as shown in the second picture below, we get a small p-value and can correctly reject the null hypothesis that the data comes from the same distribution. Due to the overlap, the probability of correctly rejecting the null hypothesis is low, meaning we have relatively low power.\nThe good news is that we can increase the power by increasing the number of samples we collect. Power analysis will tell us how many measurements we need to collect to achieve a good amount of power.\nIn summary, when distributions have a large overlap, the statistical power is low, meaning there is a low likelihood of correctly rejecting the null hypothesis. By increasing the sample size, we can improve the power of our test.\n\n\n\n\n\n\nBefore we learn how to do power analysis. Lets understand why do we need to perform power analysis in detail.\n\nNeed for Power Analysis\nIn hypothesis testing, we start with a null hypothesis of no effect and an alternative hypothesis of a true effect. The goal is to collect enough data from a sample to statistically test whether we can reasonably reject the null hypothesis in favor of the alternative hypothesis. In doing so, there’s always a risk of making one of two decision errors when interpreting study results:\n\nType I error: Rejecting the null hypothesis of no effect when it is actually true.\nType II error: Not rejecting the null hypothesis of no effect when it is actually false.\n\nPower is the probability of avoiding a Type II error. The higher the statistical power of a test, the lower the risk of making a Type II error. Power is usually set at 80%. This means that if there are true effects to be found in 100 different studies with 80% power, only 80 out of 100 statistical tests will actually detect them. If we don’t ensure sufficient power, our study may not be able to detect a true effect at all. This means that resources like time and money are wasted, and it may even be unethical to collect data from participants.\nOn the flip side, too much power means our tests are highly sensitive to true effects, including very small ones. This may lead to finding statistically significant results with very little usefulness in the real world. To balance these pros and cons of low versus high statistical power, we should use a Power Analysis to set an appropriate level.\n\n\n\nPower Analysis\nPower is mainly influenced by sample size, effect size, and significance level. A power analysis can be used to determine the necessary sample size for a study. Having enough statistical power is necessary to draw accurate conclusions about a population using sample data.\nPower is affected by several factors, but two main factors are:\n\nOverlap: How much overlap is there between the two distributions we want to identify with our study.\nSample Size: The number of samples we collect from each group.\n\nIf we want Power to be 80% and if there is very little overlap, a small sample size will suffice. However, if the overlap is greater between the two distributions, we need a larger sample size to achieve 80% power.\nTo understand the relationship between overlap and sample size, we need to realize that when we do a statistical test, we usually compare sample means rather than individual measurements. So let’s see what happens when we calculate means with different sample sizes.\n\nIf the sample size is small, there is a lot of variation in estimated means for a distribution, making it hard to be confident that any single estimated mean is a good estimate of the population mean, and there is overlap between the estimated means of the two distributions.\nBut if the sample size is large, the estimated means are so close to the population mean that they no longer overlap. This suggests a high probability that we correctly reject the null hypothesis that both samples came from the same distribution. With a large sample size, we can achieve high power. Additionally, the central limit theorem states that these results apply to any type of distribution.\n\nA power analysis consists of four main components. If you know or have estimates for any three of these, you can calculate the fourth component:\n\nStatistical Power: The likelihood that a test will detect an effect of a certain size if there is one, usually set at 80% or higher.\nSample Size: The minimum number of observations needed to observe an effect of a certain size with a given power level.\nSignificance Level (alpha): The maximum risk of rejecting a true null hypothesis that you are willing to take, usually set at 5%.\nExpected Effect Size: The combined effect of standard deviation and means of two distributions due to overlap, captured by Effect size (d). There are many different ways to capture the effect.\n\nBefore starting a study, we can use a power analysis to calculate the minimum sample size for a desired power level and significance level, along with an expected effect size. Traditionally, the significance level is set to 5% and the desired power level to 80%. That means we only need to figure out an expected effect size to calculate a sample size from a power analysis.\nThe stats.power module of the statsmodels package in Python contains the required functions for carrying out power analysis for the most commonly used statistical tests such as t-test, normal-based test, F-tests, and Chi-square goodness-of-fit test. Its solve_power function takes three of the four components mentioned above as input parameters and calculates the sample size."
  },
  {
    "objectID": "insights/StatisticsandProbability/14. ANOVA - Analysis of Variance.html",
    "href": "insights/StatisticsandProbability/14. ANOVA - Analysis of Variance.html",
    "title": "ANOVA",
    "section": "",
    "text": "ANOVA (Analysis of Variance) is a statistical method used for comparing the means of multiple populations. Previously, we have considered only a single population or at most two populations. A one-way ANOVA uses one independent variable, while a two-way ANOVA uses two independent variables. The statistical distribution used in ANOVA is the F-distribution, whose characteristics are as follows:\n\nThe F-distribution has a single tail (toward the right) and contains only positive values.\n\n\n\nThe F-statistic, which is the critical statistic in ANOVA, is the ratio of variation between the sample means to the variation within the samples. The formula is as follows: \\[F = \\frac{\\text{variation between sample means}}{\\text{variation within the samples}}\\]\nThe different populations are referred to as treatments.\nA high value of the F-statistic implies that the variation between samples is considerable compared to the variation within the samples. In other words, the populations or treatments from which the samples are drawn are actually different from one another.\nRandom variations between treatments are more likely to occur when the variation within the sample is considerable.\n\nUse a one-way ANOVA when you have collected data about one categorical independent variable and one quantitative dependent variable. The independent variable should have at least three levels (i.e., at least three different groups or categories).\nANOVA tells you if the dependent variable changes according to the level of the independent variable. For example:\n\nYour independent variable is social media use, and you assign groups to low, medium, and high levels of social media use to find out if there is a difference in hours of sleep per night.\nYour independent variable is the brand of soda, and you collect data on Coke, Pepsi, Sprite, and Fanta to find out if there is a difference in the price per 100ml.\n\nANOVA determines whether the groups created by the levels of the independent variable are statistically different by calculating whether the means of the treatment levels are different from the overall mean of the dependent variable. If any of the group means is significantly different from the overall mean, then the null hypothesis is rejected.\nANOVA uses the F-test for statistical significance. This allows for the comparison of multiple means at once, as the error is calculated for the whole set of comparisons rather than for each individual two-way comparison (which would happen with a t-test).\nThe F-test compares the variance in each group mean from the overall group variance. If the variance within groups is smaller than the variance between groups, the F-test will find a higher F-value, and therefore a higher likelihood that the difference observed is real and not due to chance.\nThe assumptions of the ANOVA test are the same as the general assumptions for any parametric test:\n\nIndependence of observations: The data were collected using statistically valid methods, and there are no hidden relationships among observations. If your data fail to meet this assumption because you have a confounding variable that you need to control for statistically, use an ANOVA with blocking variables.\nNormally distributed response variable: The values of the dependent variable follow a normal distribution.\nHomogeneity of variance: The variation within each group being compared is similar for every group. If the variances are different among the groups, then ANOVA probably isn’t the right fit for the data.\n\n\n\nA few agricultural research scientists have planted a new variety of cotton called “AB cotton.” They have used three different fertilizers – A, B, and C – for three separate plots of this variety. The researchers want to find out if the yield varies with the type of fertilizer used. Yields in bushels per acre are mentioned in the below table. Conduct an ANOVA test at a 5% level of significance to see if the researchers can conclude that there is a difference in yields.\n\n\n\nFertilizer A\nFertilizer b\nFertilizer c\n\n\n\n\n40\n45\n55\n\n\n30\n35\n40\n\n\n35\n55\n30\n\n\n45\n25\n20\n\n\n\nNull hypothesis: \\(H_0 : \\mu_1 = \\mu_2 = \\mu_3\\)\nAlternative hypothesis: \\(H_1 : \\mu_1 ! = \\mu_2 ! = \\mu_3\\)\nthe level of significance: \\(\\alpha\\)=0.05\n\nimport scipy.stats as stats\n\na=[40,30,35,45]\nb=[45,35,55,25]\nc=[55,40,30,20]\n\nstats.f_oneway(a,b,c)\n\nF_onewayResult(statistic=0.10144927536231883, pvalue=0.9045455407589628)\n\n\nSince the calculated p-value (0.904)&gt;0.05, we fail to reject the null hypothesis.There is no significant difference between the three treatments, at a 5% significance level.\n\n\n\nA botanist wants to know whether or not plant growth is influenced by sunlight exposure and watering frequency. She plants 30 seeds and lets them grow for two months under different conditions for sunlight exposure and watering frequency. After two months, she records the height of each plant, in inches.\n\nimport numpy as np\nimport pandas as pd\n\n#create data\ndf = pd.DataFrame({'water': np.repeat(['daily', 'weekly'], 15),\n                   'sun': np.tile(np.repeat(['low', 'med', 'high'], 5), 2),\n                   'height': [6, 6, 6, 5, 6, 5, 5, 6, 4, 5,\n                              6, 6, 7, 8, 7, 3, 4, 4, 4, 5,\n                              4, 4, 4, 4, 4, 5, 6, 6, 7, 8]})\n\n\ndf[:10]\n\n\n\n\n\n\n\n\nwater\nsun\nheight\n\n\n\n\n0\ndaily\nlow\n6\n\n\n1\ndaily\nlow\n6\n\n\n2\ndaily\nlow\n6\n\n\n3\ndaily\nlow\n5\n\n\n4\ndaily\nlow\n6\n\n\n5\ndaily\nmed\n5\n\n\n6\ndaily\nmed\n5\n\n\n7\ndaily\nmed\n6\n\n\n8\ndaily\nmed\n4\n\n\n9\ndaily\nmed\n5\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n#perform two-way ANOVA\nmodel = ols('height ~ C(water) + C(sun) + C(water):C(sun)', data=df).fit()\nsm.stats.anova_lm(model, typ=2)\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nC(water)\n8.533333\n1.0\n16.0000\n0.000527\n\n\nC(sun)\n24.866667\n2.0\n23.3125\n0.000002\n\n\nC(water):C(sun)\n2.466667\n2.0\n2.3125\n0.120667\n\n\nResidual\n12.800000\n24.0\nNaN\nNaN"
  },
  {
    "objectID": "insights/StatisticsandProbability/14. ANOVA - Analysis of Variance.html#one-way-anova",
    "href": "insights/StatisticsandProbability/14. ANOVA - Analysis of Variance.html#one-way-anova",
    "title": "ANOVA",
    "section": "",
    "text": "A few agricultural research scientists have planted a new variety of cotton called “AB cotton.” They have used three different fertilizers – A, B, and C – for three separate plots of this variety. The researchers want to find out if the yield varies with the type of fertilizer used. Yields in bushels per acre are mentioned in the below table. Conduct an ANOVA test at a 5% level of significance to see if the researchers can conclude that there is a difference in yields.\n\n\n\nFertilizer A\nFertilizer b\nFertilizer c\n\n\n\n\n40\n45\n55\n\n\n30\n35\n40\n\n\n35\n55\n30\n\n\n45\n25\n20\n\n\n\nNull hypothesis: \\(H_0 : \\mu_1 = \\mu_2 = \\mu_3\\)\nAlternative hypothesis: \\(H_1 : \\mu_1 ! = \\mu_2 ! = \\mu_3\\)\nthe level of significance: \\(\\alpha\\)=0.05\n\nimport scipy.stats as stats\n\na=[40,30,35,45]\nb=[45,35,55,25]\nc=[55,40,30,20]\n\nstats.f_oneway(a,b,c)\n\nF_onewayResult(statistic=0.10144927536231883, pvalue=0.9045455407589628)\n\n\nSince the calculated p-value (0.904)&gt;0.05, we fail to reject the null hypothesis.There is no significant difference between the three treatments, at a 5% significance level."
  },
  {
    "objectID": "insights/StatisticsandProbability/14. ANOVA - Analysis of Variance.html#two-way-anova",
    "href": "insights/StatisticsandProbability/14. ANOVA - Analysis of Variance.html#two-way-anova",
    "title": "ANOVA",
    "section": "",
    "text": "A botanist wants to know whether or not plant growth is influenced by sunlight exposure and watering frequency. She plants 30 seeds and lets them grow for two months under different conditions for sunlight exposure and watering frequency. After two months, she records the height of each plant, in inches.\n\nimport numpy as np\nimport pandas as pd\n\n#create data\ndf = pd.DataFrame({'water': np.repeat(['daily', 'weekly'], 15),\n                   'sun': np.tile(np.repeat(['low', 'med', 'high'], 5), 2),\n                   'height': [6, 6, 6, 5, 6, 5, 5, 6, 4, 5,\n                              6, 6, 7, 8, 7, 3, 4, 4, 4, 5,\n                              4, 4, 4, 4, 4, 5, 6, 6, 7, 8]})\n\n\ndf[:10]\n\n\n\n\n\n\n\n\nwater\nsun\nheight\n\n\n\n\n0\ndaily\nlow\n6\n\n\n1\ndaily\nlow\n6\n\n\n2\ndaily\nlow\n6\n\n\n3\ndaily\nlow\n5\n\n\n4\ndaily\nlow\n6\n\n\n5\ndaily\nmed\n5\n\n\n6\ndaily\nmed\n5\n\n\n7\ndaily\nmed\n6\n\n\n8\ndaily\nmed\n4\n\n\n9\ndaily\nmed\n5\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n#perform two-way ANOVA\nmodel = ols('height ~ C(water) + C(sun) + C(water):C(sun)', data=df).fit()\nsm.stats.anova_lm(model, typ=2)\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nC(water)\n8.533333\n1.0\n16.0000\n0.000527\n\n\nC(sun)\n24.866667\n2.0\n23.3125\n0.000002\n\n\nC(water):C(sun)\n2.466667\n2.0\n2.3125\n0.120667\n\n\nResidual\n12.800000\n24.0\nNaN\nNaN"
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html",
    "href": "insights/StatisticsandProbability/12. Z-test.html",
    "title": "Z-Test",
    "section": "",
    "text": "A one-sample Z-test is utilized to evaluate if the mean of a single sample differs from a known or hypothesized population mean. Several criteria must be fulfilled for a one-sample Z-test:\n\nThe population from which the sample is drawn follows a normal distribution.\nThe sample size exceeds 30.\nOnly one sample is obtained.\nThe hypothesis concerns the population mean.\nThe population standard deviation is known.\n\nThe test statistic is computed using the formula:\n\\[ z = \\frac {(\\overline x - \\mu)}{\\frac{\\sigma}{\\sqrt n}}\\]\nwhere \\(x\\) denotes the sample mean, \\(\\mu\\) represents the population mean, \\(\\sigma\\) stands for the population standard deviation, and \\(n\\) is the sample size."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#one-sample-z-test",
    "href": "insights/StatisticsandProbability/12. Z-test.html#one-sample-z-test",
    "title": "Z-Test",
    "section": "",
    "text": "A one-sample Z-test is utilized to evaluate if the mean of a single sample differs from a known or hypothesized population mean. Several criteria must be fulfilled for a one-sample Z-test:\n\nThe population from which the sample is drawn follows a normal distribution.\nThe sample size exceeds 30.\nOnly one sample is obtained.\nThe hypothesis concerns the population mean.\nThe population standard deviation is known.\n\nThe test statistic is computed using the formula:\n\\[ z = \\frac {(\\overline x - \\mu)}{\\frac{\\sigma}{\\sqrt n}}\\]\nwhere \\(x\\) denotes the sample mean, \\(\\mu\\) represents the population mean, \\(\\sigma\\) stands for the population standard deviation, and \\(n\\) is the sample size."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#one-sample-z-test-one-tail",
    "href": "insights/StatisticsandProbability/12. Z-test.html#one-sample-z-test-one-tail",
    "title": "Z-Test",
    "section": "One-Sample Z-Test: One-Tail",
    "text": "One-Sample Z-Test: One-Tail\nSuppose we have a pizza delivery shop with a historical average delivery time of 45 minutes and a standard deviation of 5 minutes. However, due to recent customer complaints, the shop decides to analyze the delivery time of the last 40 orders, revealing an average delivery time of 48 minutes. We aim to ascertain if the new mean significantly exceeds the population mean.\nThe null hypothesis (\\(H_0\\)) posits that the mean delivery time equals 45 minutes: \\(\\mu = 45\\). The alternative hypothesis (\\(H_1\\)) suggests that the mean delivery time surpasses 45 minutes: \\(\\mu &gt; 45\\). Let’s adopt a significance level of \\(\\alpha = 0.05\\). In this scenario, the region of rejection will be situated on the right tail.\n\nz = (48-45)/(5/(40)**0.5)\nprint(z)\n\n3.7947331922020555\n\n\n\nimport scipy.stats as stats\np_value = 1 - stats.norm.cdf(z) # cumulative distribution function\nprint(p_value)\n\n7.390115516725526e-05\n\n\nSince the p-value is less than \\(\\alpha\\), we reject the null hypothesis. There is a significant difference, at a level of 0.05, between the average delivery time of the sample and the historical population average."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#one-sample-z-test-two-tail",
    "href": "insights/StatisticsandProbability/12. Z-test.html#one-sample-z-test-two-tail",
    "title": "Z-Test",
    "section": "One-Sample Z-Test: Two-Tail",
    "text": "One-Sample Z-Test: Two-Tail\nSuppose we aim to investigate whether a drug has an impact on IQ. In this scenario, we opt for a two-tail test because we’re interested in determining whether the drug affects IQ, regardless of whether it has a positive or negative effect.\nGiven a significance level of \\(\\alpha = 0.05\\), our rejection regions are 0.025 on both the right and left tails.\nAssuming our population mean \\(\\mu = 100\\) and population standard deviation \\(\\sigma = 15\\), we conduct a study involving a sample of 100 subjects. Upon analysis, we discover that the mean IQ of the sample is 96.\n\nz = (100-96)/(15/(100**0.5))\nprint(\"statistic: \", round(z, 4))\n\nstatistic:  2.6667\n\n\n\nimport scipy.stats as stats\ncritical = stats.norm.ppf(1-0.025) # cumulative distribution function\nprint(\"Critical:\", round(critical, 4))\n\nCritical: 1.96\n\n\nSince our test statistic is greater than the critical statistic, we conclude that our drug has a significant influence on IQ values at a criterion level of \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#two-sample-z-test",
    "href": "insights/StatisticsandProbability/12. Z-test.html#two-sample-z-test",
    "title": "Z-Test",
    "section": "Two-Sample Z-Test",
    "text": "Two-Sample Z-Test\nA two-sample z-test is similar to a one-sample z-test, with the main differences being:\n\nThere are two groups/populations under consideration, and we draw one sample from each population.\nBoth population distributions are assumed to be normal.\nBoth population standard deviations are known.\nThe formula for calculating the test statistic is:\n\n\\[z = \\frac{\\overline{x}_1 - \\overline{x}_2} {\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\]\nAn organization manufactures LED bulbs in two production units, A and B. The quality control team believes that the quality of production at unit A is better than that of B. Quality is measured by how long a bulb works. The team takes samples from both units to test this. The mean life of LED bulbs at units A and B are 1001.3 and 810.47, respectively. The sample sizes are 40 and 44. The population variances are known: \\(\\sigma_A^2 = 48127\\) and \\(\\sigma_B^2 = 59173\\).\nConduct the appropriate test, at a 5% significance level, to verify the claim of the quality control team.\nNull hypothesis: \\(H_0: \\mu_A ≤ \\mu_B\\)\nAlternate hypothesis: \\(H_1: \\mu_A &gt; \\mu_B\\)\nLet’s fix the level of significance at \\(\\alpha = 0.05\\).\n\nz = (1001.34-810.47)/(48127/40+59173/44)**0.5\nprint(z)\n\n3.781260568723408\n\n\n\nimport scipy.stats as stats\np_value = 1 - stats.norm.cdf(z)\np_value\n\n7.801812433294586e-05\n\n\np-value (0.000078)&lt;\\(\\alpha\\)(0.05), we reject the null hypothesis. The LED bulbs produced at unit A have a significantly longer life than those at unit B, at a 5% level."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#hypothesis-tests-with-proportions",
    "href": "insights/StatisticsandProbability/12. Z-test.html#hypothesis-tests-with-proportions",
    "title": "Z-Test",
    "section": "Hypothesis Tests with Proportions",
    "text": "Hypothesis Tests with Proportions\nProportion tests are utilized with nominal data and are effective for comparing percentages or proportions. For instance, a survey collecting responses from a department in an organization might claim that 85% of people in the organization are satisfied with its policies. Historically, the satisfaction rate has been 82%. Here, we compare a percentage or proportion taken from the sample with a percentage/proportion from the population. The following are some characteristics of the sampling distribution of proportions:\n\nThe sampling distribution of the proportions taken from the sample is approximately normal.\nThe mean of this sampling distribution (\\(\\overline{p}\\)) equals the population proportion (\\(p\\)).\nCalculating the test statistic: The following equation gives the \\(z\\)-value:\n\n\\[ z = \\frac{\\overline{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\]\nWhere \\(\\overline{p}\\) is the sample proportion, \\(p\\) is the population proportion, and \\(n\\) is the sample size."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#one-sample-proportion-z-test",
    "href": "insights/StatisticsandProbability/12. Z-test.html#one-sample-proportion-z-test",
    "title": "Z-Test",
    "section": "One-Sample Proportion Z-Test",
    "text": "One-Sample Proportion Z-Test\nIt is known that 40% of the total customers are satisfied with the services provided by a mobile service center. The customer service department of this center decides to conduct a survey for assessing the current customer satisfaction rate. It surveys 100 of its customers and finds that only 30 out of the 100 customers are satisfied with its services. Conduct a hypothesis test at a 5% significance level to determine if the percentage of satisfied customers has reduced from the initial satisfaction level (40%).\nNull Hypothesis: \\(H_0: p = 0.4\\)\nAlternate Hypothesis: \\(H_1: p &lt; 0.4\\)\nThe &lt; sign indicates a lower-tail test.\nLet’s fix the level of significance at \\(\\alpha = 0.05\\).\n\nz=(0.3-0.4)/((0.4)*(1-0.4)/100)**0.5\nz\n\n-2.041241452319316\n\n\n\nimport scipy.stats as stats\n\np=stats.norm.cdf(z)\np\n\n0.02061341666858179\n\n\np-value (0.02) &lt; 0.05. We reject the null hypothesis. At a 5% significance level, the percentage of customers satisfied with the service center’s services has reduced."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#two-sample-proportion-z-test",
    "href": "insights/StatisticsandProbability/12. Z-test.html#two-sample-proportion-z-test",
    "title": "Z-Test",
    "section": "Two-Sample Proportion Z-Test",
    "text": "Two-Sample Proportion Z-Test\nHere, we compare proportions taken from two independent samples belonging to two different populations. The following equation gives the formula for the critical test statistic:\n\\[ z = \\frac {(\\overline{p}_1 - \\overline{p}_2)}{\\sqrt{\\frac{p_c(1-p_c)}{N_1} + \\frac{p_c(1-p_c)}{N_2}}}\\]\nIn the preceding formula, \\(\\overline{p}_1\\) is the proportion from the first sample, and \\(\\overline{p}_2\\) is the proportion from the second sample. \\(N_1\\) is the sample size of the first sample, and \\(N_2\\) is the sample size of the second sample. \\(p_c\\) is the pooled variance.\n\\[\\overline{p}_1 = \\frac{x_1}{N_1} ;  \\overline{p}_2 = \\frac {x_2}{N_2} ;  p_c = \\frac {x_1 + x_2}{N_1 + N_2}\\]\nIn the preceding formula, \\(x_1\\) is the number of successes in the first sample, and \\(x_2\\) is the number of successes in the second sample."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#investigation-of-passenger-compliance-with-child-safety-guidelines",
    "href": "insights/StatisticsandProbability/12. Z-test.html#investigation-of-passenger-compliance-with-child-safety-guidelines",
    "title": "Z-Test",
    "section": "Investigation of Passenger Compliance with Child Safety Guidelines",
    "text": "Investigation of Passenger Compliance with Child Safety Guidelines\nA ride-sharing company is investigating complaints by its drivers regarding passenger compliance with child safety guidelines, specifically concerning the use of child seats and seat belts. Surveys were independently conducted in two major cities, A and B, to gather data on passenger compliance. The company aims to determine if there is a difference in the proportion of passengers conforming to child safety guidelines between the two cities. The data for the two cities is summarized in the following table:\n\n\n\n\nCity A\nCity B\n\n\n\n\nTotal surveyed\n200\n230\n\n\nNo. of complaints\n110\n106\n\n\n\nThe law enforcement authority seeks to evaluate if the proportion of compliant passengers differs significantly between City A and City B."
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#hypotheses-for-two-sample-proportion-test",
    "href": "insights/StatisticsandProbability/12. Z-test.html#hypotheses-for-two-sample-proportion-test",
    "title": "Z-Test",
    "section": "Hypotheses for Two-Sample Proportion Test",
    "text": "Hypotheses for Two-Sample Proportion Test\nFor the two-sample proportion test comparing compliance rates between City A and City B:\n\nNull hypothesis: \\(H_0: p_A = p_B\\)\nAlternative hypothesis: \\(H_1: p_A \\neq p_B\\)\n\nThis constitutes a two-tail test because the region of rejection could be located on either side.\nThe significance level \\(\\alpha\\) is set at 0.05, resulting in an area of 0.025 on both sides.\n\nx1,n1,x2,n2=110,200,106,230\np1=x1/n1\np2=x2/n2\npc=(x1+x2)/(n1+n2)\nz_statistic=(p1-p2)/(((pc*(1-pc)/n1)+(pc*(1-pc)/n2))**0.5)\nz_statistic\n\n1.8437643201697864\n\n\n\ncritical = stats.norm.ppf(1-0.025)\ncritical\n\n1.959963984540054\n\n\n\np_value =2*(1-stats.norm.cdf(z))\np_value\n\n1.9587731666628365"
  },
  {
    "objectID": "insights/StatisticsandProbability/12. Z-test.html#conclusion-of-two-sample-proportion-test",
    "href": "insights/StatisticsandProbability/12. Z-test.html#conclusion-of-two-sample-proportion-test",
    "title": "Z-Test",
    "section": "Conclusion of Two-Sample Proportion Test",
    "text": "Conclusion of Two-Sample Proportion Test\nBased on the statistical analysis:\n\nSince the test statistic is less than the critical value, or the p-value is greater than 0.05, we fail to reject the null hypothesis.\nTherefore, there is no significant difference between the proportion of passengers in these cities complying with child safety norms, at a 5% significance level."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Before delving into the intricacies of hypothesis testing, it’s imperative to grasp some foundational concepts."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html#population-parameters-vs.-sample-statistics",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html#population-parameters-vs.-sample-statistics",
    "title": "Hypothesis Testing",
    "section": "Population Parameters vs. Sample Statistics",
    "text": "Population Parameters vs. Sample Statistics\nA Parameter denotes a characteristic of an entire population, such as the population mean. As it’s typically impractical to measure the entire population, the true value of the parameter often eludes us. Commonly used parameters in statistics, like the population mean and standard deviation, are symbolized by Greek letters such as \\(\\mu\\) (mu) and \\(\\sigma\\) (sigma).\nConversely, a Statistic represents a characteristic calculated from a sample. For instance, computing the mean and standard deviation of a sample yields sample statistics. In statistical parlance, sample parameters are denoted by Latin letters.\nInferential statistics entails leveraging sample statistics to draw inferences about a population. This involves using sample statistics to estimate population parameters. To ensure validity, representative sampling techniques like random sampling are pivotal for obtaining unbiased estimates. Unbiased estimates are considered accurate on average, whereas biased estimates systematically deviate from the truth."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html#parametric-vs-nonparametric-analysis",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html#parametric-vs-nonparametric-analysis",
    "title": "Hypothesis Testing",
    "section": "Parametric vs Nonparametric Analysis",
    "text": "Parametric vs Nonparametric Analysis\nParametric statistics assumes that the sample data stems from populations describable by probability distributions with fixed parameters. Consequently, parametric analysis reigns as the predominant statistical method.\nIn contrast, nonparametric tests refrain from assuming any specific probability distribution for the underlying data."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html#significance-level-alpha",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html#significance-level-alpha",
    "title": "Hypothesis Testing",
    "section": "Significance Level (Alpha)",
    "text": "Significance Level (Alpha)\nThe significance level serves as a yardstick dictating the requisite strength of evidence from sample data to infer the presence of an effect in the population. Also known as alpha (\\(\\alpha\\)), it’s a predetermined threshold established prior to the study. The significance level delineates the evidence threshold needed to reject the null hypothesis in favor of the alternative hypothesis.\nReflecting the probability of erroneously rejecting the null hypothesis when true, the significance level quantifies the risk of asserting an effect’s existence when none prevails. Lower significance levels signify heightened evidentiary thresholds, demanding more robust evidence before null hypothesis rejection. For instance, a significance level of 0.05 implies a 5% chance of committing a false positive error—declaring an effect’s existence in its absence."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html#p-values",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html#p-values",
    "title": "Hypothesis Testing",
    "section": "P-Values",
    "text": "P-Values\nP-values gauge the strength of evidence against the null hypothesis furnished by sample data. A P-value below the established significance level denotes statistical significance.\nThe P-value represents the probability of observing an effect in the sample data as extreme, or even more so, than the one observed if the null hypothesis held true. Essentially, it quantifies the extent to which the sample data contravenes the null hypothesis. Lower P-values denote more compelling evidence against the null hypothesis.\nWhen a P-value falls at or below the significance level, the null hypothesis is discarded, and the results are deemed statistically significant. This implies that the sample data furnishes adequate evidence to endorse the alternative hypothesis positing the effect’s presence in the population.\nConversely, when a P-value exceeds the significance level, the sample data fails to supply sufficient evidence for effect existence, prompting null hypothesis retention.\nStatistically, these verdicts translate as follows:\n\nReject the null hypothesis when the P-value equals or falls below the significance level.\nRetain the null hypothesis when the P-value exceeds the significance level."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html#hypothesis-testing",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html#hypothesis-testing",
    "title": "Hypothesis Testing",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nHypothesis testing is a statistical technique that evaluates the evidence of two opposing statements (hypotheses) about a population based on sample data. These hypotheses are known as the null hypothesis and the alternative hypothesis.\nThe objective of hypothesis testing is to assess the sample statistic and its corresponding sampling error to determine which of the two hypotheses is more strongly supported by the data. If the null hypothesis can be rejected, it means that the results are statistically significant and the alternative hypothesis is favored, suggesting that an effect exists in the population.\nIt is important to note that failing to reject the null hypothesis does not necessarily mean that the null hypothesis is true, nor does rejecting the null hypothesis necessarily imply that the alternative hypothesis is true. The results of a hypothesis test are only a suggestion or indication about the population, not a conclusive proof of either hypothesis.\nThe null hypothesis is the theory that there is no effect (i.e., the effect size is equal to zero). It is commonly represented by \\(H_0\\).\nThe alternative hypothesis is the opposite theory, stating that the population parameter does not equal the value specified in the null hypothesis (i.e., there is a non-zero effect). It is usually represented by \\(H_1\\) or \\(H_A\\).\nThe steps involved in hypothesis testing are as follows:\n\nState the null and alternative hypothesis.\nSpecify the significance level and calculate the critical value of the test statistic.\nChoose the appropriate test based on factors such as the number of samples, population distribution, statistic being tested, sample size, and knowledge of the population standard deviation.\nCalculate the relevant test statistic (z-statistic, t-statistic, chi-square statistic, or f-statistic) or p-value.\nCompare the calculated test statistic with the critical test statistic or the p-value with the significance level.\n\nIf using the test statistic:\n\nReject the null hypothesis if the calculated test statistic is greater than the critical test statistic (upper-tail test)\nReject the null hypothesis if the calculated test statistic is less than the critical test statistic (lower-tail test)\n\nIf using the p-value:\n\nReject the null hypothesis if the p-value is less than the significance level.\n\n\nDraw a conclusion based on the comparison made in step 5."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html#confidence-interval",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html#confidence-interval",
    "title": "Hypothesis Testing",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nA confidence interval can be calculated for various parameters such as population mean, population proportion, difference of population means, and difference of population proportions, among others. To construct a confidence interval, one needs to have a sample statistic that estimates the population parameter of interest and a measure of variability or standard error for that statistic. The confidence interval is calculated by adding and subtracting the standard error to the sample statistic. The result is a range of values that contains the true population parameter with a specified level of confidence.\nThe key concept behind a confidence interval is that if we repeated our sampling process many times, the true population parameter would fall within the confidence interval for the specified percentage of these samples. For example, if we have a 95% confidence interval for a population mean, we can say that if we repeated our sampling process 100 times, the true population mean would fall within the calculated confidence interval 95 times out of 100.\nIn conclusion, confidence intervals provide a range of plausible values for a population parameter based on the observed sample data and the level of confidence specified by the researcher. The confidence interval reflects the precision of the estimate, with wider intervals indicating less precision and narrower intervals indicating more precision."
  },
  {
    "objectID": "insights/StatisticsandProbability/10. Hypothesis Testing.html#sampling-error",
    "href": "insights/StatisticsandProbability/10. Hypothesis Testing.html#sampling-error",
    "title": "Hypothesis Testing",
    "section": "Sampling Error",
    "text": "Sampling Error\nA sampling error refers to the discrepancy between a population parameter and a sample statistic. In a study, the sampling error represents the difference between the mean calculated from a sample and the actual mean of the population. Despite using a random sample selection process, sampling errors are still a possibility as the sample may not perfectly reflect the population with regards to numerical values such as means and standard deviations. To improve the accuracy of generalizing findings from a sample to a population, it’s essential to minimize the sampling error. One way to do this is to increase the sample size."
  },
  {
    "objectID": "insights/StatisticsandProbability/08.  Central Limit Theorem.html",
    "href": "insights/StatisticsandProbability/08.  Central Limit Theorem.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "The Central Limit Theorem (CLT) posits that the distribution of sample means drawn from a population will approximate a normal distribution, irrespective of the shape of the population distribution, provided the sample size is sufficiently large (typically n &gt; 30). Even for populations that are already normally distributed, the theorem remains valid for smaller sample sizes."
  },
  {
    "objectID": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#the-central-limit-theorem",
    "href": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#the-central-limit-theorem",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "The Central Limit Theorem (CLT) posits that the distribution of sample means drawn from a population will approximate a normal distribution, irrespective of the shape of the population distribution, provided the sample size is sufficiently large (typically n &gt; 30). Even for populations that are already normally distributed, the theorem remains valid for smaller sample sizes."
  },
  {
    "objectID": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#estimating-the-population-mean",
    "href": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#estimating-the-population-mean",
    "title": "Central Limit Theorem",
    "section": "Estimating the Population Mean",
    "text": "Estimating the Population Mean\nWhile the sample mean serves as an estimate for the population mean, it’s essential to recognize that the standard deviation of the sampling distribution (\\(\\sigma_{\\overline{x}}\\)) differs from the population standard deviation (\\(\\sigma\\))."
  },
  {
    "objectID": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#the-standard-error",
    "href": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#the-standard-error",
    "title": "Central Limit Theorem",
    "section": "The Standard Error",
    "text": "The Standard Error\nThe standard deviation of the sampling distribution (\\(\\sigma_{\\overline{x}}\\)) is termed the standard error and is linked to the population standard deviation by the formula:\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nHere, \\(\\sigma\\) represents the population standard deviation, and \\(n\\) denotes the sample size.\nAs the sample size increases, the standard error diminishes toward 0, and the sample mean (\\(\\overline{x}\\)) converges towards the population mean (\\(\\mu\\))."
  },
  {
    "objectID": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#estimates-and-confidence-intervals",
    "href": "insights/StatisticsandProbability/08.  Central Limit Theorem.html#estimates-and-confidence-intervals",
    "title": "Central Limit Theorem",
    "section": "Estimates and Confidence Intervals",
    "text": "Estimates and Confidence Intervals\n\nPoint Estimate\nA point estimate is a single statistic calculated from a sample used to estimate an unknown population parameter. For instance, the sample mean can serve as a point estimate for the population mean.\n\n\nInterval Estimate\nAn interval estimate is a range of values believed to encompass the true population parameter. It represents the margin of error in estimating the population parameter.\n\n\nConfidence Interval\nA confidence interval is a range of values within which the population mean is presumed to lie. It can be calculated as follows:\n\nWhen Population Standard Deviation is Known\nFor a random sample of size \\(n\\) with mean \\(\\overline{x}\\), taken from a population with standard deviation \\(\\sigma\\) and mean \\(\\mu\\), the confidence interval for the population mean is:\n\\[\\overline{x} - \\frac{z\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{x} + \\frac{z\\sigma}{\\sqrt{n}}\\]\n\n\nWhen Population Standard Deviation is Unknown\nIn cases where the population standard deviation is unknown, the sample standard deviation (\\(s\\)) substitutes \\(\\sigma\\) in calculating the confidence interval:\n\\[\\overline{x} - \\frac{zs}{\\sqrt{n}} \\leq \\mu \\leq \\overline{x} + \\frac{zs}{\\sqrt{n}}\\]\nHere:\n\\(\\overline{x}\\): Sample mean.\n\\(n\\): Sample size.\n\\(\\mu\\): Population mean (the parameter we are estimating).\n\\(\\sigma\\): Population standard deviation (known when calculating the confidence interval with the formula that includes \\(\\sigma\\)).\n\\(s\\): Sample standard deviation (used as an estimate of the population standard deviation when it’s unknown).\n\\(z\\): The critical value from the standard normal distribution corresponding to the desired confidence level. It is determined based on the chosen confidence level (e.g., 95% confidence level corresponds to a z-score of approximately 1.96). This value is used to calculate the margin of error.\n\n\n\nExample\nSuppose we have grades of 10 students drawn from a population, and we aim to ascertain the 95% confidence interval for the population mean.\n\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import t\n\ngrades =  np.array([3.1,2.9,3.2,3.4,3.7,3.9,3.9,2.8,3.4,3.6])\n\nstats.t.interval(0.95, len(grades)-1, loc=np.mean(grades), scale=stats.sem(grades))\n\n(3.1110006165952773, 3.668999383404722)\n\n\nThe arguments inside t.interval function are 95% confidence interval, degrees of freedom (n-1), sample mean and the standard error calculated by stats.sem function."
  },
  {
    "objectID": "insights/StatisticsandProbability/06. Measures of Central Tendency.html",
    "href": "insights/StatisticsandProbability/06. Measures of Central Tendency.html",
    "title": "Introduction to Measures of Central Tendency",
    "section": "",
    "text": "Measures of central tendency are statistical metrics that describe the central point of a dataset. They provide a summary that represents a typical value within the dataset. Key measures of central tendency include the mean, median, mode, percentile, and quartile.\n\n\nThe mean is the arithmetic average of a dataset, calculated by summing all values and dividing by the number of values.\nProperties of the Mean:\n\nThe sum of deviations of the items from their arithmetic mean is always zero, i.e., \\(\\sum (x - \\overline{x}) = 0\\).\nThe sum of the squared deviations from the arithmetic mean (A.M.) is minimized compared to deviations from any other value.\nReplacing each item in the series with the mean results in a sum equal to the sum of the original items.\nThe mean is affected by every value in the dataset.\nIt is a calculated value and not dependent on the position within the series.\nIt is sensitive to extreme values (outliers).\nThe mean cannot typically be identified by inspection.\nIn some cases, the mean may not represent an actual value within the dataset (e.g., an average of 10.7 patients admitted per day).\nThe arithmetic mean is not suitable for extremely asymmetrical distributions.\n\n\n\n\nThe median is the middle value in an ordered dataset, representing the 50th percentile.\nProperties of the Median:\n\nThe median is not influenced by all data values.\nIt is determined by its position in the dataset and not by individual values.\nThe distance from the median to all other values is minimized compared to any other point.\nEvery dataset has a single median.\nThe median cannot be algebraically manipulated or combined.\nIt remains stable in grouped data procedures.\nIt is not applicable to qualitative data.\nThe data must be ordered for median calculation.\nThe median is suitable for ratio, interval, and ordinal scales.\nOutliers and skewed data have less impact on the median.\nThe median is a better measure than the mean in skewed distributions.\n\n\n\n\nThe mode is the most frequently occurring value in a dataset with discrete values.\nProperties of the Mode:\n\nThe mode is useful when the most typical case is desired.\nIt can be used with nominal or categorical data, such as religious preference, gender, or political affiliation.\nThe mode may not be unique; a dataset can have more than one mode or none at all.\n\n\n\n\nA percentile indicates the percentage of values in a dataset that fall below a particular value. The median is the 50th percentile.\n\n\n\nA quartile divides an ordered dataset into four equal parts.\n\n\\(Q_1\\) (first quartile) corresponds to the 25th percentile.\n\\(Q_2\\) corresponds to the median.\n\\(Q_3\\) corresponds to the 75th percentile."
  },
  {
    "objectID": "insights/StatisticsandProbability/06. Measures of Central Tendency.html#overview",
    "href": "insights/StatisticsandProbability/06. Measures of Central Tendency.html#overview",
    "title": "Introduction to Measures of Central Tendency",
    "section": "",
    "text": "Measures of central tendency are statistical metrics that describe the central point of a dataset. They provide a summary that represents a typical value within the dataset. Key measures of central tendency include the mean, median, mode, percentile, and quartile.\n\n\nThe mean is the arithmetic average of a dataset, calculated by summing all values and dividing by the number of values.\nProperties of the Mean:\n\nThe sum of deviations of the items from their arithmetic mean is always zero, i.e., \\(\\sum (x - \\overline{x}) = 0\\).\nThe sum of the squared deviations from the arithmetic mean (A.M.) is minimized compared to deviations from any other value.\nReplacing each item in the series with the mean results in a sum equal to the sum of the original items.\nThe mean is affected by every value in the dataset.\nIt is a calculated value and not dependent on the position within the series.\nIt is sensitive to extreme values (outliers).\nThe mean cannot typically be identified by inspection.\nIn some cases, the mean may not represent an actual value within the dataset (e.g., an average of 10.7 patients admitted per day).\nThe arithmetic mean is not suitable for extremely asymmetrical distributions.\n\n\n\n\nThe median is the middle value in an ordered dataset, representing the 50th percentile.\nProperties of the Median:\n\nThe median is not influenced by all data values.\nIt is determined by its position in the dataset and not by individual values.\nThe distance from the median to all other values is minimized compared to any other point.\nEvery dataset has a single median.\nThe median cannot be algebraically manipulated or combined.\nIt remains stable in grouped data procedures.\nIt is not applicable to qualitative data.\nThe data must be ordered for median calculation.\nThe median is suitable for ratio, interval, and ordinal scales.\nOutliers and skewed data have less impact on the median.\nThe median is a better measure than the mean in skewed distributions.\n\n\n\n\nThe mode is the most frequently occurring value in a dataset with discrete values.\nProperties of the Mode:\n\nThe mode is useful when the most typical case is desired.\nIt can be used with nominal or categorical data, such as religious preference, gender, or political affiliation.\nThe mode may not be unique; a dataset can have more than one mode or none at all.\n\n\n\n\nA percentile indicates the percentage of values in a dataset that fall below a particular value. The median is the 50th percentile.\n\n\n\nA quartile divides an ordered dataset into four equal parts.\n\n\\(Q_1\\) (first quartile) corresponds to the 25th percentile.\n\\(Q_2\\) corresponds to the median.\n\\(Q_3\\) corresponds to the 75th percentile."
  },
  {
    "objectID": "insights/StatisticsandProbability/04. Variables and Data types.html#understanding-variables",
    "href": "insights/StatisticsandProbability/04. Variables and Data types.html#understanding-variables",
    "title": "Variables and Data types",
    "section": "Understanding Variables",
    "text": "Understanding Variables\nIn statistical studies, variables denote characteristics of the subjects under analysis. Selecting appropriate variables is pivotal in designing successful experiments, as they help anticipate outcomes.\nFor instance, when predicting house prices, variables like the number of bedrooms, location, age, amenities nearby, and presence of a garage or pool are considered. These factors, aiding in price prediction, are termed variables."
  },
  {
    "objectID": "insights/StatisticsandProbability/04. Variables and Data types.html#independent-and-dependent-variables",
    "href": "insights/StatisticsandProbability/04. Variables and Data types.html#independent-and-dependent-variables",
    "title": "Variables and Data types",
    "section": "Independent and Dependent Variables",
    "text": "Independent and Dependent Variables\nIndependent Variable: Also known as explanatory or predictor variables, these are factors manipulated in an experiment to observe their impact on outcomes. They represent causes and are not influenced by other study variables.\nDependent Variable: Referred to as response or outcome variables, these are observed results of an experiment. They represent effects and their values depend on changes made to the independent variable."
  },
  {
    "objectID": "insights/StatisticsandProbability/04. Variables and Data types.html#types-of-data",
    "href": "insights/StatisticsandProbability/04. Variables and Data types.html#types-of-data",
    "title": "Variables and Data types",
    "section": "Types of Data",
    "text": "Types of Data\nData, crucial for understanding relationships between variables, making predictions, and supporting decision-making, comes in various types. To accurately analyze and interpret data, it’s essential to comprehend these types:\nQuantitative Data: Deals with quantities and measurements and can be either continuous or discrete. Continuous data refers to uninterrupted values along a scale, like distance and time. Discrete data, however, refers to specific values, like the number of students in a class or the outcome of rolling a die.\nCategorical Data: Represents groupings and is further categorized into nominal, ordinal, and binary types. Nominal data assigns values to categories without any inherent order, such as people’s names and colors. Ordinal data, in contrast, assigns values with an order, like rating levels and grades. Binary data, the simplest form, has only two possible values, such as heads or tails in a coin flip, or yes or no."
  },
  {
    "objectID": "insights/StatisticsandProbability/04. Variables and Data types.html#measurement-scales",
    "href": "insights/StatisticsandProbability/04. Variables and Data types.html#measurement-scales",
    "title": "Variables and Data types",
    "section": "Measurement Scales",
    "text": "Measurement Scales\nMeasurement scales, also referred to as levels of measurement, elucidate how precisely variables are recorded in scientific research. Here, a variable denotes any attribute capable of assuming different values in a dataset (e.g., height, test scores).\nThere exist four measurement scales:\n\nNominal: Data can only be categorized.\nOrdinal: Data can be categorized and ordered.\nInterval: Data can be categorized, ordered, and equally spaced.\nRatio: Data can be categorized, ordered, equally spaced, and possess a true zero point.\n\nThe level of measurement for a variable profoundly influences the types of analyses feasible on it. Ranging from nominal (low) to ratio (high), measurement scales vary in complexity and precision."
  },
  {
    "objectID": "insights/StatisticsandProbability/02. Probability and Rules of Probability.html#overview",
    "href": "insights/StatisticsandProbability/02. Probability and Rules of Probability.html#overview",
    "title": "Probability",
    "section": "Overview",
    "text": "Overview\nProbability is defined as a quantitative measure of uncertainty – a numerical value that conveys the strength of our belief in the occurrence of an event.\nThe probability of an event is always a number between 0 and 1 both 0 and 1 inclusive.\nIf an event’s probability is nearer to 1, the higher is the likelihood that the event will occur; the closer the event’s probability to 0, the smaller is the likelihood that the event will occur.\nIf the event cannot occur, its probability is 0. If it must occur (i.e., its occurrence is certain), its probability is 1.\n1.1 Random experiment\nAn experiment is random means that the experiment has more than one possible outcome and it is not possible to predict with certainty which outcome that will be. For instance, in an experiment of tossing an ordinary coin, it can be predicted with certainty that the coin will land either heads up or tails up, but it is not known for sure whether heads or tails will occur. If a die is thrown once, any of the six numbers, i.e., 1, 2, 3, 4, 5, 6 may turn up, not sure which number will come up.\n(i) Outcome A possible result of a random experiment is called its outcome for example if the experiment consists of tossing a coin twice, some of the outcomes are HH, HT etc.\n(ii) Sample Space A sample space is the set of all possible outcomes of an experiment. In fact, it is the universal set S pertinent to a given experiment. The sample space for the experiment of tossing a coin twice is given by S = {HH, HT, TH, TT}\nThe sample space for the experiment of drawing a card out of a deck is the set of all cards in the deck.\n1.2 Event\nAn event is a subset of a sample space S. For example, the event of drawing an ace from a deck is A = {Ace of Heart, Ace of Club, Ace of Diamond, Ace of Spade}\n1.3 Types of events\n(i) Impossible and Sure Events The empty set φ and the sample space S describe events. In fact φ is called an impossible event and S, i.e., the whole sample space is called a sure event.\n(ii) Simple or Elementary Event If an event E has only one sample point of a sample space, i.e., a single outcome of an experiment, it is called a simple or elementary event. The sample space of the experiment of tossing two coins is given by S = {HH, HT, TH, TT} The event E1 = {HH} containing a single outcome HH of the sample space S is a simple or elementary event. If one card is drawn from a well shuffled deck, any particular card drawn like ‘queen of Hearts’ is an elementary event.\n(iii) Compound Event If an event has more than one sample point it is called a compound event, for example, S = {HH, HT} is a compound event.\n(iv) Complementary event Given an event A, the complement of A is the event consisting of all sample space outcomes that do not correspond to the occurrence of A.\nThe complement of A is denoted by A′ or A . It is also called the event ‘not A’. Further P( A ) denotes the probability that A will not occur. A′ = A = S – A = {w : w ∈ S and w ∉A}\nSimple or Elementary Event If an event E has only one sample point of a sample space, i.e., a single outcome of an experiment, it is called a simple or elementary event. The sample space of the experiment of tossing two coins is given by S = {HH, HT, TH, TT} The event E1 = {HH} containing a single outcome HH of the sample space S is a simple or elementary event. If one card is drawn from a well shuffled deck, any particular card drawn like ‘queen of Hearts’ is an elementary event. (iii) Compound Event If an event has more than one sample point it is called a compound event, for example, S = {HH, HT} is a compound event. (iv) Complementary event Given an event A, the complement of A is the event consisting of all sample space outcomes that do not correspond to the occurrence of A.\n1.4 Event ‘A or B’ \nIf A and B are two events associated with same sample space, then the event ‘A or B’ is same as the event A ∪ B and contains all those elements which are either in A or in B or in both. Further more, P (A∪B) denotes the probability that A or B (or both) will occur.\n1.5 Event ‘A and B’ If A and B are two events associated with a sample space, then the event ‘A and B’ is same as the event A∩ B and contains all those elements which are common to both A and B. Further more, P (A ∩ B) denotes the probability that both A and B will simultaneously occur.\n1.6 The Event ‘A but not B’ (Difference A – B) An event A – B is the set of all those elements of the same space S which are in A but not in B, i.e., A – B = A ∩ B′.\n1.7 Mutually exclusive Two events A and B of a sample space S are mutually exclusive if the occurrence of any one of them excludes the occurrence of the other event. Hence, the two events A and B cannot occur simultaneously, and thus P(A∩B) = 0.\nRemark Simple or elementary events of a sample space are always mutually exclusive. For example, the elementary events {1}, {2}, {3}, {4}, {5} or {6} of the experiment of throwing a dice are mutually exclusive.\nConsider the experiment of throwing a die once.\nThe events E = getting a even number and F = getting an odd number are mutually exclusive events because E ∩ F = φ.\nNote For a given sample space, there may be two or more mutually exclusive events.\n1.8 Exhaustive events If E1 , E2 , …, En are n events of a sample space S and if\nare called exhaustive events. In other words, events E1 , E2 , …, En of a sample space S are said to be exhaustive if atleast one of them necessarily occur whenever the experiment is performed. Consider the example of rolling a die. We have S = {1, 2, 3, 4, 5, 6}. Define the two events A : ‘a number less than or equal to 4 appears.’ B : ‘a number greater than or equal to 4 appears.’ Now A : {1, 2, 3, 4}, B = {4, 5, 6} A ∪ B = {1, 2, 3, 4, 5, 6} = S Such events A and B are called exhaustive events.\n\nUnderstanding Fundamental Concepts in Probability\nBefore delving into the intricacies of probability, it’s essential to grasp some fundamental terms and definitions associated with it.\n\nRandom Experiment:\nA random experiment is characterized by its unpredictable outcomes when repeated under identical conditions. Examples include rolling a die or tossing an unbiased coin.\n\n\nOutcome:\nAn outcome refers to the result obtained from a single trial of an experiment.\n\n\nSample Space:\nThe sample space represents a comprehensive list encompassing all potential outcomes of an experiment. For instance, in the case of tossing a coin, the sample space would be \\(\\{Heads, Tails\\}\\), while for rolling a die, it would consist of \\(\\{1, 2, 3, 4, 5, 6\\}\\).\n\n\nEvent:\nAn event denotes a subset of the sample space and can comprise either a single outcome or a combination of outcomes. For instance, obtaining at least two heads in a row when a coin is tossed four times constitutes an event. Another example could involve getting heads on a coin and rolling a six on a die simultaneously.\n\n\n\nProbability:\nProbability serves as a quantifiable measure of the likelihood of an event occurring.\nNote: Events cannot be predicted with absolute certainty. Probability allows us to assess the likelihood of an event happening, ranging between 0 and 1. A probability of “Zero” signifies that the event is impossible, while a value of “One” indicates certainty.\nThe probability of an event \\(A\\), denoted as \\(P(A)\\), is calculated using the formula:\n\\[ P(A) = \\frac {n(A)}{n(S)} \\]\nwhere:\n- \\(P(A)\\) represents the probability of event \\(A\\) occurring.\n- \\(n(A)\\) denotes the number of favorable outcomes for event \\(A\\).\n- \\(n(S)\\) signifies the total number of possible outcomes.\nExample:\nThe probability of rolling a number less than or equal to 2 when tossing a dieis \\(\\frac{2}{6} = \\frac{1}{3}\\).\n\n\nRules of Probability\nUnderstanding the rules governing probability is crucial for accurate analysis and interpretation.\n\nThe probability of an event can range anywhere from 0 to 1:\n\\(0 \\leq P(A) \\leq 1.\\)\nThis signifies that probabilities lie within the range of certainty from impossible (0) to certain (1).\nSum of all probabilities should add up to 1\n\\(P(A) + P(\\overline{A}) = 1.\\)\nThis rule highlights that the combined probability of an event occurring and not occurring is always equal to 1.\nComplementary Rule - Probability of event A not happening:\n\\(P(\\overline{A})=1-P(A).\\)\nIt indicates that the probability of an event not occurring is equal to 1 minus the probability of the event occurring.\nAddition Rule (A and B are not necessarily disjoint) - Probability of A happening or B happening:\n\\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B).\\)\nThis rule calculates the probability of either event A or event B happening, accounting for the overlap if they are not mutually exclusive\nAddition Rule (A and B are disjoint) - Probability of A happening or B happening:\n\\(P(A\\cup B)=P(A)+P(B).\\)\nThis rule simplifies the addition of probabilities when events A and B are mutually exclusive.\nMultiplication Rule - Chain Rule:\n\\(P(A\\cap B)=P(A)*P(B|A)=P(B)*P(A|B).\\)\nThis rule computes the joint probability of events A and B occurring, taking into account the conditional probabilities.\nIf A and B are independent events, then:\n\\(P(A\\cap B)=P(A)*P(B).\\)\nThis implies that the occurrence of one event does not affect the probability of the other event.\n\\(P(A\\setminus B)=P(A)-P(A\\cap B).\\)\nThis rule calculates the probability of event A happening excluding the outcomes also included in event B.\n\\(If A\\subset B, \\text{then}\\ P(A)\\leq P(B).\\)\nThis indicates that the probability of a subset event A is always less than or equal to the probability of the superset event B.\n$P()=0. $\nThe probability of the empty set is always zero."
  },
  {
    "objectID": "insights/StatisticsandProbability/02. Probability and Rules of Probability.html#conditional-probability",
    "href": "insights/StatisticsandProbability/02. Probability and Rules of Probability.html#conditional-probability",
    "title": "Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nConditional probability of event A given event B is the probability that A occurs given that B has occurred.\n\\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\,.\\]\nLet’s illustrate this with an example:\nSuppose we roll a fair die, and let event A be the outcome being an odd number (i.e., A={1,3,5}), and event B be the outcome being less than or equal to 3 (i.e., B={1,2,3}). What is the probability of A given B, \\(P(A|B)\\)?\n\\[P(B) = \\frac{3}{6} \\quad , \\quad P(A \\cap B) = \\frac{2}{6}\\]\n\\[P(A|B) = \\frac{2}{3}\\]"
  },
  {
    "objectID": "insights/StatisticsandProbability/02. Probability and Rules of Probability.html#law-of-large-numbers",
    "href": "insights/StatisticsandProbability/02. Probability and Rules of Probability.html#law-of-large-numbers",
    "title": "Probability",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nThe law of large numbers asserts that as the sample size increases, the average or mean of the sample values will converge towards the expected value.\nThis principle can be exemplified through a basic scenario of flipping a coin. With a coin having equal chances of landing heads or tails, the expected probability of it landing heads is 1/2 or 0.5 over an infinite number of flips.\nHowever, if we only flip the coin 10 times, we may observe a deviation from the expected value. For instance, the coin might land heads only 3 times out of the 10 flips, which doesn’t align closely with the expected probability of 0.5. This discrepancy is due to the relatively small sample size.\nAs the number of flips increases, say to 20 or 30 times, we would expect the proportion of heads to gradually approach 0.5. For instance, after 20 flips, we might see 9 heads, and after 30 flips, we might observe 22 heads. With a larger sample size, the observed proportion of heads tends to converge towards the expected value of 0.5."
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/ww_classifier.html#our-data",
    "href": "insights/MachineLearningAlgorithm/ww_classifier.html#our-data",
    "title": "CS 224N Lecture 3: Word Window Classification",
    "section": "Our Data",
    "text": "Our Data\nThe task at hand is to assign a label of 1 to words in a sentence that correspond with a LOCATION, and a label of 0 to everything else.\nIn this simplified example, we only ever see spans of length 1.\n\ntrain_sents = [s.lower().split() for s in [\"we 'll always have Paris\",\n                                           \"I live in Germany\",\n                                           \"He comes from Denmark\",\n                                           \"The capital of Denmark is Copenhagen\"]]\ntrain_labels = [[0, 0, 0, 0, 1],\n                [0, 0, 0, 1],\n                [0, 0, 0, 1],\n                [0, 0, 0, 1, 0, 1]]\n\nassert all([len(train_sents[i]) == len(train_labels[i]) for i in range(len(train_sents))])\n\n\ntest_sents = [s.lower().split() for s in [\"She comes from Paris\"]]\ntest_labels = [[0, 0, 0, 1]]\n\nassert all([len(test_sents[i]) == len(test_labels[i]) for i in range(len(test_sents))])"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/ww_classifier.html#creating-a-dataset-of-batched-tensors.",
    "href": "insights/MachineLearningAlgorithm/ww_classifier.html#creating-a-dataset-of-batched-tensors.",
    "title": "CS 224N Lecture 3: Word Window Classification",
    "section": "Creating a dataset of batched tensors.",
    "text": "Creating a dataset of batched tensors.\nPyTorch (like other deep learning frameworks) is optimized to work on tensors, which can be thought of as a generalization of vectors and matrices with arbitrarily large rank.\nHere well go over how to translate data to a list of vocabulary indices, and how to construct batch tensors out of the data for easy input to our model.\nWe’ll use the torch.utils.data.DataLoader object handle ease of batching and iteration.\n\nConverting tokenized sentence lists to vocabulary indices.\nLet’s assume we have the following vocabulary:\n\nid_2_word = [\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"we\", \"always\", \"have\", \"paris\",\n              \"i\", \"live\", \"in\", \"germany\",\n              \"he\", \"comes\", \"from\", \"denmark\",\n              \"the\", \"of\", \"is\", \"copenhagen\"]\nword_2_id = {w:i for i,w in enumerate(id_2_word)}\n\n\ninstance = train_sents[0]\nprint(instance)\n\n['we', \"'ll\", 'always', 'have', 'paris']\n\n\n\ndef convert_tokens_to_inds(sentence, word_2_id):\n    return [word_2_id.get(t, word_2_id[\"&lt;unk&gt;\"]) for t in sentence]\n\n\ntoken_inds = convert_tokens_to_inds(instance, word_2_id)\npp.pprint(token_inds)\n\n[2, 1, 3, 4, 5]\n\n\nLet’s convince ourselves that worked:\n\nprint([id_2_word[tok_idx] for tok_idx in token_inds])\n\n['we', '&lt;unk&gt;', 'always', 'have', 'paris']\n\n\n\n\nPadding for windows.\nIn the word window classifier, for each word in the sentence we want to get the +/- n window around the word, where 0 &lt;= n &lt; len(sentence).\nIn order for such windows to be defined for words at the beginning and ends of the sentence, we actually want to insert padding around the sentence before converting to indices:\n\ndef pad_sentence_for_window(sentence, window_size, pad_token=\"&lt;pad&gt;\"):\n    return [pad_token]*window_size + sentence + [pad_token]*window_size \n\n\nwindow_size = 2\ninstance = pad_sentence_for_window(train_sents[0], window_size)\nprint(instance)\n\n['&lt;pad&gt;', '&lt;pad&gt;', 'we', \"'ll\", 'always', 'have', 'paris', '&lt;pad&gt;', '&lt;pad&gt;']\n\n\nLet’s make sure this works with our vocabulary:\n\nfor sent in train_sents:\n    tok_idxs = convert_tokens_to_inds(pad_sentence_for_window(sent, window_size), word_2_id)\n    print([id_2_word[idx] for idx in tok_idxs])\n\n['&lt;pad&gt;', '&lt;pad&gt;', 'we', '&lt;unk&gt;', 'always', 'have', 'paris', '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;pad&gt;', '&lt;pad&gt;', 'i', 'live', 'in', 'germany', '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;pad&gt;', '&lt;pad&gt;', 'he', 'comes', 'from', 'denmark', '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;pad&gt;', '&lt;pad&gt;', 'the', '&lt;unk&gt;', 'of', 'denmark', 'is', 'copenhagen', '&lt;pad&gt;', '&lt;pad&gt;']\n\n\n\n\nBatching sentences together with a DataLoader\nWhen we train our model, we rarely update with respect to a single training instance at a time, because a single instance provides a very noisy estimate of the global loss’s gradient. We instead construct small batches of data, and update parameters for each batch.\nGiven some batch size, we want to construct batch tensors out of the word index lists we’ve just created with our vocab.\nFor each length B list of inputs, we’ll have to:\n(1) Add window padding to sentences in the batch like we just saw.\n(2) Add additional padding so that each sentence in the batch is the same length.\n(3) Make sure our labels are in the desired format.\nAt the level of the dataest we want:\n(4) Easy shuffling, because shuffling from one training epoch to the next gets rid of \n    pathological batches that are tough to learn from.\n(5) Making sure we shuffle inputs and their labels together!\nPyTorch provides us with an object torch.utils.data.DataLoader that gets us (4) and (5). All that’s required of us is to specify a collate_fn that tells it how to do (1), (2), and (3).\n\nl = torch.LongTensor(train_labels[0])\npp.pprint((\"raw train label instance\", l))\nprint(l.size())\n\n('raw train label instance', tensor([0, 0, 0, 0, 1]))\ntorch.Size([5])\n\n\n\none_hots = torch.zeros((2, len(l)))\npp.pprint((\"unfilled label instance\", one_hots))\nprint(one_hots.size())\n\n('unfilled label instance',\n tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]))\ntorch.Size([2, 5])\n\n\n\none_hots[1] = l\npp.pprint((\"one-hot labels\", one_hots))\n\n('one-hot labels', tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1.]]))\n\n\n\nl_not = ~l.byte()\none_hots[0] = l_not\npp.pprint((\"one-hot labels\", one_hots))\n\n('one-hot labels', tensor([[1., 1., 1., 1., 0.],\n        [0., 0., 0., 0., 1.]]))\n\n\n\nfrom torch.utils.data import DataLoader\nfrom functools import partial\n\n\ndef my_collate(data, window_size, word_2_id):\n    \"\"\"\n    For some chunk of sentences and labels\n        -add winow padding\n        -pad for lengths using pad_sequence\n        -convert our labels to one-hots\n        -return padded inputs, one-hot labels, and lengths\n    \"\"\"\n    \n    x_s, y_s = zip(*data)\n\n    # deal with input sentences as we've seen\n    window_padded = [convert_tokens_to_inds(pad_sentence_for_window(sentence, window_size), word_2_id)\n                                                                                  for sentence in x_s]\n    # append zeros to each list of token ids in batch so that they are all the same length\n    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n    \n    # convert labels to one-hots\n    labels = []\n    lengths = []\n    for y in y_s:\n        lengths.append(len(y))\n        label = torch.zeros((len(y),2 ))\n        true = torch.LongTensor(y) \n        false = ~true.byte()\n        label[:, 0] = false\n        label[:, 1] = true\n        labels.append(label)\n    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n    \n    return padded.long(), padded_labels, torch.LongTensor(lengths)\n\n\n# Shuffle True is good practice for train loaders.\n# Use functools.partial to construct a partially populated collate function\nexample_loader = DataLoader(list(zip(train_sents, \n                                                      train_labels)), \n                                             batch_size=2, \n                                             shuffle=True, \n                                             collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))\n\n\nfor batched_input, batched_labels, batch_lengths in example_loader:\n    pp.pprint((\"inputs\", batched_input, batched_input.size()))\n    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\n    pp.pprint(batch_lengths)\n    break\n\n('inputs',\n tensor([[ 0,  0,  2,  1,  3,  4,  5,  0,  0],\n        [ 0,  0, 10, 11, 12, 13,  0,  0,  0]]),\n torch.Size([2, 9]))\n('labels',\n tensor([[[1., 0.],\n         [1., 0.],\n         [1., 0.],\n         [1., 0.],\n         [0., 1.]],\n\n        [[1., 0.],\n         [1., 0.],\n         [1., 0.],\n         [0., 1.],\n         [0., 0.]]]),\n torch.Size([2, 5, 2]))\ntensor([5, 4])"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/ww_classifier.html#modeling",
    "href": "insights/MachineLearningAlgorithm/ww_classifier.html#modeling",
    "title": "CS 224N Lecture 3: Word Window Classification",
    "section": "Modeling",
    "text": "Modeling\n\nThinking through vectorization of word windows.\nBefore we go ahead and build our model, let’s think about the first thing it needs to do to its inputs.\nWe’re passed batches of sentences. For each sentence i in the batch, for each word j in the sentence, we want to construct a single tensor out of the embeddings surrounding word j in the +/- n window.\nThus, the first thing we’re going to need a (B, L, 2N+1) tensor of token indices.\nA terrible but nevertheless informative iterative solution looks something like the following, where we iterate through batch elements in our (dummy), iterating non-padded word positions in those, and for each non-padded word position, construct a window:\n\ndummy_input = torch.zeros(2, 8).long()\ndummy_input[:,2:-2] = torch.arange(1,9).view(2,4)\npp.pprint(dummy_input)\n\ntensor([[0, 0, 1, 2, 3, 4, 0, 0],\n        [0, 0, 5, 6, 7, 8, 0, 0]])\n\n\n\ndummy_output = [[[dummy_input[i, j-2+k].item() for k in range(2*2+1)] \n                                                     for j in range(2, 6)] \n                                                            for i in range(2)]\ndummy_output = torch.LongTensor(dummy_output)\nprint(dummy_output.size())\npp.pprint(dummy_output)\n\ntorch.Size([2, 4, 5])\ntensor([[[0, 0, 1, 2, 3],\n         [0, 1, 2, 3, 4],\n         [1, 2, 3, 4, 0],\n         [2, 3, 4, 0, 0]],\n\n        [[0, 0, 5, 6, 7],\n         [0, 5, 6, 7, 8],\n         [5, 6, 7, 8, 0],\n         [6, 7, 8, 0, 0]]])\n\n\nTechnically it works: For each element in the batch, for each word in the original sentence and ignoring window padding, we’ve got the 5 token indices centered at that word. But in practice will be crazy slow.\nInstead, we ideally want to find the right tensor operation in the PyTorch arsenal. Here, that happens to be Tensor.unfold.\n\ndummy_input.unfold(1, 2*2+1, 1)\n\ntensor([[[0, 0, 1, 2, 3],\n         [0, 1, 2, 3, 4],\n         [1, 2, 3, 4, 0],\n         [2, 3, 4, 0, 0]],\n\n        [[0, 0, 5, 6, 7],\n         [0, 5, 6, 7, 8],\n         [5, 6, 7, 8, 0],\n         [6, 7, 8, 0, 0]]])\n\n\n\n\nA model in full.\nIn PyTorch, we implement models by extending the nn.Module class. Minimally, this requires implementing an __init__ function and a forward function.\nIn __init__ we want to store model parameters (weights) and hyperparameters (dimensions).\n\nclass SoftmaxWordWindowClassifier(nn.Module):\n    \"\"\"\n    A one-layer, binary word-window classifier.\n    \"\"\"\n    def __init__(self, config, vocab_size, pad_idx=0):\n        super(SoftmaxWordWindowClassifier, self).__init__()\n        \"\"\"\n        Instance variables.\n        \"\"\"\n        self.window_size = 2*config[\"half_window\"]+1\n        self.embed_dim = config[\"embed_dim\"]\n        self.hidden_dim = config[\"hidden_dim\"]\n        self.num_classes = config[\"num_classes\"]\n        self.freeze_embeddings = config[\"freeze_embeddings\"]\n        \n        \"\"\"\n        Embedding layer\n        -model holds an embedding for each layer in our vocab\n        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n        -by default, embeddings are parameters (so gradients pass through them)\n        \"\"\"\n        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n        if self.freeze_embeddings:\n            self.embed_layer.weight.requires_grad = False\n        \n        \"\"\"\n        Hidden layer\n        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n        -nn.Sequential allows you to efficiently specify sequentially structured models\n            -first the linear transformation is evoked on the embedded word windows\n            -next the nonlinear transformation tanh is evoked.\n        \"\"\"\n        self.hidden_layer = nn.Sequential(nn.Linear(self.window_size*self.embed_dim, \n                                                    self.hidden_dim), \n                                          nn.Tanh())\n        \n        \"\"\"\n        Output layer\n        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n        \"\"\"\n        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n        \n        \"\"\"\n        Softmax\n        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n        -pytorch has both logsoftmax and softmax functions (and many others)\n        -since our loss is the negative LOG likelihood, we use logsoftmax\n        -technically you can take the softmax, and take the log but PyTorch's implementation\n         is optimized to avoid numerical underflow issues.\n        \"\"\"\n        self.log_softmax = nn.LogSoftmax(dim=2)\n        \n    def forward(self, inputs):\n        \"\"\"\n        Let B:= batch_size\n            L:= window-padded sentence length\n            D:= self.embed_dim\n            S:= self.window_size\n            H:= self.hidden_dim\n            \n        inputs: a (B, L) tensor of token indices\n        \"\"\"\n        B, L = inputs.size()\n        \n        \"\"\"\n        Reshaping.\n        Takes in a (B, L) LongTensor\n        Outputs a (B, L~, S) LongTensor\n        \"\"\"\n        # Fist, get our word windows for each word in our input.\n        token_windows = inputs.unfold(1, self.window_size, 1)\n        _, adjusted_length, _ = token_windows.size()\n        \n        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n        assert token_windows.size() == (B, adjusted_length, self.window_size)\n        \n        \"\"\"\n        Embedding.\n        Takes in a torch.LongTensor of size (B, L~, S) \n        Outputs a (B, L~, S, D) FloatTensor.\n        \"\"\"\n        embedded_windows = self.embed_layer(token_windows)\n        \n        \"\"\"\n        Reshaping.\n        Takes in a (B, L~, S, D) FloatTensor.\n        Resizes it into a (B, L~, S*D) FloatTensor.\n        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n        \"\"\"\n        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n        \n        \"\"\"\n        Layer 1.\n        Takes in a (B, L~, S*D) FloatTensor.\n        Resizes it into a (B, L~, H) FloatTensor\n        \"\"\"\n        layer_1 = self.hidden_layer(embedded_windows)\n        \n        \"\"\"\n        Layer 2\n        Takes in a (B, L~, H) FloatTensor.\n        Resizes it into a (B, L~, 2) FloatTensor.\n        \"\"\"\n        output = self.output_layer(layer_1)\n        \n        \"\"\"\n        Softmax.\n        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n        \"\"\"\n        output = self.log_softmax(output)\n        \n        return output\n\n\n\nTraining.\nNow that we’ve got a model, we have to train it.\n\ndef loss_function(outputs, labels, lengths):\n    \"\"\"Computes negative LL loss on a batch of model predictions.\"\"\"\n    B, L, num_classes = outputs.size()\n    num_elems = lengths.sum().float()\n        \n    # get only the values with non-zero labels\n    loss = outputs*labels\n    \n    # rescale average\n    return -loss.sum() / num_elems\n\n\ndef train_epoch(loss_function, optimizer, model, train_data):\n    \n    ## For each batch, we must reset the gradients\n    ## stored by the model.   \n    total_loss = 0\n    for batch, labels, lengths in train_data:\n        # clear gradients\n        optimizer.zero_grad()\n        # evoke model in training mode on batch\n        outputs = model.forward(batch)\n        # compute loss w.r.t batch\n        loss = loss_function(outputs, labels, lengths)\n        # pass gradients back, startiing on loss value\n        loss.backward()\n        # update parameters\n        optimizer.step()\n        total_loss += loss.item()\n    \n    # return the total to keep track of how you did this time around\n    return total_loss\n    \n\n\nconfig = {\"batch_size\": 4,\n          \"half_window\": 2,\n          \"embed_dim\": 25,\n          \"hidden_dim\": 25,\n          \"num_classes\": 2,\n          \"freeze_embeddings\": False,\n         }\nlearning_rate = .0002\nnum_epochs = 10000\nmodel = SoftmaxWordWindowClassifier(config, len(word_2_id))\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\ntrain_loader = torch.utils.data.DataLoader(list(zip(train_sents, train_labels)), \n                                           batch_size=2, \n                                           shuffle=True, \n                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))\n\n\nlosses = []\nfor epoch in range(num_epochs):\n    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n    if epoch % 100 == 0:\n        losses.append(epoch_loss)\nprint(losses)\n\n[1.4967301487922668, 1.408476173877716, 1.3443800806999207, 1.2865177989006042, 1.2272869944572449, 1.1691689491271973, 1.1141255497932434, 1.0696152448654175, 1.023829996585846, 0.978839099407196, 0.937132716178894, 0.8965558409690857, 0.8551942408084869, 0.8171629309654236, 0.7806291580200195, 0.7467736303806305, 0.7136902511119843, 0.6842415034770966, 0.6537061333656311, 0.6195352077484131, 0.5914349257946014, 0.5682767033576965, 0.5430445969104767, 0.5190333724021912, 0.49760693311691284, 0.47582894563674927, 0.45516568422317505, 0.4298042058944702, 0.41591694951057434, 0.39368535578250885, 0.3817802667617798, 0.36694473028182983, 0.35200121998786926, 0.3370656222105026, 0.31913231313228607, 0.3065541982650757, 0.2946578562259674, 0.28842414915561676, 0.27765345573425293, 0.26745346188545227, 0.25778329372406006, 0.24860621988773346, 0.23990143835544586, 0.22729042172431946, 0.22337404638528824, 0.21637336909770966, 0.20889568328857422, 0.20218300074338913, 0.19230441004037857, 0.19007354974746704, 0.18426819890737534, 0.17840557545423508, 0.173139289021492, 0.16499895602464676, 0.1602725237607956, 0.1590176522731781, 0.15144427865743637, 0.14732149988412857, 0.14641961455345154, 0.13959994912147522, 0.13598214834928513, 0.13251276314258575, 0.13197287172079086, 0.12871850654482841, 0.1253872662782669, 0.12239058315753937, 0.1171659529209137, 0.11695125326514244, 0.11428486183285713, 0.11171672493219376, 0.10924769192934036, 0.10686498507857323, 0.1045713983476162, 0.10218603909015656, 0.10022115334868431, 0.09602915123105049, 0.09616792947053909, 0.09424330666661263, 0.09223027899861336, 0.090587567538023, 0.08691023662686348, 0.08717184513807297, 0.08540527895092964, 0.0839710421860218, 0.08230703324079514, 0.0808291956782341, 0.07777531817555428, 0.0780084915459156, 0.07678597420454025, 0.07535399869084358, 0.07408255711197853, 0.07296567782759666, 0.07176320999860764, 0.07059716433286667, 0.0694643184542656, 0.06684627756476402, 0.06579622253775597, 0.06477534398436546, 0.06378135085105896, 0.06281331554055214]\n\n\n\n\nPrediction.\n\ntest_loader = torch.utils.data.DataLoader(list(zip(test_sents, test_labels)), \n                                           batch_size=1, \n                                           shuffle=False, \n                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))\n\n\nfor test_instance, labs, _ in test_loader:\n    outputs = model.forward(test_instance)\n    print(torch.argmax(outputs, dim=2))\n    print(torch.argmax(labs, dim=2))\n\ntensor([[0, 0, 0, 1]])\ntensor([[0, 0, 0, 1]])"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/numpy_practice.html",
    "href": "insights/MachineLearningAlgorithm/numpy_practice.html",
    "title": "Numpy Practice",
    "section": "",
    "text": "Author: Alireza Dirafzoon\nContributions are welcome :)\n\n\nimport numpy as np \n\n\n### array()\na = [1, 2, 3]\nx = np.array(a) \nx = np.asarray(a)\nx\n\narray([1, 2, 3])\n\n\n\nx.tolist()\n\n[1, 2, 3]\n\n\n\nx.astype(np.float32)\n\narray([1., 2., 3.], dtype=float32)\n\n\n\n### arange()\nnp.arange(3) \nnp.arange(0,7,2) \nnp.arange(3, -1, -1)\n\narray([3, 2, 1, 0])\n\n\n\n### zeros, ones, eye, linspace\nnp.zeros(3) \nnp.zeros((3,3)) \n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\nnp.ones(3)\nnp.ones((3,3))\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nnp.linspace(0,10,3) # 3 points, from 0 to 10, inclusive \nnp.linspace(0,7,3) \n\narray([0. , 3.5, 7. ])\n\n\n\n### np.random\n\n# random.rand(): uniform distr over [0, 1)\nnp.random.rand()   \nnp.random.rand(2)\nnp.random.rand(2,3)\n\narray([[0.7771434 , 0.08427174, 0.84780602],\n       [0.6069425 , 0.72381233, 0.54255502]])\n\n\n\n# random.randn(): normal distr.\nnp.random.randn(2,3)\n\narray([[ 2.2473354 , -1.27775236, -0.70635289],\n       [-1.56768889,  0.33955847, -0.16860601]])\n\n\n\n# random.randint: int in [low,high) / [0, high)\nnp.random.randint(1,4)\nnp.random.randint(1,4, (2,2))\n\narray([[2, 3],\n       [3, 1]])\n\n\n\nnp.random.randint(4)\n\n3\n\n\n\n## array methods \n\n### reshape\na = np.arange(1,7)\na = a.reshape(2,3)\na\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\n### max, min, atgmax, argmin \na.max(axis = 0)\n\narray([4, 5, 6])\n\n\n\na.argmax(axis=0)\n\narray([1, 1, 1])\n\n\n\na.max(axis = 1)\n\narray([3, 6])\n\n\n\na.max()\n\n6\n\n\n\na.argmax()\n\n5\n\n\n\n### shape and dtype \na.shape\n\n(2, 3)\n\n\n\na.dtype\n\ndtype('int64')\n\n\n\na.nbytes\n\n48\n\n\n\n### 2D array/matrix \nm = np.arange(12).reshape(3,4)\nm\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\nm.mean(axis=0)\n\narray([4., 5., 6., 7.])\n\n\n\nm.std(axis=1)\n\narray([1.11803399, 1.11803399, 1.11803399])\n\n\n\nm.T # or m.transpose\n\narray([[ 0,  4,  8],\n       [ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11]])\n\n\n\nm.reshape((-1,12))\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\n\n\n\nm.reshape(-1)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n\n\nm.ravel()\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n\n\nm\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\n## indexing and selection\n\n\nprint(m[1][2])\nprint(m[1,2])\nprint(m[1,:])\n\n6\n6\n[4 5 6 7]\n\n\n\n## boradcasting\na = np.zeros(10)\na[5:8] = 5 # note we can't do this with list \na\n\narray([0., 0., 0., 0., 0., 5., 5., 5., 0., 0.])\n\n\n\nsuba = a[:3]\nsuba[:] = 2\nsuba\n\narray([2., 2., 2.])\n\n\n\na # note that suba is not a copy, just points to a slice of a\n\narray([2., 2., 2., 0., 0., 5., 5., 5., 0., 0.])\n\n\n\nsuba = np.copy(a[:3])\nsuba\n\narray([2., 2., 2.])\n\n\n\nm = np.zeros((4,4))\nm[1] = 2\nm\n\narray([[0., 0., 0., 0.],\n       [2., 2., 2., 2.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\n# fancy indexing\nm[[1,3]]\n\narray([[2., 2., 2., 2.],\n       [0., 0., 0., 0.]])\n\n\n\n## selection \na = np.arange(4)\na &gt; 2 # note we can't do this with list \n\narray([False, False, False,  True])\n\n\n\n(a == 2).astype(np.int16).sum()\n\n1\n\n\n\na.nonzero()\n\n(array([1, 2, 3]),)\n\n\n\n## Operations\n\n\na, b = np.arange(0,3), np.arange(3,6)\na + b\na - b \na * b # element-wise \na/b # element-wise \n\narray([0.  , 0.25, 0.4 ])\n\n\n\nnp.multiply(a,b) # element-wise \n\narray([ 0,  4, 10])\n\n\n\n# dot product of arrays\nnp.dot(a,b)\n\n14\n\n\n\n# cross product \nnp.cross(a,b)\n\narray([-3,  6, -3])\n\n\n\n# matrix multiplication\nnp.matmul(a,b.T)\n\n14\n\n\n\na = np.arange(9).reshape((3,3)) #2D\nb = np.array([0,1,0]) # 1D\nprint(a,b) \nnp.matmul(a,b) # 2D * 1D -&gt; broadcasts the 1D array, treating it as a col \n\n[[0 1 2]\n [3 4 5]\n [6 7 8]] [0 1 0]\n\n\narray([3, 4, 5])\n\n\n\nnp.power(a,2) # element-wise \nnp.power(a,b) # element-wise \nnp.mod(a,b)\n\narray([0, 1, 2])\n\n\n\nnp.sqrt(a)\nnp.exp(a)\nnp.sin(a)\nnp.log(a)\n\n/Users/alirezadirafzoon/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n  after removing the cwd from sys.path.\n\n\narray([[      -inf, 0.        , 0.69314718],\n       [1.09861229, 1.38629436, 1.60943791],\n       [1.79175947, 1.94591015, 2.07944154]])\n\n\n\n## Kmeans \n\n\nx1 = np.add(np.random.randn(10,2), 5)\nx2 = np.add(np.random.randn(10,2), -5)\nX = np.concatenate([x1,x2], axis=0)\nmu, clusters = kmeans(X,2)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-67-a1d78b800ed7&gt; in &lt;module&gt;\n      2 x2 = np.add(np.random.randn(10,2), -5)\n      3 X = np.concatenate([x1,x2], axis=0)\n----&gt; 4 mu, clusters = find_centers(X,2)\n\n&lt;ipython-input-52-11ad8b933b9e&gt; in find_centers(X, K)\n     24 def find_centers(X, K):\n     25     # Initialize to K random centers\n---&gt; 26     oldmu = random.sample(X, K)\n     27     mu = random.sample(X, K)\n     28     while not has_converged(mu, oldmu):\n\n~/opt/anaconda3/lib/python3.7/random.py in sample(self, population, k)\n    315             population = tuple(population)\n    316         if not isinstance(population, _Sequence):\n--&gt; 317             raise TypeError(\"Population must be a sequence or set.  For dicts, use list(d).\")\n    318         randbelow = self._randbelow\n    319         n = len(population)\n\nTypeError: Population must be a sequence or set.  For dicts, use list(d).\n\n\n\n\n\n\n[[4.8649386655349955, 3.9952475402226817],\n [5.498489206113001, 5.069951322563478],\n [4.929449898684354, 5.719151512307626],\n [4.595440437145644, 4.810271477510138],\n [5.285073437207049, 5.922053828848186],\n [3.2378112256065865, 4.595935658934975],\n [3.8231073755832887, 6.144586325794659],\n [4.1009988278675245, 6.559105478655928],\n [3.9976386132206, 4.424471531025596],\n [4.691876028371731, 5.345908717367563],\n [-5.720985281350966, -5.68922383985498],\n [-5.4201288230000815, -4.431411907717413],\n [-3.6983426126902725, -4.636565625778152],\n [-5.342010805119905, -6.095419133835849],\n [-4.2666049359220235, -3.284073438471302],\n [-6.469221214094414, -7.369070651238069],\n [-3.284553291631532, -5.672466183383029],\n [-3.4845642662555996, -5.40312458836927],\n [-5.6863731385517005, -5.30056130289524],\n [-5.194321373602274, -5.935463756358125]]\n\n\n\nimport random\ndef dist():\n    pass \ndef assign_clusters(X, mu):\n    \ndef kmeans(X,k):\n    mu = random.sample(X,k)\n    it = 1 \n    max_it  = 100\n    while it &lt; max_it: \n        # assign clusters to centers \n        clusters = assign_clusters(X, mu)\n        # calculate new centers \n        mu = calculate_centers(mu, clusters)\n    return mu, clusters\n\narray([[ 4.61084009,  3.81951528,  4.80554869,  4.89529018,  4.32093641,\n         4.23753206,  2.72005748,  6.7060486 ,  4.02801539,  4.04573508,\n        -6.07068165, -7.1437371 , -6.49431954, -4.90412879, -5.25460504,\n        -3.86646858, -6.98290866, -4.82434449, -6.14940609, -6.55090156],\n       [ 4.79319455,  4.07846865,  6.5072268 ,  4.9865201 ,  4.66317278,\n         2.8773762 ,  3.67165213,  4.9436099 ,  3.93374775,  1.65634458,\n        -5.16876443, -5.99996567, -4.70701913, -5.61868297, -5.08846978,\n        -6.31017567, -3.46475003, -4.53046633, -4.35615641, -7.20485829]])\n\n\n\nmu = np.random.rand(5,2)\nx = np.random.rand(2)\n\n\n[-mu[i[0]] for i in enumerate(mu)]\n\n[array([-0.722217  , -0.95781472]),\n array([-0.43986246, -0.58463802]),\n array([-0.16399214, -0.27117604]),\n array([-0.88255848, -0.98718324]),\n array([-0.53056903, -0.60690576])]\n\n\n\nfor i, mu_i in enumerate(mu):\n    print(min(i, np.linalg.norm(x - mu_i), key=lambda x:x[1]))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-48-7924ee5c2f3f&gt; in &lt;module&gt;\n      1 for i, mu_i in enumerate(mu):\n----&gt; 2     print(min(i, np.linalg.norm(x - mu_i), key=lambda x:x[1]))\n\n&lt;ipython-input-48-7924ee5c2f3f&gt; in &lt;lambda&gt;(x)\n      1 for i, mu_i in enumerate(mu):\n----&gt; 2     print(min(i, np.linalg.norm(x - mu_i), key=lambda x:x[1]))\n\nTypeError: 'int' object is not subscriptable\n\n\n\n\n\n\n0 [1, 2, 3]\n1 [4, 5, 6]"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/logistic_regression.html",
    "href": "insights/MachineLearningAlgorithm/logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Logistic regression is a statistical method used for binary classification, which means it is used to predict the probability of an event occurring or not. It is a type of generalized linear model that is used when the dependent variable is binary or categorical.\nIn logistic regression, the dependent variable is binary (i.e., it can take on one of two values, usually 0 or 1), and the independent variables can be either continuous or categorical. The goal of logistic regression is to find the relationship between the independent variables and the dependent variable by estimating the probability of the dependent variable being 1 given the values of the independent variables.\nThe logistic regression model uses a logistic function (also known as the sigmoid function) to map the input values of the independent variables to a value between 0 and 1, which represents the probability of the dependent variable being 1. The logistic function is defined as:\ncss Copy code p = 1 / (1 + e^(-z)) where p is the predicted probability of the dependent variable being 1, e is the base of the natural logarithm, and z is the linear combination of the independent variables.\nThe logistic regression model estimates the values of the coefficients of the independent variables that maximize the likelihood of observing the data given the model. This is typically done using maximum likelihood estimation or gradient descent optimization.\nOnce the model is trained, it can be used to make predictions on new data by inputting the values of the independent variables into the logistic function and obtaining the predicted probability of the dependent variable being 1. The model can then classify the new observation as 1 or 0 based on a threshold probability value that is chosen by the user."
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/logistic_regression.html#code",
    "href": "insights/MachineLearningAlgorithm/logistic_regression.html#code",
    "title": "Logistic Regression",
    "section": "Code",
    "text": "Code\nHere’s an example implementation using gradient descent optimization:\n\nimport numpy as np\n\nclass LogisticRegression:\n    \n    def __init__(self, learning_rate=0.01, n_iters=1000):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n        \n    def fit(self, X, y):\n        # initialize weights and bias to zeros\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        # gradient descent optimization\n        for i in range(self.n_iters):\n            # calculate predicted probabilities and cost\n            z = np.dot(X, self.weights) + self.bias\n            y_pred = self._sigmoid(z)\n            cost = (-1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n            \n            # calculate gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n            db = (1 / n_samples) * np.sum(y_pred - y)\n            \n            # update weights and bias\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n            \n    def predict(self, X):\n        # calculate predicted probabilities\n        z = np.dot(X, self.weights) + self.bias\n        y_pred = self._sigmoid(z)\n        # convert probabilities to binary predictions\n        return np.round(y_pred).astype(int)\n    \n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n\nTest\n\n# create sample dataset\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny = np.array([0, 0, 1, 1, 1])\n\n# initialize logistic regression model\nlr = LogisticRegression()\n\n# train model on sample dataset\nlr.fit(X, y)\n\n# make predictions on new data\nX_new = np.array([[6, 7], [7, 8]])\ny_pred = lr.predict(X_new)\n\nprint(y_pred)  # [1, 1]\n\n\n[1 1]\n\n\n\n\nImprovements\nhere are some possible improvements you could make to the code:\n\nAdd regularization: Regularization can help prevent overfitting and improve the generalization performance of the model. You could add L1 or L2 regularization to the cost function and adjust the regularization strength with a hyperparameter. Here’s an example of how to add L2 regularization to the code:\nUse a more sophisticated optimization algorithm: Gradient descent is a simple and effective optimization algorithm, but it may not be the most efficient or accurate for large or complex datasets. You could try using a more sophisticated algorithm, such as stochastic gradient descent (SGD), mini-batch SGD, or Adam, which can converge faster and find better optima. Here’s an example of how to use mini-batch SGD:\n\n\nimport numpy as np\n\nclass LogisticRegression:\n    \n    def __init__(self, learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=32):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.regularization = regularization\n        self.reg_strength = reg_strength\n        self.batch_size = batch_size\n        self.weights = None\n        self.bias = None\n        \n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        n_batches = n_samples // self.batch_size\n        for i in range(self.n_iters):\n            batch_indices = np.random.choice(n_samples, self.batch_size)\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            z = np.dot(X_batch, self.weights) + self.bias\n            y_pred = self._sigmoid(z)\n            cost = (-1 / self.batch_size) * np.sum(y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred))\n            if self.regularization == 'l2':\n                reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(self.weights ** 2)\n                cost += reg_cost\n            elif self.regularization == 'l1':\n                reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(np.abs(self.weights))\n                cost += reg_cost\n            dw = (1 / self.batch_size) * np.dot(X_batch.T, (y_pred - y_batch))\n            db = (1 / self.batch_size) * np.sum(y_pred - y_batch)\n            if self.regularization == 'l2':\n                dw += (self.reg_strength / n_samples) * self.weights\n            elif self.regularization == 'l1':\n                dw += (self.reg_strength / n_samples) * np.sign(self.weights)\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n            \n    def predict(self, X):\n        z = np.dot(X, self.weights) + self.bias\n        y_pred = self._sigmoid(z)\n        return np.round(y_pred).astype(int)\n    \n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\nThis implementation includes the following improvements:\n\nRegularization: You can choose between L1 or L2 regularization by setting the regularization parameter to either ‘l1’ or ‘l2’, and adjust the regularization strength with the reg_strength parameter.\nMini-batch stochastic gradient descent: The model uses mini-batch SGD (instead of simple gradient descent) to update the weights and bias, which can converge faster and find better optima.\n\n\n\nTest\n\n# create sample dataset\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny = np.array([0, 0, 1, 1, 1])\n\n# initialize logistic regression model\nlr = LogisticRegression(learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=2)\n\n# train model on sample dataset\nlr.fit(X, y)\n\n# make predictions on new data\nX_new = np.array([[6, 7], [7, 8]])\ny_pred = lr.predict(X_new)\n\nprint(y_pred)  # [1, 1]\n\n[1 1]"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/logistic_regression.html#visualize",
    "href": "insights/MachineLearningAlgorithm/logistic_regression.html#visualize",
    "title": "Logistic Regression",
    "section": "Visualize",
    "text": "Visualize\nIt is difficult to visualize logistic regression since it is a high-dimensional problem. However, we can visualize the decision boundary of a logistic regression model for a two-dimensional dataset.\nHere’s an example of how to visualize the decision boundary of the LogisticRegression class on a 2D dataset using the matplotlib library:\n\nimport matplotlib.pyplot as plt\n\n# create 2D dataset\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny = np.array([0, 0, 1, 1, 1])\n\n# initialize logistic regression model\nlr = LogisticRegression(learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=2)\n\n# train model on dataset\nlr.fit(X, y)\n\n# plot decision boundary\nx1 = np.linspace(0, 6, 100)\nx2 = np.linspace(0, 8, 100)\nxx, yy = np.meshgrid(x1, x2)\nZ = lr.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n\n# plot data points\nplt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Spectral)\n\nplt.show()"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/linear_regression.html",
    "href": "insights/MachineLearningAlgorithm/linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a statistical method used to model the relationship between a dependent variable (often denoted as “y”) and one or more independent variables (often denoted as “x”). The basic idea of linear regression is to find the straight line that best fits the data points in a scatter plot.\nThe most common form of linear regression is simple linear regression, which models the relationship between two variables:\n\\(y = mx + b\\)\nwhere y is the dependent variable, x is the independent variable, m is the slope, and b is the intercept.\nGiven a set of input data (\\(\\{x_i, y_i\\}\\)), the goal of linear regression is to find the values of m and b that best fit the data\nThe values of m and b are chosen to minimize the “sum of squared errors” (SSE) \\((\\sum (y - \\hat{y})^2)\\).\nTaking the partial derivatives with respect to m and b, set them equal to 0, and solve for m and b, we get:\nm = sum((x - x_mean) * (y - y_mean)) / sum((x - x_mean)^2)\nb = y_mean - m * x_mean\nMultiple linear regression is a more general form of linear regression that models the relationship between multiple independent variables and one dependent variable. The formula for the best-fit hyperplane in multiple linear regression is:\n\\(y = w_0 + w_1.x_1 + w_2.x_2 + ... + w_n.x_n = X^T. W\\)"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/linear_regression.html#code",
    "href": "insights/MachineLearningAlgorithm/linear_regression.html#code",
    "title": "Linear Regression",
    "section": "Code",
    "text": "Code\n\nSimple linear regression\nHere is a basic implementation of simple linear regression in Python using the least squares method:\n\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.slope = None\n        self.intercept = None\n\n    def fit(self, X, y):\n        n = len(X)\n        x_mean = np.mean(X)\n        y_mean = np.mean(y)\n        numerator = 0\n        denominator = 0\n        for i in range(n):\n            numerator += (X[i] - x_mean) * (y[i] - y_mean)\n            denominator += (X[i] - x_mean) ** 2\n        self.slope = numerator / denominator\n        self.intercept = y_mean - self.slope * x_mean\n\n    def predict(self, X):\n        y_pred = []\n        for x in X:\n            y_pred.append(self.slope * x + self.intercept)\n        return y_pred\n\n\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 5, 4, 5])\nlr = LinearRegression()\nlr.fit(X, y)\nprint(lr.slope)  # Output: 0.6\nprint(lr.intercept)  # Output: 2.2\ny_pred = lr.predict(X)\nprint(y_pred)  # Output: [2.8, 3.4, 4.0, 4.6, 5.2]\n\n\n# print(f\"The value of x is {x:.2f}\")\n\n0.6\n2.2\n[2.8000000000000003, 3.4000000000000004, 4.0, 4.6, 5.2]\n\n\n\n\nVectorized\n\\(y = X.W\\)\n$W = (XT.X){-1}X^T.y $\n\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.W = None\n\n    def fit(self, X, y):\n        '''\n        X: n x d \n        '''\n        # Add bias term to X -&gt; [1 X]\n        n = X.shape[0]\n        X = np.hstack([np.ones((n, 1)), X])\n        self.W = np.linalg.inv(X.T @ X) @ X.T @ y\n\n    def predict(self, X):\n        n = X.shape[0]\n        X = np.hstack([X, np.ones((n, 1))])\n        return X @ self.W\n    \n\n\n# Create example input data\nX = np.array([[2, 2], [4, 5], [7, 8]])\ny = np.array([9, 17, 26])\n\n# Fit linear regression model\nlr = LinearRegression()\nlr.fit(X, y)\nprint(lr.W) # [3. 1. 2.]\n\n# Make predictions on new data\nX_new = np.array([[10, 11], [13, 14]])\ny_pred = lr.predict(X_new)\nprint(y_pred)  # Output: [43. 55.]\n\n[3. 1. 2.]\n[43. 55.]\n\n\n\n\nImprovements\nhere are some improvements to the simple linear regression implementation to make it more robust:\n\nAdd input validation: Add input validation to check that the input arrays X and y have the same length and are not empty.\nUse NumPy broadcasting: Instead of looping through the data to calculate the numerator and denominator, we can use NumPy broadcasting to perform the calculations in a vectorized way. This will make the code faster and more efficient.\nAdd regularization: Regularization can help prevent overfitting by adding a penalty term to the cost function. One common regularization technique is L2 regularization, which adds the sum of squares of the coefficients to the cost function. This can be easily added to the code by adding a regularization parameter to the constructor.\nUse gradient descent: For large datasets, calculating the inverse of the matrix in the normal equation can be computationally expensive. To overcome this, we can use gradient descent to minimize the cost function. This can be implemented by adding a method that updates the coefficients iteratively using the gradient descent algorithm.\n\nHere’s the updated code that incorporates these improvements:\n\nimport numpy as np\n\n\nclass LinearRegressionGD:\n    def __init__(self, regul=0):\n        self.regul = regul\n        self.W = None\n\n    def fit(self, X, y, lr=0.01, num_iter=1000):\n        # Input validation\n        if len(X) != len(y) or len(X) == 0:\n            raise ValueError(\"X and y must have the same length and cannot be empty\")\n        \n        # Add bias term to X -&gt; [1 X]\n        X = np.hstack([np.ones((len(X), 1)), X])\n\n        # Initialize W to zeros\n        self.W = np.zeros(X.shape[1])\n\n        # Use gradient descent to minimize cost function\n        for i in range(num_iter):\n            # Calculate predicted values\n            y_pred = np.dot(X, self.W)\n\n            # Calculate cost function\n            cost = np.sum((y_pred - y) ** 2) + self.regul * np.sum(self.W ** 2)\n\n            # Calculate gradients\n            gradients = 2 * np.dot(X.T, (y_pred - y)) + 2 * self.regul * self.W\n\n            # Update W\n            self.W = self.W - lr * gradients\n\n            if (i % 1000 == 0 ): print(cost)\n\n    def predict(self, X):\n        # Add bias term to X\n        X = np.hstack([np.ones((len(X), 1)), X])\n\n        # Calculate predicted values\n        y_pred = np.dot(X, self.W)\n        return y_pred\n\n\n\nTest\n\nX = np.array([[1, 2, 3, 4, 5]]).T\ny = np.array([2, 4, 5, 4, 5])\nlr = LinearRegressionGD(regul=0.1)\nlr.fit(X, y, lr=0.01, num_iter=10000)\nprint(lr.W)  # Output: [ 1.99964292  0.65345474 ]\ny_pred = lr.predict(X)\nprint(y_pred)  # # Output: [2.65309766, 3.3065524, 3.96000714, 4.61346188, 5.26691662]\n\n86.0\n2.8791287270130335\n2.8791287270130344\n2.8791287270130344\n2.8791287270130344\n2.8791287270130344\n2.8791287270130344\n2.8791287270130344\n2.8791287270130344\n2.8791287270130344\n[1.99964292 0.65345474]\n[2.65309766 3.3065524  3.96000714 4.61346188 5.26691662]"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/linear_regression.html#visualize",
    "href": "insights/MachineLearningAlgorithm/linear_regression.html#visualize",
    "title": "Linear Regression",
    "section": "Visualize",
    "text": "Visualize\n\nimport matplotlib.pyplot as plt \n\n# Plot the data and the linear regression line\nplt.scatter(X, y, color='blue')\nplt.plot(X, y_pred, color='red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.show()"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/k_means_2.html",
    "href": "insights/MachineLearningAlgorithm/k_means_2.html",
    "title": "K-means with multi-dimensional data",
    "section": "",
    "text": "\\(X_{n \\times d}\\)\n\nimport numpy as np\nimport time\n\n\nn, d, k=1000, 20, 4\nmax_itr=100\n\n\nX=np.random.random((n,d))\n\n\\[ argmin_j  ||x_i - c_j||_2 \\]\n\ndef k_means(X, k):\n    #Randomly Initialize Centroids\n    np.random.seed(0)\n    C= X[np.random.randint(n,size=k),:]\n    E=np.float('inf')\n    for itr in range(max_itr):\n        \n        # Find the distance of each point from the centroids \n        E_prev=E\n        E=0\n        center_idx=np.zeros(n)\n        for i in range(n):\n            min_d=np.float('inf')\n            c=0\n            for j in range(k):\n                d=np.linalg.norm(X[i,:]-C[j,:],2)\n                if d&lt;min_d:\n                    min_d=d\n                    c=j\n            \n            E+=min_d\n            center_idx[i]=c\n            \n        #Find the new centers\n        for j in range(k):\n            C[j,:]=np.mean( X[center_idx==j,:] ,0)\n        \n        if itr%10==0:\n            print(E)\n        if E_prev==E:\n            break\n            \n    return C, E, center_idx\n\n\\[ argmin_j  ||x_i - c_j||_2 \\]\n\\[||x_i - c_j||_2 = \\sqrt{(x_i - c_j)^T (x_i-c_j)} = \\sqrt{x_i^T x_i -2 x_i^T c_j + c_j^T c_j} \\]\n\n$ diag(X~X^T)$, can be used to get \\(x_i^T x_i\\)\n$X~C^T $, can be used to get \\(x_i^T c_j\\)\n\\(diag(C~C^T)\\), can be used to get \\(c_j^T c_j\\)\n\n\ndef k_means_vectorized(X, k):\n    \n    #Randomly Initialize Centroids\n    np.random.seed(0)\n    C= X[np.random.randint(n,size=k),:]\n    E=np.float('inf')\n    for itr in range(max_itr):\n        # Find the distance of each point from the centroids \n        XX= np.tile(np.diag(np.matmul(X, X.T)), (k,1) ).T\n        XC=np.matmul(X, C.T)\n        CC= np.tile(np.diag(np.matmul(C, C.T)), (n,1)) \n\n        D= np.sqrt(XX-2*XC+CC)\n\n        # Assign the elements to the centroids:\n        center_idx=np.argmin(D, axis=1)\n\n        #Find the new centers\n        for j in range(k):\n            C[j,:]=np.mean( X[center_idx==j,:] ,0)\n\n        #Find the error\n        E_prev=E\n        E=np.sum(D[np.arange(n),center_idx])\n        if itr%10==0:\n            print(E)\n        if E_prev==E:\n            break\n    \n    return C, E, center_idx\n\n\nstart=time.time()\nC, E, center_idx = k_means(X, k)\nprint(time.time()-start,'seconds')\n\n1517.502248752696\n1218.91004301866\n1217.362137659097\n0.8816308975219727 seconds\n\n\n\nstart=time.time()\nC, E, center_idx = k_means_vectorized(X, k)\nprint(time.time()-start,'seconds')\n\n1517.502248752696\n1218.9100430186547\n1217.3621376590977\n0.09020209312438965 seconds"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/knn.html",
    "href": "insights/MachineLearningAlgorithm/knn.html",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "",
    "text": "K-Nearest Neighbors (KNN) is a classification algorithm that assigns a class label to a data point based on the class labels of its k nearest neighbors in the training set. It is a non-parametric algorithm, which means that it does not make any assumptions about the distribution of the data.\nThe KNN algorithm works as follows:\nThe distance between two data points can be computed using a distance metric such as Euclidean distance, Manhattan distance, or Minkowski distance. The choice of distance metric depends on the nature of the data and the problem at hand.\nOne important aspect of the KNN algorithm is the choice of the value of k. A small value of k will result in a more flexible decision boundary that can capture complex patterns in the data, but may also lead to overfitting. A large value of k will result in a smoother decision boundary that may not capture fine details in the data, but is less prone to overfitting. The value of k is typically chosen using cross-validation.\nKNN can also be used for regression tasks, where the goal is to predict a continuous value instead of a class label. In this case, the predicted value for a test point is the average of the values of its k nearest neighbors in the training set."
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/knn.html#code",
    "href": "insights/MachineLearningAlgorithm/knn.html#code",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Code",
    "text": "Code\nHere is an implementation of the KNN classifier in Python:\n\nfrom collections import Counter\nimport numpy as np\n\nclass KNN:\n    def __init__(self, k=3, distance='euclidean'):\n        self.k = k\n        self.distance = distance\n        \n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n        \n    def predict(self, X_test):\n        y_pred = []\n        for x in X_test:\n            # Compute distances between the test point and all training points\n            if self.distance == 'euclidean':\n                distances = np.linalg.norm(self.X_train - x, axis=1)\n            elif self.distance == 'manhattan':\n                distances = np.sum(np.abs(self.X_train - x), axis=1)\n            else:\n                distances = np.power(np.sum(np.power(np.abs(self.X_train - x), self.distance), axis=1), 1/self.distance)\n                \n            # Select the k nearest neighbors\n            nearest_indices = np.argsort(distances)[:self.k]\n            nearest_labels = self.y_train[nearest_indices]\n            \n            # Assign the class label that appears most frequently among the k nearest neighbors\n            label = Counter(nearest_labels).most_common(1)[0][0]\n            y_pred.append(label)\n        \n        return np.array(y_pred)\n\nThe KNN class has two main methods: fit and predict. The fit method takes the training data as input and stores it in instance variables. The predict method takes the test data as input and computes the class labels for each test point using the KNN algorithm.\nThe predict method computes the distances between the test point and all training points, selects the k nearest neighbors based on the distances, and assigns the class label that appears most frequently among the k nearest neighbors to the test point.\nThe distance parameter allows the user to choose the distance metric to use for computing distances. The default value is euclidean, but the user can also choose manhattan or any other value p for the Minkowski distance.\n\nTest\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the iris dataset\niris = load_iris()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\n# Create a KNN classifier with k=5 and euclidean distance\nknn = KNN(k=5, distance='euclidean')\n\n# Train the classifier on the training data\nknn.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = knn.predict(X_test)\n\n# Compute the accuracy of the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 1.0\n\n\n\n\nVisualization\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load the iris dataset\niris = load_iris()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(iris.data[:, :2], iris.target, test_size=0.2, random_state=42)\n\n# Create a KNN classifier with k=5 and euclidean distance\nknn = KNN(k=5, distance='euclidean')\n\n# Train the classifier on the training data\nknn.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = knn.predict(X_test)\n\n# Create scatter plots of the test data with colored points representing the true and predicted labels\nfig, ax = plt.subplots()\nscatter1 = ax.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='b', cmap='viridis', label=iris.target_names[0])\nscatter2 = ax.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='g', cmap='viridis', label=iris.target_names[1])\nscatter3 = ax.scatter(X_test[y_test==2, 0], X_test[y_test==2, 1], c='r', cmap='viridis', label=iris.target_names[2])\nscatter4 = ax.scatter(X_test[:, 0], X_test[:, 1], c='k', cmap='viridis', marker='x', label='Predicted Label')\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nax.set_title('KNN Classifier Results')\nhandles = [scatter1, scatter2, scatter3, scatter4]\nlabels = [h.get_label() for h in handles]\nax.legend(handles=handles, labels=labels)\nplt.show()"
  },
  {
    "objectID": "insights/DeepLearning/perceptron.html",
    "href": "insights/DeepLearning/perceptron.html",
    "title": "SOTA Insights",
    "section": "",
    "text": "The perceptron algorithm is a type of linear classification algorithm used to classify data into two categories. It is a simple algorithm that learns from the mistakes made during the classification process and adjusts the weights of the input features to improve the accuracy of the classification.\ny_pred = sign(w0 + w1*x1 + w2*x2 + ... + wn*xn)\nwi = wi + learning_rate * (target - y_pred) * xi\nHere is an implementation of the perceptron algorithm in Python:\n\nimport numpy as np\n\nclass Perceptron:\n    def __init__(self, lr=0.01, n_iter=100):\n        self.lr = lr\n        self.n_iter = n_iter\n\n    def fit(self, X, y):\n        self.weights = np.zeros(1 + X.shape[1])\n        self.errors = []\n\n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X, y):\n                update = self.lr * (target - self.predict(xi))\n                self.weights[1:] += update * xi\n                self.weights[0] += update\n                errors += int(update != 0.0)\n            self.errors.append(errors)\n        return self\n\n    def net_input(self, X):\n        return np.dot(X, self.weights[1:]) + self.weights[0]\n\n    def predict(self, X):\n        return np.where(self.net_input(X) &gt;= 0.0, 1, -1)\n\nThe Perceptron class has the following methods:\ninit(self, lr=0.01, n_iter=100): Initializes the perceptron with a learning rate (lr) and number of iterations (n_iter) to perform during training.\nfit(self, X, y): Trains the perceptron on the input data X and target labels y. The method initializes the weights to zero and iterates through the data n_iter times, adjusting the weights after each misclassification. The method returns the trained perceptron.\nnet_input(self, X): Computes the weighted sum of inputs and bias.\npredict(self, X): Predicts the class label for a given input X based on the current weights.\nTo use the perceptron algorithm, you can create an instance of the Perceptron class, and then call the fit method with your input data X and target labels y. Here is an example usage:\n\nX = np.array([[2.0, 1.0], [3.0, 4.0], [4.0, 2.0], [3.0, 1.0]])\ny = np.array([-1, 1, 1, -1])\nperceptron = Perceptron()\nperceptron.fit(X, y)\n\nnew_X = np.array([[5.0, 2.0], [1.0, 3.0]])\nperceptron.predict(new_X)\n\n\narray([-1,  1])"
  },
  {
    "objectID": "insights/DeepLearning/convolution.html",
    "href": "insights/DeepLearning/convolution.html",
    "title": "Convolution",
    "section": "",
    "text": "def convolve(signal, kernel):\n    output = []\n    kernel_size = len(kernel)\n    padding = kernel_size // 2 # assume zero padding\n    padded_signal = [0] * padding + signal + [0] * padding\n    \n    for i in range(padding, len(signal) + padding):\n        sum = 0\n        for j in range(kernel_size):\n            sum += kernel[j] * padded_signal[i - padding + j]\n        output.append(sum)\n    \n    return output\n\n\nsignal = [1, 2, 3, 4, 5, 6]\nkernel = [1, 0, -1]\noutput = convolve(signal, kernel)\nprint(output)\n\n[-2, -2, -2, -2, -2, 5]"
  },
  {
    "objectID": "insights/DeepLearning/convolution.html#d-convolution",
    "href": "insights/DeepLearning/convolution.html#d-convolution",
    "title": "Convolution",
    "section": "",
    "text": "def convolve(signal, kernel):\n    output = []\n    kernel_size = len(kernel)\n    padding = kernel_size // 2 # assume zero padding\n    padded_signal = [0] * padding + signal + [0] * padding\n    \n    for i in range(padding, len(signal) + padding):\n        sum = 0\n        for j in range(kernel_size):\n            sum += kernel[j] * padded_signal[i - padding + j]\n        output.append(sum)\n    \n    return output\n\n\nsignal = [1, 2, 3, 4, 5, 6]\nkernel = [1, 0, -1]\noutput = convolve(signal, kernel)\nprint(output)\n\n[-2, -2, -2, -2, -2, 5]"
  },
  {
    "objectID": "insights/DeepLearning/convolution.html#d-convolution-1",
    "href": "insights/DeepLearning/convolution.html#d-convolution-1",
    "title": "Convolution",
    "section": "3D convolution",
    "text": "3D convolution\n\nimport numpy as np\n\ndef convolution(image, kernel):\n    # get the size of the input image and kernel\n    (image_height, image_width, image_channels) = image.shape\n    (kernel_height, kernel_width, kernel_channels) = kernel.shape\n    \n    # calculate the padding needed for 'same' convolution\n    pad_h = (kernel_height - 1) // 2\n    pad_w = (kernel_width - 1) // 2\n    \n    # pad the input image with zeros\n    padded_image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w), (0, 0)), 'constant')\n    \n    # create an empty output tensor\n    output_height = image_height\n    output_width = image_width\n    output_channels = kernel_channels\n    output = np.zeros((output_height, output_width, output_channels))\n    \n    # perform the convolution operation\n    for i in range(output_height):\n        for j in range(output_width):\n            for k in range(output_channels):\n                output[i, j, k] = np.sum(kernel[:, :, k] * padded_image[i:i+kernel_height, j:j+kernel_width, :])\n    \n    return output\n\n\n# create an example image and kernel\nimage = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\nkernel = np.array([[[1, 0], [0, -1]], [[0, 1], [-1, 0]]])\n\n# perform the convolution operation\noutput = convolution(image, kernel)\n\nprint('Input image:')\nprint(image)\n\nprint('\\nKernel:')\nprint(kernel)\n\nprint('\\nOutput:')\nprint(output)\n\nInput image:\n[[[ 1  2]\n  [ 3  4]]\n\n [[ 5  6]\n  [ 7  8]]\n\n [[ 9 10]\n  [11 12]]]\n\nKernel:\n[[[ 1  0]\n  [ 0 -1]]\n\n [[ 0  1]\n  [-1  0]]]\n\nOutput:\n[[[-6.  2.]\n  [-2. -2.]]\n\n [[-6.  2.]\n  [-2. -2.]]\n\n [[-3.  1.]\n  [-1. -1.]]]"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAI Research Highlights in 2024\n\n\n\n\n\n\n\n\nJul 27, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Anushka Dhiman, a Computer Vision Engineer and an AI enthusiast interested in the area of AI driven application and researches. I have experience in developing cutting-edge AI systems, including applications in computer vision, natural language processing, and generative AI. I’ve worked on projects such as AI based monitoring and security system in Video Analytics and Survallience.\nMy journey into AI has been fueled by a passion for continuous learning, exploring groundbreaking technologies, and sharing knowledge to empower others in the field."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html",
    "href": "blog/AIResearchHighlightsIn2024/index.html",
    "title": "AI Research Highlights in 2024",
    "section": "",
    "text": "If you’re curious about Quarto, take a look at Tom Mock’s slide deck from rstudio::conf(2022) where he introduces Quarto, particularly for R Markdown users.\nTom’s slides can be found at rstd.io/quarto-curious."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#learn-more",
    "href": "blog/AIResearchHighlightsIn2024/index.html#learn-more",
    "title": "AI Research Highlights in 2024",
    "section": "Learn more",
    "text": "Learn more\nSee the conference schedule at rstudio.com/conference/2022/schedule/ for more Quarto talks on Day 2 of the conference!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn, explore, and connect.",
    "section": "",
    "text": "SOTA Insights is more than a blog; it’s a platform for curious minds. Whether you’re diving into the complexities of transformers, exploring NLP applications, or delving into computer vision, my aim is to guide you with well-explained articles, real-world use cases, and hands-on examples.\nAt SOTA Insights, you’ll find:\n\nInsights: Articles and tutorials on SOTA algorithms and architectures, complete with code and visualizations.\nBlogs: Updates on the latest trends, breakthroughs, and technologies in AI.\n\nSOTA Insights is just the beginning of my journey to explore, learn, and contribute to the AI ecosystem. I hope you’ll join me in this mission."
  },
  {
    "objectID": "insights/DeepLearning/feedforward.html",
    "href": "insights/DeepLearning/feedforward.html",
    "title": "Forward propagation:",
    "section": "",
    "text": "Z1 = X.W1 + b1\nA1 = ReLU(Z1)  \nZ2 = A1.W2 + b2\nexp_scores = exp(Z2)  \nprobs = exp_scores / sum(exp_scores)"
  },
  {
    "objectID": "insights/DeepLearning/feedforward.html#code",
    "href": "insights/DeepLearning/feedforward.html#code",
    "title": "Forward propagation:",
    "section": "Code",
    "text": "Code\n\nimport numpy as np\n\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.params = {}\n        self.params['W1'] = np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def forward(self, X):\n        W1, b1 = self.params['W1'], self.params['b1']\n        W2, b2 = self.params['W2'], self.params['b2']\n        z1 = np.dot(X, W1) + b1\n        a1 = np.maximum(0, z1) # ReLU activation function\n        z2 = np.dot(a1, W2) + b2\n        # probs = 1 / (1 + np.exp(-z2)) # Sigmoid activation function\n        exp_z = np.exp(z2)\n        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n        return probs\n\n    def loss(self, X, y):\n        probs = self.forward(X)\n        correct_logprobs = -np.log(probs[range(len(X)), y])\n        data_loss = np.sum(correct_logprobs)\n        return 1.0/len(X) * data_loss\n\n    def train(self, X, y, num_epochs, learning_rate=0.1):\n        for epoch in range(num_epochs):\n            # Forward propagation\n            z1 = np.dot(X, self.params['W1']) + self.params['b1']\n            a1 = np.maximum(0, z1) # ReLU activation function\n            z2 = np.dot(a1, self.params['W2']) + self.params['b2']\n            # probs = 1 / (1 + np.exp(-z2)) # Sigmoid activation function\n            exp_z = np.exp(z2)\n            probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n            # Backpropagation\n            delta3 = probs\n            delta3[range(len(X)), y] -= 1\n            dW2 = np.dot(a1.T, delta3)\n            db2 = np.sum(delta3, axis=0)\n            delta2 = np.dot(delta3, self.params['W2'].T) * (a1 &gt; 0) # derivative of ReLU\n            dW1 = np.dot(X.T, delta2)\n            db1 = np.sum(delta2, axis=0)\n\n            # Update parameters\n            self.params['W1'] -= learning_rate * dW1\n            self.params['b1'] -= learning_rate * db1\n            self.params['W2'] -= learning_rate * dW2\n            self.params['b2'] -= learning_rate * db2\n\n            # Print loss for monitoring training progress\n            if epoch % 100 == 0:\n                loss = self.loss(X, y)\n                print(\"Epoch {}: loss = {}\".format(epoch, loss))\n\nThis code defines a TwoLayerNet class with an initializer that takes the input size, hidden size, and output size as arguments. The weights and biases for the two layers are initialized randomly in this function.\nThe forward function takes an input X and performs the forward propagation to calculate the output probabilities for each class.\nThe loss function calculates the cross-entropy loss between the predicted probabilities and the true labels y.\nThe train function performs the backpropagation to update the weights and biases based on the calculated gradients. The number of training epochs and learning rate can be specified as arguments to this function.\n\nTest\nHere’s an example of how to use the TwoLayerNet class to train and test the network on a toy dataset:\n\n# Generate a toy dataset\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\n# Initialize a neural network\nnet = TwoLayerNet(input_size=2, hidden_size=10, output_size=2)\n\n# Train the neural network\nnet.train(X, y, num_epochs=1000)\n\n# Test the neural network\nprobs = net.forward(X)\npredictions = np.argmax(probs, axis=1)\nprint(\"Predictions: \", predictions)\n\nEpoch 0: loss = 0.8791617000548932\nEpoch 100: loss = 0.03272609589944909\nEpoch 200: loss = 0.010130354895034843\nEpoch 300: loss = 0.005517334222420798\nEpoch 400: loss = 0.0036701620853277555\nEpoch 500: loss = 0.002707635703438397\nEpoch 600: loss = 0.0021206045443387493\nEpoch 700: loss = 0.0017317523015295431\nEpoch 800: loss = 0.0014568091215886065\nEpoch 900: loss = 0.0012539964886349238\nPredictions:  [0 1 1 0]\n\n\n\n\nImprovements\nThere are several ways to improve the implementation of a two-layer neural network with softmax. Here are a few suggestions:\n\nWeight initialization: The current implementation initializes the weights randomly using a Gaussian distribution. However, it is recommended to use other weight initialization methods such as Xavier or He initialization to improve convergence and avoid vanishing or exploding gradients. One possible implementation for Xavier initialization of the weights is:\n\n\n# Xavier initialization\nself.params['W1'] = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\nself.params['W2'] = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n\n\nLearning rate decay: The learning rate is a hyperparameter that determines the step size at each iteration during training. However, using a fixed learning rate may lead to suboptimal performance or slow convergence. A common technique is to gradually decrease the learning rate over time, known as learning rate decay, to fine-tune the network weights as the optimization process progresses.\n\n\n# Learning rate decay\nlearning_rate = 0.1\nlr_decay = 0.95\nlr_decay_epoch = 100\nfor epoch in range(num_epochs):\n    # ...\n    if epoch % lr_decay_epoch == 0:\n        learning_rate *= lr_decay\n\n\nRegularization: Overfitting can occur when the model is too complex and the training data is limited. Regularization techniques such as L1 or L2 regularization can be applied to the loss function to prevent overfitting and improve the generalization performance of the model.\n\n\n# L2 regularization\nreg_lambda = 0.1\ndata_loss += 0.5 * reg_lambda * (np.sum(self.params['W1'] ** 2) + np.sum(self.params['W2'] ** 2))\n\n\nMini-batch training: The current implementation updates the weights using the entire training set at each iteration, which can be computationally expensive for large datasets. An alternative is to use mini-batch training, where a random subset of the training data is used at each iteration to update the weights. This can speed up the training process and improve convergence.\n\n\n# Mini-batch training\nbatch_size = 64\nnum_batches = len(X) // batch_size\nfor epoch in range(num_epochs):\n    for i in range(num_batches):\n        # Select a random batch of data\n        batch_mask = np.random.choice(len(X), batch_size)\n        X_batch = X[batch_mask]\n        y_batch = y[batch_mask]\n\n        # Forward and backward propagation using the batch data\n        # ...\n\n\nOptimization algorithm: The current implementation uses stochastic gradient descent (SGD) as the optimization algorithm. However, there are other optimization algorithms such as Adam, Adagrad, and RMSprop that can improve the convergence speed and performance of the network.\n\n\n# Adam optimization\nbeta1, beta2 = 0.9, 0.999\neps = 1e-8\nmW1, vW1 = 0, 0\nmW2, vW2 = 0, 0\nfor epoch in range(num_epochs):\n    # Forward and backward propagation\n    # ...\n    # Update parameters using Adam optimization\n    mW1 = beta1 * mW1 + (1 - beta1) * dW1\n    vW1 = beta2 * vW1 + (1 - beta2) * (dW1 ** 2)\n    mW2 = beta1 * mW2 + (1 - beta1) * dW2\n    vW2 = beta2 * vW2 + (1 - beta2) * (dW2 ** 2)\n    self.params['W1'] -= learning_rate * mW1 / (np.sqrt(vW1) + eps)\n    self.params['b1'] -= learning_rate * db1\n    self.params['W2'] -= learning_rate * mW2 / (np.sqrt(vW2) + eps)\n    self.params['b2'] -= learning_rate * db2\n\n\n\nOther extensions:\n\nArbitrary activation function\nArbitrary loss function\nExtension to multiple layers\n\n\n\nimport numpy as np\n\nclass ActivationFunction:\n    def __init__(self):\n        pass\n\n    def __call__(self, x):\n        raise NotImplementedError\n\n    def derivative(self, x):\n        raise NotImplementedError\n\nclass ReLU(ActivationFunction):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, x):\n        return np.maximum(0, x)\n\n    def derivative(self, x):\n        return (x &gt; 0).astype(float)\n\nclass Softmax(ActivationFunction):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, x):\n        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        return probs\n\n    def derivative(self, x):\n        raise NotImplementedError\n\nclass MultiLayerNet:\n    def __init__(self, input_size, hidden_sizes, output_size, activation_function, loss_function, reg_lambda=0.0):\n        self.params = {}\n        self.num_layers = 1 + len(hidden_sizes)\n        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n\n        for i in range(1, self.num_layers + 1):\n            self.params[f'W{i}'] = np.random.randn(self.layer_sizes[i-1], self.layer_sizes[i]) / np.sqrt(self.layer_sizes[i-1])\n            self.params[f'b{i}'] = np.zeros(self.layer_sizes[i])\n\n        self.activation_function = activation_function\n        self.activation_function_derivatives = [activation_function.derivative for _ in range(self.num_layers)]\n        self.loss_function = loss_function\n        self.reg_lambda = reg_lambda\n\n    def forward(self, X):\n        layer_output = X\n        self.layer_inputs = []\n        self.layer_outputs = [X]\n\n        for i in range(1, self.num_layers + 1):\n            W, b = self.params[f'W{i}'], self.params[f'b{i}']\n            layer_input = np.dot(layer_output, W) + b\n            self.layer_inputs.append(layer_input)\n            layer_output = self.activation_function(layer_input)\n            self.layer_outputs.append(layer_output)\n\n        return layer_output\n\n    def backward(self, X, y, output):\n        delta = output - y\n        dW = {}\n        db = {}\n        delta = delta / X.shape[0]\n\n        for i in reversed(range(1, self.num_layers + 1)):\n            layer_input = self.layer_inputs[i-1]\n            activation_derivative = self.activation_function_derivatives[i-1](layer_input)\n            dW[f'W{i}'] = np.dot(self.layer_outputs[i-1].T, delta) + self.reg_lambda * self.params[f'W{i}']\n            db[f'b{i}'] = np.sum(delta, axis=0)\n            delta = np.dot(delta, self.params[f'W{i}'].T) * activation_derivative\n\n        return dW, db\n\n    def loss(self, X, y, output):\n        data_loss = self.loss_function(output, y)\n        reg_loss = 0.0\n\n        for i in range(1, self.num_layers + 1):\n            reg_loss += 0.5 * self.reg_lambda * np.sum(self.params[f'W{i}'] ** 2)\n\n        total_loss = data_loss + reg_loss\n        return total_loss\n\n    def train(self, X, y, num_epochs, learning_rate=0.1):\n        for epoch in range(num_epochs):\n            # Forward propagation\n            output = self.forward(X)\n\n            # Backward propagation\n            dW, db = self.backward(X, y, output)\n\n            # Update parameters\n            for i in range(1, self.num_layers + 1):\n                self.params[f'W{i}'] -= learning_rate * dW[f'W{i}']\n                self.params[f'b{i}'] -= learning_rate * db[f'b{i}']\n\n            # Print loss for monitoring training progress\n            if epoch % 100 == 0:\n                loss = self.loss(X, y, output)\n                print(f\"Epoch {epoch}, loss: {loss}\")\n\n\n\n\n\nTest\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Generate a toy classification dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Normalize the input data\nmean = X_train.mean(axis=0)\nstd = X_train.std(axis=0)\nX_train = (X_train - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Define the mean squared error loss function\ndef mse_loss(output, y):\n    return np.mean((output - y) ** 2)\n\n# Create a multi-layer neural network with 2 hidden layers\nnet = MultiLayerNet(input_size=10, hidden_sizes=[20, 10], output_size=1,\n                    activation_function=Sigmoid(), loss_function=mse_loss, reg_lambda=0.01)\n\n# Train the network for 1000 epochs\nnet.train(X_train, y_train, num_epochs=1000, learning_rate=0.01)\n\n# Evaluate the trained network on the test set\noutput = net.forward(X_test)\npredicted_classes = np.round(output)\naccuracy = np.mean(predicted_classes == y_test)\nprint(f\"Test accuracy: {accuracy}\")"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/decision_tree.html",
    "href": "insights/MachineLearningAlgorithm/decision_tree.html",
    "title": "SOTA Insights",
    "section": "",
    "text": "A decision tree is a type of machine learning algorithm used for classification and regression tasks. It consists of a tree-like structure where each internal node represents a feature or attribute, each branch represents a decision based on that feature, and each leaf node represents a predicted output.\nTo train a decision tree, the algorithm uses a dataset with labeled examples to create the tree structure. It starts with the root node, which includes all the examples, and selects the feature that provides the most information gain to split the data into two subsets. It then repeats this process for each subset until it reaches a stopping criterion, such as a maximum tree depth or minimum number of examples in a leaf node.\nOnce the decision tree is trained, it can be used to predict the output for new, unseen examples. To make a prediction, the algorithm starts at the root node and follows the branches based on the values of the input features until it reaches a leaf node. The predicted output for that example is the value associated with the leaf node.\nDecision trees have several advantages, such as being easy to interpret and visualize, handling both numerical and categorical data, and handling missing values. However, they can also suffer from overfitting if the tree is too complex or if there is noise or outliers in the data.\nTo address this issue, various techniques such as pruning, ensemble methods, and regularization can be used to simplify the decision tree or combine multiple trees to improve generalization performance. Additionally, decision trees may not perform well with highly imbalanced datasets or datasets with many irrelevant features, and they may not be suitable for tasks where the relationships between features and outputs are highly nonlinear or complex.\n\nimport numpy as np\n\nclass DecisionTree:\n    def __init__(self, max_depth=None):\n        self.max_depth = max_depth\n        \n    def fit(self, X, y):\n        self.n_classes_ = len(np.unique(y))\n        self.n_features_ = X.shape[1]\n        self.tree_ = self._grow_tree(X, y)\n        \n    def predict(self, X):\n        return [self._predict(inputs) for inputs in X]\n        \n    def _gini(self, y):\n        _, counts = np.unique(y, return_counts=True)\n        impurity = 1 - np.sum([(count / len(y)) ** 2 for count in counts])\n        return impurity\n        \n    def _best_split(self, X, y):\n        m = y.size\n        if m &lt;= 1:\n            return None, None\n        \n        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n        best_idx, best_thr = None, None\n        \n        for idx in range(self.n_features_):\n            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n            num_left = [0] * self.n_classes_\n            num_right = num_parent.copy()\n            for i in range(1, m):\n                c = classes[i - 1]\n                num_left[c] += 1\n                num_right[c] -= 1\n                gini_left = 1.0 - sum(\n                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n                )\n                gini_right = 1.0 - sum(\n                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n                )\n                gini = (i * gini_left + (m - i) * gini_right) / m\n                if thresholds[i] == thresholds[i - 1]:\n                    continue\n                if gini &lt; best_gini:\n                    best_gini = gini\n                    best_idx = idx\n                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n        \n        return best_idx, best_thr\n        \n    def _grow_tree(self, X, y, depth=0):\n        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n        predicted_class = np.argmax(num_samples_per_class)\n        node = Node(predicted_class=predicted_class)\n        if depth &lt; self.max_depth:\n            idx, thr = self._best_split(X, y)\n            if idx is not None:\n                indices_left = X[:, idx] &lt; thr\n                X_left, y_left = X[indices_left], y[indices_left]\n                X_right, y_right = X[~indices_left], y[~indices_left]\n                node.feature_index = idx\n                node.threshold = thr\n                node.left = self._grow_tree(X_left, y_left, depth + 1)\n                node.right = self._grow_tree(X_right, y_right, depth + 1)\n        return node\n        \n    def _predict(self, inputs):\n        node = self.tree_\n        while node.left:\n            if inputs[node.feature_index] &lt; node.threshold:\n                node = node.left\n            else:\n                node = node.right\n        return node.predicted_class\n    \nclass Node:\n    def __init__(self, *, predicted_class):\n        self.predicted_class = predicted_class\n        self.feature_index = 0\n        self.threshold = 0.0 \n        self.left = None\n        self.right = None\n\n    def is_leaf_node(self):\n        return self.left is None and self.right is None\n\n\n\n\n\nTest\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the decision tree\ntree = DecisionTree(max_depth=3)\ntree.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = tree.predict(X_test)\n\n# Compute the accuracy of the predictions\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 1.0"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/k_means.html",
    "href": "insights/MachineLearningAlgorithm/k_means.html",
    "title": "K-means",
    "section": "",
    "text": "K-means clustering is a popular unsupervised machine learning algorithm used for grouping similar data points into k - clusters. Goal: to partition a given dataset into k (predefined) clusters.\nThe k-means algorithm works by first randomly initializing k cluster centers, one for each cluster. Each data point in the dataset is then assigned to the nearest cluster center based on their distance. The distance metric used is typically Euclidean distance, but other distance measures such as Manhattan distance or cosine similarity can also be used.\nAfter all the data points have been assigned to a cluster, the algorithm calculates the new mean for each cluster by taking the average of all the data points assigned to that cluster. These new means become the new cluster centers. The algorithm then repeats the assignment and mean calculation steps until the cluster assignments no longer change or until a maximum number of iterations is reached.\nThe final output of the k-means algorithm is a set of k clusters, where each cluster contains the data points that are most similar to each other based on the distance metric used. The algorithm is commonly used in various fields such as image segmentation, market segmentation, and customer profiling."
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/k_means.html#code",
    "href": "insights/MachineLearningAlgorithm/k_means.html#code",
    "title": "K-means",
    "section": "Code",
    "text": "Code\nHere’s an implementation of k-means clustering algorithm in Python from scratch:\n\nimport numpy as np\n\nclass KMeans:\n    def __init__(self, k, max_iterations=100):\n        self.k = k\n        self.max_iterations = max_iterations\n        \n    def fit(self, X):\n        # Initialize centroids randomly\n        self.centroids = X[np.random.choice(range(len(X)), self.k, replace=False)]\n        \n        for i in range(self.max_iterations):\n            # Assign each data point to the nearest centroid\n            cluster_assignments = []\n            for j in range(len(X)):\n                distances = np.linalg.norm(X[j] - self.centroids, axis=1)\n                cluster_assignments.append(np.argmin(distances))\n            \n            # Update centroids\n            for k in range(self.k):\n                cluster_data_points = X[np.where(np.array(cluster_assignments) == k)]\n                if len(cluster_data_points) &gt; 0:\n                    self.centroids[k] = np.mean(cluster_data_points, axis=0)\n            \n            # Check for convergence\n            if i &gt; 0 and np.array_equal(self.centroids, previous_centroids):\n                break\n            \n            # Update previous centroids\n            previous_centroids = np.copy(self.centroids)\n        \n        # Store the final cluster assignments\n        self.cluster_assignments = cluster_assignments\n    \n    def predict(self, X):\n        # Assign each data point to the nearest centroid\n        cluster_assignments = []\n        for j in range(len(X)):\n            distances = np.linalg.norm(X[j] - self.centroids, axis=1)\n            cluster_assignments.append(np.argmin(distances))\n        \n        return cluster_assignments\n\nThe KMeans class has an init method that takes the number of clusters (k) and the maximum number of iterations to run (max_iterations). The fit method takes the input dataset (X) and runs the k-means clustering algorithm. The predict method takes a new dataset (X) and returns the cluster assignments for each data point based on the centroids learned during training.\nNote that this implementation assumes that the input dataset X is a NumPy array with each row representing a single data point and each column representing a feature. The algorithm also uses Euclidean distance to calculate the distances between data points and centroids.\n\nTest\n\n\nx1 = np.random.randn(5,2) + 5\nx2 = np.random.randn(5,2) - 5\nX = np.concatenate([x1,x2], axis=0)\n\n# Initialize the KMeans object with k=3\nkmeans = KMeans(k=2)\n\n# Fit the k-means model to the dataset\nkmeans.fit(X)\n\n# Get the cluster assignments for the input dataset\ncluster_assignments = kmeans.predict(X)\n\n# Print the cluster assignments\nprint(cluster_assignments)\n\n# Print the learned centroids\nprint(kmeans.centroids)\n\n[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n[[-5.53443211 -5.13920695]\n [ 4.46522152  5.04931144]]\n\n\n\n\nVisualize\n\nfrom matplotlib import pyplot as plt\n# Plot the data points with different colors based on their cluster assignments\ncolors = ['r', 'b']\nfor i in range(kmeans.k):\n    plt.scatter(X[np.where(np.array(cluster_assignments) == i)][:,0], \n                X[np.where(np.array(cluster_assignments) == i)][:,1], \n                color=colors[i])\n\n# Plot the centroids as black circles\nplt.scatter(kmeans.centroids[:,0], kmeans.centroids[:,1], color='black', marker='o')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOptimization\nHere are some ways to optimize the k-means clustering algorithm:\nRandom initialization of centroids: Instead of initializing the centroids using the first k data points, we can randomly initialize them to improve the convergence of the algorithm. This can be done by selecting k random data points from the input dataset as the initial centroids.\nEarly stopping: We can stop the k-means algorithm if the cluster assignments and centroids do not change after a certain number of iterations. This helps to avoid unnecessary computation.\nVectorization: We can use numpy arrays and vectorized operations to speed up the computation. This avoids the need for loops and makes the code more efficient.\nHere’s an optimized version of the k-means clustering algorithm that implements these optimizations:\n\nimport numpy as np\n\nclass KMeans:\n    def __init__(self, k=3, max_iters=100, tol=1e-4):\n        self.k = k\n        self.max_iters = max_iters\n        self.tol = tol\n    \n    def fit(self, X):\n        # Initialize centroids randomly\n        self.centroids = X[np.random.choice(X.shape[0], self.k, replace=False)]\n        \n        # Iterate until convergence or maximum number of iterations is reached\n        for i in range(self.max_iters):\n            # Assign each data point to the closest centroid\n            distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n            cluster_assignments = np.argmin(distances, axis=1)\n            \n            # Update the centroids based on the new cluster assignments\n            new_centroids = np.array([np.mean(X[np.where(cluster_assignments == j)], axis=0) \n                                      for j in range(self.k)])\n            \n            # Check for convergence\n            if np.linalg.norm(new_centroids - self.centroids) &lt; self.tol:\n                break\n                \n            self.centroids = new_centroids\n    \n    def predict(self, X):\n        # Assign each data point to the closest centroid\n        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n        cluster_assignments = np.argmin(distances, axis=1)\n        \n        return cluster_assignments\n\nThis optimized version initializes the centroids randomly, uses vectorized operations for computing distances and updating the centroids, and checks for convergence after each iteration to stop the algorithm if it has converged.\nFollow ups:\n\nComputattional complexity: O(it * knd)\nImprove space: use index instead of copy\nImprove time:\n\ndim reduction\nsubsample (cons?)\n\nmini-batch\nk-median https://mmuratarat.github.io/2019-07-23/kmeans_from_scratch"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/k_nearest_neighbors.html",
    "href": "insights/MachineLearningAlgorithm/k_nearest_neighbors.html",
    "title": "K-nearest neighbour",
    "section": "",
    "text": "\\(X_{n \\times d}\\)\n\\(Y_{n \\times 1}\\)\n\\(Z_{m \\times d}\\)\n\nimport numpy as np\nimport time\nfrom collections import Counter\n\n\nn, d, m=500, 20, 4\nk=5\n\n\nX=np.random.random((n,d))\nZ=np.random.random((m,d))\nY=np.random.randint(3,size=n)\n\n\\[ argmin_i  ||x_i - z_j||_2 \\]\n\ndef KNN(X, Y, Z, k):\n    res=[]\n    for j in range(m):\n        d=np.zeros(n)\n        for i in range(n):\n            # Find the distance from each point \n            d[i]=np.linalg.norm(X[i,:]-Z[j,:], 2)\n\n        c=np.argsort(d)\n        label=Counter(Y[c[0:k]]).most_common()[0][0]\n        res.append(label)\n    return res\n\n\\[ argmin_j  ||x_i - z_j||_2 \\]\n\\[||x_i - z_j||_2 = \\sqrt{(x_i - z_j)^T (x_i-z_j)} = \\sqrt{x_i^T x_i -2 x_i^T z_j + z_j^T z_j} \\]\n\n$ diag(X~X^T)$, can be used to get \\(x_i^T x_i\\)\n$X~Z^T $, can be used to get \\(x_i^T z_j\\)\n\\(diag(Z~Z^T)\\), can be used to get \\(z_j^T z_j\\)\n\n\ndef KNN_vectorized(X, Y, Z, k):\n    \n    # Find the distance from each point \n    XX= np.tile(np.diag(np.matmul(X, X.T)), (m,1) ).T\n    XZ=np.matmul(X, Z.T)\n    ZZ= np.tile(np.diag(np.matmul(Z, Z.T)), (n,1)) \n    D= np.sqrt(XX-2*XZ+ZZ)\n    res=[]\n    for j in range(m):\n        c=np.argsort(D[:,j])\n        label=Counter(Y[c[0:k]]).most_common()[0][0]\n        res.append(label)\n    \n    return res\n\n\nstart=time.time()\nres = KNN(X, Y, Z, k)\nprint(time.time()-start,'seconds')\n\n0.022996902465820312 seconds\n\n\n\nstart=time.time()\nres = KNN_vectorized(X, Y, Z, k)\nprint(time.time()-start,'seconds')\n\n0.0029761791229248047 seconds"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/linear_regression_md.html",
    "href": "insights/MachineLearningAlgorithm/linear_regression_md.html",
    "title": "Linear regression python multi-dimensional data",
    "section": "",
    "text": "Linear Regression with two variables in one dimensional data\n\\[ F(X)=X \\times W \\] \\[ C=|| F(X) - Y ||_2^2 + \\lambda ||W||_2^2\\]\n\\(X_{n \\times k}\\)\n\\(W_{k \\times p}\\)\n\\(Y_{n \\times p}\\)\n\nimport numpy as np\nimport random\n\n\nn, k, p=100, 8, 3 \nX=np.random.random([n,k])\nW=np.random.random([k,p])\nY=np.random.random([n,p])\nmax_itr=1000\nalpha=0.0001\nLambda=0.01\n\nGradient is as follows: \\[ X^T 2 E + \\lambda 2 W\\]\n\n# F(x)= w[0]*x + w[1]\ndef F(X, W):\n    return np.matmul(X,W)\n\ndef cost(Y_est, Y, W, Lambda):\n    E=Y_est-Y\n    return E, np.linalg.norm(E,2)+ Lambda * np.linalg.norm(W,2)\n\ndef gradient(E,X, W, Lambda):\n    return 2* np.matmul(X.T, E) + Lambda* 2* W\n\n\ndef fit(W, X, Y, alpha, Lambda, max_itr):\n    for i in range(max_itr):\n        \n        Y_est=F(X,W)\n        E, c= cost(Y_est, Y, W, Lambda)\n        Wg=gradient(E, X, W, Lambda)\n        W=W - alpha * Wg\n        if i%100==0:\n            print(c)\n        \n    return W\n\nTo take into account for the biases, we concatenate X by a 1 column, and increase the number of rows in W by one\n\nX=np.concatenate( (X, np.ones((n,1))), axis=1 ) \nW=np.concatenate( (W, np.random.random((1,p)) ), axis=0 )\n\nW = fit(W, X, Y, alpha, Lambda, max_itr)\n\n34.3004759224227\n4.265835757989014\n4.052505749060854\n3.8807845759072968\n3.7422281683979812\n3.6303399157863434\n3.5398708528835554\n3.4665749938168915\n3.4070257924246747\n3.3584711183863862"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/logistic_regression_md.html",
    "href": "insights/MachineLearningAlgorithm/logistic_regression_md.html",
    "title": "logistic regression multi-dimensional data",
    "section": "",
    "text": "logistic regression multi-dimensional data\n\\[ F(X)=X \\times W \\] \\[ H(x)= \\frac{1}{1+ e ^{-F(x)}} \\] \\[ C= -\\frac{1}{n} \\sum_{i,j} (Y \\odot log(H(x)) + (1-Y) \\odot log(1-H(x)) ) \\]\n\\(X_{n \\times k}\\)\n\\(W_{k \\times p}\\)\n\\(Y_{n \\times p}\\)\n\nimport numpy as np\nimport random\n\n\nn, k, p=100, 8, 3 \n\n\nX=np.random.random([n,k])\nW=np.random.random([k,p])\n\ny=np.random.randint(p, size=(1,n))\nY=np.zeros((n,p))\nY[np.arange(n), y]=1\n\nmax_itr=5000\nalpha=0.01\nLambda=0.01\n\nGradient is as follows: \\[ X^T (H(x)-Y) + \\lambda 2 W\\]\n\n# F(x)= w[0]*x + w[1]\ndef F(X, W):\n    return np.matmul(X,W)\n\ndef H(F):\n    return 1/(1+np.exp(-F))\n\ndef cost(Y_est, Y):\n    E= - (1/n) * (np.sum(Y*np.log(Y_est) + (1-Y)*np.log(1-Y_est)))  + np.linalg.norm(W,2)\n    return E, np.sum(np.argmax(Y_est,1)==y)/n\n\ndef gradient(Y_est, Y, X):\n    return (1/n) * np.matmul(X.T, (Y_est - Y) ) + Lambda* 2* W\n\n\ndef fit(W, X, Y, alpha, max_itr):\n    for i in range(max_itr):\n        \n        F_x=F(X,W)\n        Y_est=H(F_x)\n        E, c= cost(Y_est, Y)\n        Wg=gradient(Y_est, Y, X)\n        W=W - alpha * Wg\n        if i%1000==0:\n            print(E, c)\n        \n    return W, Y_est\n\nTo take into account for the biases, we concatenate X by a 1 column, and increase the number of rows in W by one\n\nX=np.concatenate( (X, np.ones((n,1))), axis=1 ) \nW=np.concatenate( (W, np.random.random((1,p)) ), axis=0 )\n\nW, Y_est = fit(W, X, Y, alpha, max_itr)\n\n9.368653735228364 0.31\n4.994251188297815 0.43\n4.951873226767272 0.48\n4.922370610237865 0.47\n4.901694423284286 0.48"
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/svm.html",
    "href": "insights/MachineLearningAlgorithm/svm.html",
    "title": "Support Vector Machines (SVMs)",
    "section": "",
    "text": "Support Vector Machines (SVMs) are a type of machine learning algorithm used for classification and regression analysis. In particular, linear SVMs are used for binary classification problems where the goal is to separate two classes by a hyperplane.\nThe hyperplane is a line that divides the feature space into two regions. The SVM algorithm tries to find the hyperplane that maximizes the margin, which is the distance between the hyperplane and the closest points from each class. The points closest to the hyperplane are called support vectors and play a crucial role in the algorithm’s optimization process.\nIn linear SVMs, the hyperplane is defined by a linear function of the input features. The algorithm tries to find the optimal values of the coefficients of this function, called weights, that maximize the margin. This optimization problem can be formulated as a quadratic programming problem, which can be efficiently solved using standard optimization techniques.\nIn addition to finding the optimal hyperplane, SVMs can also handle non-linearly separable data by using a kernel trick. This technique maps the input features into a higher-dimensional space, where they might become linearly separable. The SVM algorithm then finds the optimal hyperplane in this transformed feature space, which corresponds to a non-linear decision boundary in the original feature space.\nLinear SVMs have been widely used in many applications, including text classification, image classification, and bioinformatics. They have the advantage of being computationally efficient and easy to interpret. However, they may not perform well in highly non-linearly separable datasets, where non-linear SVMs may be a better choice."
  },
  {
    "objectID": "insights/MachineLearningAlgorithm/svm.html#code",
    "href": "insights/MachineLearningAlgorithm/svm.html#code",
    "title": "Support Vector Machines (SVMs)",
    "section": "Code",
    "text": "Code\n\nimport numpy as np\n\nclass SVM:\n    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        y_ = np.where(y &lt;= 0, -1, 1)\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        # Gradient descent\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) &gt;= 1\n                if condition:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n                else:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n                    self.b -= self.lr * y_[idx]\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.w) - self.b\n        return np.sign(linear_output)\n\n\n\n\n# Example usage\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nX, y = datasets.make_blobs(n_samples=100, centers=2, random_state=42)\ny = np.where(y == 0, -1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsvm = SVM()\nsvm.fit(X_train, y_train)\ny_pred = svm.predict(X_test)\n\n\n# Evaluate model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 1.0\n\n\n\n# Generate data\nX, y = make_classification(n_features=5, n_samples=100, n_informative=5, n_redundant=0, n_classes=2, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n# Initialize SVM model\nsvm = SVM()\n\n# Train model\nsvm.fit(X_train, y_train)\n\n# Make predictions\ny_pred = svm.predict(X_test)\n\n# Evaluate model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.5"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html",
    "title": "Permutation and Combination",
    "section": "",
    "text": "The study of permutations and combinations is concerned with determining the number of different ways of arranging and selecting objects out of a given number of objects, without actually listing them. There are some basic counting techniques which will be useful in determining the number of different ways of arranging or selecting objects. The two basic counting principles are given below:"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#overview",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#overview",
    "title": "Permutation and Combination",
    "section": "",
    "text": "The study of permutations and combinations is concerned with determining the number of different ways of arranging and selecting objects out of a given number of objects, without actually listing them. There are some basic counting techniques which will be useful in determining the number of different ways of arranging or selecting objects. The two basic counting principles are given below:"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#fundamental-principle-of-counting",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#fundamental-principle-of-counting",
    "title": "Permutation and Combination",
    "section": "Fundamental principle of counting",
    "text": "Fundamental principle of counting\n1.1 Multiplication principle (Fundamental Principle of Counting)\nSuppose an event E can occur in m different ways and associated with each way of occurring of E, another event F can occur in n different ways, then the total number of occurrence of the two events in the given order is m × n .\n1.2 Addition principle If an event E can occur in m ways and another event F can occur in n ways, and suppose that both can not occur together, then E or F can occur in m + n ways.\n1.3 Permutations A permutation is an arrangement of objects in a definite order.\n1.4 Permutation of n different objects: The number of permutations of n objects taken all at a time, denoted by the symbol $ ^nP_n $ , is given by\nwhere $ {n!} $ = n(n – 1) (n – 2) … 3.2.1, read as factorial n, or n factorial.\nThe number of permutations of n objects taken r at a time, where 0 &lt; r ≤ n, denoted by $ ^nP_r $ , is given by\n\\[ ^nP_r = \\frac{n!}{(n-r)!} \\]\nwhere:\n- $ n $ is the number of elements in the set.\n- $ r $ is the number of elements taken together.\n1.5 When repetition of objects is allowed The number of permutations of n things taken all at a time, when repetion of objects is allowed is nn. The number of permutations of n objects, taken r at a time, when repetition of objects is allowed, is nr.\n1.6 Permutations when the objects are not distinct The number of permutations of n objects of which p1 are of one kind, p2 are of second kind, …, pk are of k th kind and the rest if any, are of different kinds is \\[ \\frac{n!}{(p1!*p2!*p3!..p)!} \\]\n1.7 Combination On many occasions we are not interested in arranging but only in selecting r objects from given n objects. A combination is a selection of some or all of a number of different objects where the order of selection is immaterial. The formula for calculating the number of combinations from $ n $ elements taken $ r $ at a time is given by: \\[^n C_r = \\frac{n!}{(n-r)! r!}\\]\nLet’s explore some examples to better understand these concepts."
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#permutation-and-combination-in-python",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#permutation-and-combination-in-python",
    "title": "Permutation and Combination",
    "section": "Permutation and Combination in Python",
    "text": "Permutation and Combination in Python\nThese methods can be found in itertools package.\nPermutation\nFirst import itertools package to implement the permutations method in python. This method takes a list as an input and returns an object list of tuples that contain all permutations in a list form.\nExample:\nPermutation taking 2 elements together from a set of 3 elements:\n\\[ ^3P_2 = \\frac{3!}{(3-2)!} = \\frac{3!}{1!} = \\frac{6}{1} = 6 \\]\nPermutation taking 3 elements together from a set of 3 elements:\n\\[ ^3P_3 = \\frac{3!}{(3-3)!} = \\frac{3!}{0!} = \\frac{6}{1} = 6 \\]\n\n# import permutations using itertools package\nfrom itertools import permutations \n\nchar_set = {'A', 'B', 'C'} # define a charater set of 3 elements\n\npermutations1 = permutations(char_set, 2) # permutations taking two elements\n\nfor i in permutations1:\n    print(i)\n\n('B', 'A')\n('B', 'C')\n('A', 'B')\n('A', 'C')\n('C', 'B')\n('C', 'A')\n\n\n\npermutations2 = permutations(char_set, 3) # permutations by taking two elements\nfor i in permutations2:\n    print(i)\n\n('B', 'A', 'C')\n('B', 'C', 'A')\n('A', 'B', 'C')\n('A', 'C', 'B')\n('C', 'B', 'A')\n('C', 'A', 'B')\n\n\nNow, let’s take a look at combinations\n\n# import permutations using itertools package\nfrom itertools import combinations \n\ncombination1 = combinations(char_set, 2) # combination taking two elements\nfor i in combination1:\n    print(i)\n\n('B', 'A')\n('B', 'C')\n('A', 'C')\n\n\n\ncombination2 = combinations(char_set, 3) # combination taking three elements\nfor i in combination2: \n    print(i)\n\n('B', 'A', 'C')\n\n\nIn addition to these, itertools provides two more functions:\n\nCombinations with Replacement: This function generates all possible combinations of $ r $ elements from a given iterable, allowing elements to be selected multiple times. It’s useful when repetitions are allowed in the selection process.\nProduct: This function computes the Cartesian product of input iterables. It generates all possible combinations where each element from one iterable is combined with every element from other iterables. It’s beneficial for creating all possible combinations of multiple sets of elements.\n\n\nfrom itertools import combinations_with_replacement \n\ncombination_with_replacement = combinations_with_replacement(char_set, 2) #combination taking two elements with replacement\nfor i in combination_with_replacement:\n    print(i)\n\n('B', 'B')\n('B', 'C')\n('B', 'A')\n('C', 'C')\n('C', 'A')\n('A', 'A')\n\n\n\nfrom itertools import product\n\nproduct_with_replacement = product(char_set, repeat=2) #product taking two elements with replacement\nfor i in product_with_replacement:\n    print(i)\n\n('B', 'B')\n('B', 'A')\n('B', 'C')\n('A', 'B')\n('A', 'A')\n('A', 'C')\n('C', 'B')\n('C', 'A')\n('C', 'C')"
  },
  {
    "objectID": "insights/StatisticsandProbability/03. Bayes Theorem.html",
    "href": "insights/StatisticsandProbability/03. Bayes Theorem.html",
    "title": "Bayes theorem",
    "section": "",
    "text": "Bayes’ theorem is a mathematical principle used to calculate the conditional probability of an event given some evidence related to that event. It establishes a relationship between the probability of an event and prior knowledge of conditions associated with it. As evidence accumulates, the probability of the event can be determined more accurately.\n\\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\n\\(P(A|B)\\), also known as the posterior probability, represents the probability of the hypothesis being true given the available data.\n\\(P(B|A)\\) is the probability of obtaining the evidence given the hypothesis.\n\\(P(A)\\) is the prior probability, representing the probability of the hypothesis being true before any data is considered.\n\\(P(B)\\) is the general probability of occurrence of the evidence, without any hypothesis, also known as the normalizing constant.\n\nExample: Fire and Smoke\nSuppose we want to find the probability of a fire given that there is smoke:\n\\[P(Fire|Smoke) =\\frac {P(Smoke|Fire) * P(Fire)}{P(Smoke)}\\]\nHere, - \\(P(Fire)\\) is the Prior - \\(P(Smoke|Fire)\\) is the Likelihood - \\(P(Smoke)\\) is the evidence\nExample:\nLet’s consider a scenario where an individual tests positive for an illness. This particular illness affects approximately 1.2% of the population at any given time. The diagnostic test for this illness has an accuracy of 85% for individuals who actually have the illness and 97% for those who do not.\nNow, let’s define the events involved:\n\n\\(A\\): The individual has the illness, also known as the hypothesis.\n\\(\\overline{A}\\): The individual does not have the illness.\n\\(B\\): The individual tests positive for the illness, also referred to as the evidence.\n\\(P(A|B)\\): The probability that the individual has the illness given a positive test result, known as the posterior probability, which is what we aim to calculate.\n\\(P(B|A)\\): The probability that the individual tests positive given that they have the illness, which is 0.85 according to the test’s accuracy.\n\\(P(A)\\): The prior probability or the likelihood of the individual having the illness without any evidence, which is 0.012 based on the prevalence of the illness in the population.\n\\(P(B)\\): The probability that the individual tests positive for the illness. This can be computed in two ways:\n\nTrue Positive (individual has the illness and tests positive): \\(P(B|A)*P(A)=0.85*0.012=0.0102.\\)\nFalse Positive (individual does not have the illness but tests positive due to test inaccuracy): \\(P(B|\\overline{A})*P(\\overline{A})=(1-0.97)*(1-0.012)=0.02964.\\)\n\nHere, \\(P(B|\\overline{A})\\) represents the probability of a positive test result for an individual who does not have the illness, indicating the test’s inaccuracy for those without the illness.\nAdditionally, \\(P(\\overline{A})\\) denotes the probability that the individual does not have the illness, which is derived from the complement of the illness prevalence.\nHence, \\(P(B)\\), the denominator in Bayes’ theorem, is the sum of these two probabilities:\n\\(P(B)= (P(B|A)*P(A)) + (P(B|\\overline{A})*P(\\overline{A}))=0.0102+0.2964=0.03984\\).\nWe can now compute the final answer using Bayes’ theorem formula:\n\\(P(A|B)=P(B|A)*P(A)/P(B) =0.85*0.012 / 0.03984 = 0.256\\).\nThus, even with a positive medical test, the individual only has a 25.6% chance of actually suffering from the illness."
  },
  {
    "objectID": "insights/StatisticsandProbability/03. Bayes Theorem.html#bayes-theorem",
    "href": "insights/StatisticsandProbability/03. Bayes Theorem.html#bayes-theorem",
    "title": "Bayes theorem",
    "section": "",
    "text": "Bayes’ theorem is a mathematical principle used to calculate the conditional probability of an event given some evidence related to that event. It establishes a relationship between the probability of an event and prior knowledge of conditions associated with it. As evidence accumulates, the probability of the event can be determined more accurately.\n\\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\n\\(P(A|B)\\), also known as the posterior probability, represents the probability of the hypothesis being true given the available data.\n\\(P(B|A)\\) is the probability of obtaining the evidence given the hypothesis.\n\\(P(A)\\) is the prior probability, representing the probability of the hypothesis being true before any data is considered.\n\\(P(B)\\) is the general probability of occurrence of the evidence, without any hypothesis, also known as the normalizing constant.\n\nExample: Fire and Smoke\nSuppose we want to find the probability of a fire given that there is smoke:\n\\[P(Fire|Smoke) =\\frac {P(Smoke|Fire) * P(Fire)}{P(Smoke)}\\]\nHere, - \\(P(Fire)\\) is the Prior - \\(P(Smoke|Fire)\\) is the Likelihood - \\(P(Smoke)\\) is the evidence\nExample:\nLet’s consider a scenario where an individual tests positive for an illness. This particular illness affects approximately 1.2% of the population at any given time. The diagnostic test for this illness has an accuracy of 85% for individuals who actually have the illness and 97% for those who do not.\nNow, let’s define the events involved:\n\n\\(A\\): The individual has the illness, also known as the hypothesis.\n\\(\\overline{A}\\): The individual does not have the illness.\n\\(B\\): The individual tests positive for the illness, also referred to as the evidence.\n\\(P(A|B)\\): The probability that the individual has the illness given a positive test result, known as the posterior probability, which is what we aim to calculate.\n\\(P(B|A)\\): The probability that the individual tests positive given that they have the illness, which is 0.85 according to the test’s accuracy.\n\\(P(A)\\): The prior probability or the likelihood of the individual having the illness without any evidence, which is 0.012 based on the prevalence of the illness in the population.\n\\(P(B)\\): The probability that the individual tests positive for the illness. This can be computed in two ways:\n\nTrue Positive (individual has the illness and tests positive): \\(P(B|A)*P(A)=0.85*0.012=0.0102.\\)\nFalse Positive (individual does not have the illness but tests positive due to test inaccuracy): \\(P(B|\\overline{A})*P(\\overline{A})=(1-0.97)*(1-0.012)=0.02964.\\)\n\nHere, \\(P(B|\\overline{A})\\) represents the probability of a positive test result for an individual who does not have the illness, indicating the test’s inaccuracy for those without the illness.\nAdditionally, \\(P(\\overline{A})\\) denotes the probability that the individual does not have the illness, which is derived from the complement of the illness prevalence.\nHence, \\(P(B)\\), the denominator in Bayes’ theorem, is the sum of these two probabilities:\n\\(P(B)= (P(B|A)*P(A)) + (P(B|\\overline{A})*P(\\overline{A}))=0.0102+0.2964=0.03984\\).\nWe can now compute the final answer using Bayes’ theorem formula:\n\\(P(A|B)=P(B|A)*P(A)/P(B) =0.85*0.012 / 0.03984 = 0.256\\).\nThus, even with a positive medical test, the individual only has a 25.6% chance of actually suffering from the illness."
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html",
    "title": "Probability distributions",
    "section": "",
    "text": "To comprehend probability distributions, it’s essential to begin with random variables, which serve as models for probability distributions in statistical analysis.\nA random variable represents a numerical outcome of a random process or a function that assigns values to each outcome of an experiment. Typically denoted by the symbol X, random variables are categorized as follows:\n\nDiscrete Random Variables: These variables can assume a finite, countable number of values. For instance, the result of a dice roll can be 1, 2, 3, 4, 5, or 6.\nContinuous Random Variables: These variables can take an infinite number of values. Examples include measurements like temperature, height, and weight.\n\nThe Probability Mass Function (PMF) of a discrete random variable is a function that yields the probability of the variable assuming a particular discrete value. However, for continuous variables, determining the absolute probability is impractical. Why? With continuous variables, the number of possible outcomes is infinite.\n\n\n\nThe graph of a probability mass function. All the values of this function must be non-negative and sum up to 1.\nConsider weight as an example. The weight of a person can take an infinite number of values, such as 25.0001 kgs, 25.0000003 kgs, and so on. Hence, it is not possible to calculate the absolute probability of a person’s weight being exactly 25 kgs, which would result in zero.\nTo overcome this limitation, we use the probability density function or PDF for continuous variables, which serves the same purpose as the probability mass function or PMF for discrete variables. The PDF represents the probability that the value of a continuous random variable falls within a given range of values.\nThe cumulative distribution function or CDF provides the probability of a random variable being less than or equal to a specified value. It is calculated as the integral of the PDF and represents the area under the curve of the PDF up to a certain point.\nThe typical types of probability distributions for discrete random variables include Binomial, Uniform, and Poisson."
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html#understanding-random-variables-and-probability-distributions",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html#understanding-random-variables-and-probability-distributions",
    "title": "Probability distributions",
    "section": "",
    "text": "To comprehend probability distributions, it’s essential to begin with random variables, which serve as models for probability distributions in statistical analysis.\nA random variable represents a numerical outcome of a random process or a function that assigns values to each outcome of an experiment. Typically denoted by the symbol X, random variables are categorized as follows:\n\nDiscrete Random Variables: These variables can assume a finite, countable number of values. For instance, the result of a dice roll can be 1, 2, 3, 4, 5, or 6.\nContinuous Random Variables: These variables can take an infinite number of values. Examples include measurements like temperature, height, and weight.\n\nThe Probability Mass Function (PMF) of a discrete random variable is a function that yields the probability of the variable assuming a particular discrete value. However, for continuous variables, determining the absolute probability is impractical. Why? With continuous variables, the number of possible outcomes is infinite.\n\n\n\nThe graph of a probability mass function. All the values of this function must be non-negative and sum up to 1.\nConsider weight as an example. The weight of a person can take an infinite number of values, such as 25.0001 kgs, 25.0000003 kgs, and so on. Hence, it is not possible to calculate the absolute probability of a person’s weight being exactly 25 kgs, which would result in zero.\nTo overcome this limitation, we use the probability density function or PDF for continuous variables, which serves the same purpose as the probability mass function or PMF for discrete variables. The PDF represents the probability that the value of a continuous random variable falls within a given range of values.\nThe cumulative distribution function or CDF provides the probability of a random variable being less than or equal to a specified value. It is calculated as the integral of the PDF and represents the area under the curve of the PDF up to a certain point.\nThe typical types of probability distributions for discrete random variables include Binomial, Uniform, and Poisson."
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html#binomial-distribution",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html#binomial-distribution",
    "title": "Probability distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nA binomial distribution describes the likelihood of obtaining a specified number of successes in a set number of trials of a binomial experiment.\nA binomial experiment is an experiment that follows these properties:\n\nThe experiment consists of \\(n\\) repeated trials.\nEach trial has two possible outcomes.\nThe probability of success (\\(p\\)) and failure (\\(1-p\\)) is the same for each trial.\nEach trial is independent of one another.\nFor example, consider tossing an unbiased coin \\(n\\) times. In this example, the probability that the outcome could be heads can be considered equal to \\(p\\), while \\(1-p\\) is the probability of tails. Each time the coin is tossed, the outcome is independent of all other trials.\n\nIf a random variable \\(X\\) follows a binomial distribution, the probability that \\(X\\) will equal \\(k\\) successes can be found by the following formula:\n\\[P(X=k) = ^n C_k p^k (1-p)^{n-k}\\]\nwhere:\n\\(p\\) is the probability of success, \\((1-p)\\) is the probability of failure\n\\(n\\) is the number of trials\nThe binomial distribution has the following characteristics:\nMean = \\(n*p\\) (number of trials * probability of success)\nVariance = \\(npq\\) (number of trials * probability of success * probimates, twenty percent (20%) of the country’s population has no health insurance.\nLet’s randomly sample \\(n=15\\) people. Let \\(X\\) denote the number in the sample with no health insurance.\nQuestion: What is the probability that exactly 3 of the 15 sampled have no health insurance?\nSolution: Calculating Binomial Probabilities\n\\[P(X=3) = ^{15}C_3 \\cdot (0.2)^3 \\cdot (0.8)^{12} = 0.25\\]\nThat is, there is a 25% chance, in sampling 15 random people, that we would find exactly 3 who have no health insurance.\n\nimport scipy.stats as stats\nn, r, p = 15, 3, 0.2\nstats.binom.pmf(r, n, p)  # Using PMF\n\n0.2501388953190401\n\n\nSecond part of the solution: Calculating Cumulative Binomial Probabilities\n“At most one” means either 0 or 1 of those sampled have no health insurance. That is, we need to find:\n\\[P(X\\leq1) = P(X=0)+P(X=1)\\]\nUsing the probability mass function for a binomial random variable with n=15 and p=0.2, we have\n\\[^{15}C_0 (0.2)^{0}(0.8)^{15} + ^{15}C_1 (0.2)^{1}(0.8)^{14} = 0.0352 + 0.1319 = 0.167\\]\nThat is, we have a 16.7% chance, in sampling 15 random people, that we would find at most one that had no health insurance.\n\nimport scipy.stats as stats\nn, r, p = 15, 1, 0.2\nstats.binom.cdf(r, n, p)  # Using CDF\n\n0.16712576742195212\n\n\n\nEffect of n and p on Shape\n\nFor small p and small n, the binomial distribution is what we call skewed right.\n\nFor large p and small n, the binomial distribution is what we call skewed left.\n\nFor p=0.5 and large and small n , the binomial distribution is what we call symmetric.\n\nFor small p and large n, the binomial distribution approaches symmetry.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nplt.figure(figsize=(5, 4))\nsns.set_style(\"whitegrid\")\n\nsample1 = np.random.binomial(n=15, p=0.2, size=250)\nsample2 = np.random.binomial(n=15, p=0.8, size=250)\nsample3 = np.random.binomial(n=15, p=0.5, size=250)\nsample4 = np.random.binomial(n=40, p=0.2, size=250)\n\nsns.kdeplot(sample1, label=\"sample1\")\nsns.kdeplot(sample2, label=\"sample2\")\nsns.kdeplot(sample3, label=\"sample3\")\nsns.kdeplot(sample4, label=\"sample4\")\n\nplt.legend(labels=[\"sample1\", \"sample2\", \"sample3\", \"sample4\"])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(5, 4))\nsns.set_style(\"whitegrid\")\n\nsample1 = np.random.binomial(n=15, p=0.2, size=250)\nsample2 = np.random.binomial(n=15, p=0.8, size=250)\nsample3 = np.random.binomial(n=15, p=0.5, size=250)\nsample4 = np.random.binomial(n=40, p=0.2, size=250)\n\nsns.kdeplot(sample1, label=\"sample1\")\nsns.kdeplot(sample2, label=\"sample2\")\nsns.kdeplot(sample3, label=\"sample3\")\nsns.kdeplot(sample4, label=\"sample4\")\n\nplt.legend(labels=[\"sample1\", \"sample2\", \"sample3\", \"sample4\"])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNOTE:\nThe Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution). It is also a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1.\n\\[P(n) = P^n (1-P)^{1-n}\\]"
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html#geometric-distribution",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html#geometric-distribution",
    "title": "Probability distributions",
    "section": "Geometric distribution",
    "text": "Geometric distribution\nThe geometric distribution describes the probability of experiencing a certain amount of failures before experiencing the first success in a series of Bernoulli trials. A Bernoulli trial is an experiment with only two possible outcomes – “success” or “failure” – and the probability of success is the same each time the experiment is conducted. An example of a Bernoulli trial is a coin flip. The coin can only land on two sides (we could call heads a “success” and tails a “failure”) and the probability of success on each flip is 0.5, assuming the coin is fair.\nIf a random variable X follows a geometric distribution, then the probability of experiencing k failures before experiencing the first success can be found by the following formula:\n\\[P(X=k) = (1-p)^kp\\]\nwhere:\n\\(k\\) is number of failures before first success\n\\(p\\) is probability of success on each trial\nFor example, suppose we want to know how many times we’ll have to flip a fair coin until it lands on heads. We can use the formula above to determine the probability of experiencing 0, 1, 2, 3 failures, etc. before the coin lands on heads:\nNote: The coin can experience 0 ‘failure’ if it lands on heads on the first flip.\n\\(P(X=0) = (1-.5)^0(.5) = 0.5\\)\n\\(P(X=1) = (1-.5)^1(.5) = 0.25\\)\n\\(P(X=2) = (1-.5)^2(.5) = 0.125\\)\n\\(P(X=3) = (1-.5)^3(.5) = 0.0625\\)"
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html#uniform-distribution",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html#uniform-distribution",
    "title": "Probability distributions",
    "section": "Uniform distribution",
    "text": "Uniform distribution\nThe uniform distribution is a probability distribution in which every value between an interval from \\(a\\) to \\(b\\) is equally likely to occur.\nIf a random variable \\(X\\) follows a uniform distribution, then the probability that \\(X\\) takes on a value between \\(x_1\\) and \\(x_2\\) can be found by the following formula:\n\\[P(x_1 &lt; X &lt; x_2) = \\frac{(x_2 – x_1)}{(b – a)}\\]\nwhere:\n\\(x_1\\): the lower value of interest\n\\(x_2\\): the upper value of interest\n\\(a\\): the minimum possible value\n\\(b\\): the maximum possible value\n\n\n\nFor example, suppose the weight of dolphins is uniformly distributed between 100 pounds and 150 pounds.\nIf we randomly select a dolphin at random, we can use the formula above to determine the probability that the chosen dolphin will weigh between 120 and 130 pounds:\n\\(P(120 &lt; X &lt; 130) = (130 – 120) / (150 – 100) = 10 / 50 = 0.2\\)\nThe probability that the chosen dolphin will weigh between 120 and 130 pounds is 0.2.\nProperties of the Uniform Distribution\nThe uniform distribution has the following properties:\n\nMean: (a + b) / 2\nMedian: (a + b) / 2\nStandard Deviation: √(b – a)2 / 12\nVariance: (b – a)2 / 12"
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html#poisson-distribution",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html#poisson-distribution",
    "title": "Probability distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nAgain, to understand the Poisson distribution, we first have to understand what Poisson experiments are.\nA Poisson experiment is an experiment that has the following properties:\n\nThe number of successes in the experiment can be counted.\nThe mean number of successes that occurs during a specific interval of time (or space) is known.\nEach outcome is independent.\nThe probability that a success will occur is proportional to the size of the interval\n\nOne example of a Poisson experiment is the number of births per hour at a given hospital. For example, suppose a particular hospital experiences an average of 10 births per hour. This is a Poisson experiment because it has the following four properties:\n\nThe number of successes in the experiment can be counted – We can count the number of births.\nThe mean number of successes that occurs during a specific interval of time is known – It is known that an average of 10 births per hour occur.\nEach outcome is independent – The probability that one mother gives birth during a given hour is independent of the probability of another mother giving birth.\nThe probability that a success will occur is proportional to the size of the interval – the longer the interval of time, the higher the probability that a birth will occur.\n\nWe can use the Poisson distribution to answer questions about probabilities regarding this Poisson experiment such as:\n\nWhat is the probability that more than 12 births occur in a given hour?\nWhat is the probability that less than 5 births occur in a given hour?\nWhat is the probability that between 8 to 11 births occur in a given hour?\n\nIf a random variable \\(X\\) follows a Poisson distribution, then the probability that \\(X = k\\) successes can be found by the following formula:\n\\[P(x=k)=\\frac{\\lambda^k e^{-\\lambda}}{k!}\\]\nwhere \\(P(x=k)\\) is the probability of the event occurring \\(k\\) number of times, \\(k\\) is the number of occurrences of the event, and \\(\\lambda\\) represents the mean number of event that occur during a specific interval.\nThe Poisson distribution can be used to calculate the number of occurrences that occur over a given period, for instance:\n+ number of arrivals at a restaurant per hour + number of work-related accidents occurring at a factory over a year + number of customer complaints at a call center in a week\nProperties of a Poisson distribution: 1. Mean=variance=\\(\\lambda\\). In a Poisson distribution, the mean and variance have the same numeric values.\n2. The events are independent, random, and cannot occur at the same time.\n\n\n\nThe horizontal axis is the index k, the number of occurrences. λ is the expected rate of occurrences. The vertical axis is the probability of k occurrences given λ. The function is defined only at integer values of k; the connecting lines are only guides for the eye.\nExample:\nIn a subway station, the average number of ticket-vending machines out of operation is two. Assuming that the number of machines out of operation follows a Poisson distribution, calculate the probability that a given point in time: 1. Exactly three machines are out of operation 2. More than two machines are out of operation\n\nimport scipy.stats as stats\nl, r = 2, 3\nstats.poisson.pmf(r,l) # probability mass function\n\n0.18044704431548356\n\n\n\nl, r = 2, 2\n1-stats.poisson.cdf(r,l) # cumlative distribution function\n\n0.3233235838169366"
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html#normal-distribution",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html#normal-distribution",
    "title": "Probability distributions",
    "section": "Normal distribution",
    "text": "Normal distribution\nA normal distribution is a symmetrical bell-shaped curve, defined by its mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\))\nCharacteristics of a normal distribution: 1. The central value (\\(\\mu\\)) is also the mode and the median for a normal distribution 2. Checking for normality: In a normal distribution, the difference between the 75th percentile value (\\(Q_3\\)) and the 50th percentile value (median or \\(Q_2\\)) equals the difference between the median (\\(Q_2\\)) and the 25th percentile(\\(Q_1\\)). In other words,\n\\[Q_3 - Q_2 = Q_2 - Q_1\\]\nIf the distribution is skewed, this equation does not hold.\n\nIn a right-skewed distribution, \\((Q_3 − Q_2)&gt; (Q_2 - Q_1)\\)\n\nIn a left-skewed distribution, \\((Q_2 - Q_1) &gt; (Q_3 - Q_2)\\)"
  },
  {
    "objectID": "insights/StatisticsandProbability/05. Probability distributions.html#standard-normal-distribution",
    "href": "insights/StatisticsandProbability/05. Probability distributions.html#standard-normal-distribution",
    "title": "Probability distributions",
    "section": "Standard normal distribution",
    "text": "Standard normal distribution\nTo standardize units and compare distributions with different means and variances, we use a standard normal distribution.\nProperties of a standard normal distribution: + The standard normal distribution is a normal distribution with a mean value of 0 and a standard deviation as 1. + Any normal distribution can be converted into standard normal distribution using the following formula:\n\\[z = \\frac {x-\\mu}{\\sigma}\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are the mean and variance of the original normal distribution.\nz-score (also called a standard score) gives you an idea of how far from the mean a data point is.\nIn a standard normal distribution,\n+ 68.2% of the values lie within 1 standard deviation of the mean + 95.4% of the values lie between 2 standard deviations of the mean + 99.8% lie within 3 standard deviations of the mean + The area under the standard normal distribution between any two points represents the proportion of values that lies between these two points. For instance, the area under the curve on either side of the mean is 0.5. Put in another way, 50% of the values lie on either side of the mean.\n\n\n\nThe standard normal distribution is a probability distribution, so the area under the curve between two points tells you the probability of variables taking on a range of values. The total area under the curve is 1 or 100%.\nEvery z-score has an associated p-value that tells you the probability of all values below or above that z-score occuring. This is the area under the curve left or right of that z-score."
  },
  {
    "objectID": "insights/StatisticsandProbability/07. Measures of Variability.html",
    "href": "insights/StatisticsandProbability/07. Measures of Variability.html",
    "title": "Measures of Variability",
    "section": "",
    "text": "Measures of dispersion provide a quantitative assessment of the spread within a distribution. They indicate whether the values are clustered around a central point or dispersed across a range. The following are the most commonly used measures of dispersion:\nRange: The range represents the difference between the highest and lowest values in a dataset.\nInterquartile Range (IQR): The IQR measures the difference between the third quartile (\\(Q_3\\)) and the first quartile (\\(Q_1\\)). It is less affected by extreme values, focusing on the middle portion of the dataset. This makes the IQR particularly useful for skewed distributions with outliers. The IQR is calculated as:\n\\[IQR = Q_3 - Q_1\\]\nVariance: Variance quantifies the extent to which the values in a dataset deviate from the mean. It provides an indication of whether the mean is a representative measure of central tendency. A small variance suggests that the mean is a good representation of the dataset. The formula for variance is:\n\\[\\sigma^2 = \\frac{\\sum (x-\\mu)^2}{N}\\]\nWhere \\(\\mu\\) is the mean, and \\(N\\) is the number of values in the dataset.\nSample Variance is given by:\n\\[S^2 = \\frac{\\sum (x-\\overline x)^2}{n-1}\\]\nWhere \\(\\overline x\\) is the sample mean, and \\(n\\) is the number of values in the sample.\nStandard deviation: This measure is calculated by taking the square root of the variance. Since the variance is not in the same units as the original data (it involves squaring the differences), taking the square root brings the standard deviation back to the same units as the data. For example, in a dataset measuring average rainfall in centimeters, the variance would be in \\(cm^2\\), which isn’t interpretable. However, the standard deviation, expressed in \\(cm\\), provides a meaningful indication of the average deviation of rainfall in centimeters.\nSkewness: This measures the degree of asymmetry of a distribution\n\n\n\nPositive Skewness: A positively skewed distribution is characterized by numerous outliers in the upper region, or right tail. It is termed “skewed right” due to its relatively elongated upper (right) tail.\nNegative Skewness: Conversely, a negatively skewed distribution exhibits a disproportionate number of outliers within its lower (left) tail. Such a distribution is referred to as “skewed left” owing to its extended lower tail.\nKurtosis: Kurtosis serves as a measure indicating the curvature, peakiness, or flatness of a given distribution of data.\n\n\n\n\nimport pandas as pd\ndata = pd.Series([19,23,19,18,25,16,17,19,15,23,21,23,21,11,6])\n\n\ndata.describe()\n\ncount    15.000000\nmean     18.400000\nstd       4.997142\nmin       6.000000\n25%      16.500000\n50%      19.000000\n75%      22.000000\nmax      25.000000\ndtype: float64\n\n\n\ndata.mode()\n\n0    19\n1    23\ndtype: int64\n\n\nThe values 19 and 23 are the most frequently occurring values\n\ndata.median()\n\n19.0\n\n\n\nrange_data = max(data)-min(data)\nrange_data\n\n19\n\n\n\ndata.std()\n\n4.99714204034952\n\n\n\ndata.var()\n\n24.97142857142857\n\n\n\nfrom scipy.stats import skew, kurtosis\n\nskew(data), kurtosis(data)\n\n(-1.038344732097918, 0.6995494033062934)\n\n\nPoints to note:\n1. The mean value is affected by outliers (extreme values). Whenever there are outliers in a dataset, it is better to use the median. 2. The standard deviation and variance are closely tied to the mean. Thus, if there are outliers, standard deviation and variance may not be representative measures too. 3. The mode is generally used for discrete data since there can be more than one modal value for continuous data."
  },
  {
    "objectID": "insights/StatisticsandProbability/09. Sampling and Sampling errors.html",
    "href": "insights/StatisticsandProbability/09. Sampling and Sampling errors.html",
    "title": "Sampling",
    "section": "",
    "text": "Sampling serves as a fundamental technique for acquiring insights into a population by gathering data from a representative subset, rather than assessing every individual within the population. It presents a pragmatic approach when exhaustive data collection proves impractical. However, it is imperative that the sample mirrors the population’s characteristics accurately."
  },
  {
    "objectID": "insights/StatisticsandProbability/09. Sampling and Sampling errors.html#types-of-errors-in-sampling",
    "href": "insights/StatisticsandProbability/09. Sampling and Sampling errors.html#types-of-errors-in-sampling",
    "title": "Sampling",
    "section": "Types of Errors in Sampling",
    "text": "Types of Errors in Sampling\nWhen making inferences about a population based on a sample, it’s possible to encounter various types of errors. These errors can be grouped into the following categories:\n\nSampling Error: The difference between the sample estimate for the population and the actual population estimate\nCoverage Error: Occurs when the population is not adequately represented and some groups are excluded\nNonresponse Error: Occurs when we fail to include nonresponsive subjects who meet the criteria of the study, but are excluded because they do not answer the survey questions.\nMeasurement Error: Occurs when the correct parameters are not measured due to flaws in the measurement method or tool used."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html",
    "title": "Parametric Tests",
    "section": "",
    "text": "Parametric tests are statistical tools predicated on the assumption that the data adheres to a normal distribution. They facilitate inferences about population parameters based on sampled data."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#one-sample-test",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#one-sample-test",
    "title": "Parametric Tests",
    "section": "One-Sample Test",
    "text": "One-Sample Test\nA one-sample test is employed when there’s a single population of interest, and a solitary sample is extracted from it. It evaluates whether there’s a notable discrepancy between the sample values and the population parameter."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#two-sample-test",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#two-sample-test",
    "title": "Parametric Tests",
    "section": "Two-Sample Test",
    "text": "Two-Sample Test\nThe two-sample test enters the picture when samples are drawn from two distinct populations. It gauges whether the population parameters diverge significantly based on the sample statistics."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#critical-test-statistic",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#critical-test-statistic",
    "title": "Parametric Tests",
    "section": "Critical Test Statistic",
    "text": "Critical Test Statistic\nThe critical test statistic denotes the threshold value of the sample test statistic pivotal in discerning whether to embrace or repudiate the null hypothesis."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#region-of-rejection",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#region-of-rejection",
    "title": "Parametric Tests",
    "section": "Region of Rejection",
    "text": "Region of Rejection\nThe region of rejection delineates the spectrum of values wherein the null hypothesis is discarded. Conversely, the region of acceptance encompasses values where the null hypothesis holds sway."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#types-of-tests",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#types-of-tests",
    "title": "Parametric Tests",
    "section": "Types of Tests",
    "text": "Types of Tests\nSeveral types of tests are at our disposal:\n\nZ-tests: Apt for ample sample sizes (n ≥ 30) with a known population standard deviation.\nT-tests: Tailored for modest sample sizes (n &lt; 30) with an unknown population standard deviation.\nF-tests: Tasked with comparing values across more than two variables.\nChi-square: Devised for the comparison of categorical data."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#one-tail-test-directional-test",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#one-tail-test-directional-test",
    "title": "Parametric Tests",
    "section": "One-Tail Test (Directional Test)",
    "text": "One-Tail Test (Directional Test)\nA one-tail test enters the fray when probing for a change in the mean, armed with the knowledge of the change’s direction.\nTwo iterations of the one-tail test exist:\n\nUpper one-tail: The region of rejection resides on the right tail. It’s invoked when scrutinizing whether the mean score has surged.\nLower one-tail: The region of rejection graces the left tail. It’s enlisted when assessing if the mean score has plummeted."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#two-tail-test-non-directional-test",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#two-tail-test-non-directional-test",
    "title": "Parametric Tests",
    "section": "Two-Tail Test (Non-Directional Test)",
    "text": "Two-Tail Test (Non-Directional Test)\nThe two-tail test is deployed when scrutinizing a change in the mean sans knowledge of the direction. The region of rejection spans both tails of the distribution."
  },
  {
    "objectID": "insights/StatisticsandProbability/11. Parametric Tests.html#the-p-value",
    "href": "insights/StatisticsandProbability/11. Parametric Tests.html#the-p-value",
    "title": "Parametric Tests",
    "section": "The P-value",
    "text": "The P-value\nThe p-value is the linchpin in deciding whether to embrace or eschew the null hypothesis. It’s computed based on the sample data and juxtaposed with a significance level, usually 0.05. + If p &lt; 0.05, it intimates that the sample data is improbable to stem from randomness and doesn’t mirror the population adequately. In such instances, the null hypothesis is jettisoned. + If p &gt; 0.05, it implies a heightened likelihood that the sample inadequately represents the population, prompting the null hypothesis’s retention\n\n\n\n\nfrom scipy.stats import norm\n\nz = 1.6448536269514722\np = 0.95\n\nprint(\"Calculating p given z: p = \", norm.cdf(z))\nprint(\"Calculating z given p: z = \", norm.ppf(p))\n\nCalculating p given z: p =  0.95\nCalculating z given p: z =  1.6448536269514722"
  },
  {
    "objectID": "insights/StatisticsandProbability/13. t-test.html",
    "href": "insights/StatisticsandProbability/13. t-test.html",
    "title": "T-Test",
    "section": "",
    "text": "In cases where the standard deviation of the population is not known and the sample size is small, the T-distribution is used. This distribution is also known as the “Student’s T distribution”.\nThe following are the key features of the T-distribution:\nThe formula for the critical test statistic in a one-sample t-test is given by the following equation:\n\\[t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}}\\]\nwhere \\(\\overline{x}\\) is the sample mean, \\(\\mu\\) is the population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size."
  },
  {
    "objectID": "insights/StatisticsandProbability/13. t-test.html#one-sample-t-test",
    "href": "insights/StatisticsandProbability/13. t-test.html#one-sample-t-test",
    "title": "T-Test",
    "section": "One-Sample T-Test",
    "text": "One-Sample T-Test\nA one-sample t-test is similar to a one-sample z-test, with the following differences:\n\nThe size of the sample is small (\\(&lt; 30\\)).\nThe population standard deviation is not known; we use the sample standard deviation (\\(s\\)) to calculate the standard error.\nThe critical statistic here is the t-statistic, given by the following formula:\n\n\\[t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}}\\]\nA coaching institute, preparing students for an exam, has 200 students, and the average score of the students in the practice tests is 80. It takes a sample of nine students and records their scores; it seems that the average score has now increased. These are the scores of these nine students: 80, 87, 80, 75, 79, 78, 89, 84, 88. Conduct a hypothesis test at a 5% significance level to verify if there is a significant increase in the average score."
  },
  {
    "objectID": "insights/StatisticsandProbability/13. t-test.html#hypotheses",
    "href": "insights/StatisticsandProbability/13. t-test.html#hypotheses",
    "title": "T-Test",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nNull hypothesis (\\(H_0\\)): \\(\\mu = 80\\)\nAlternative hypothesis (\\(H_1\\)): \\(\\mu &gt; 80\\)\n\n\nimport numpy as np\nimport scipy.stats as stats\n\nsample = np.array([80,87,80,75,79,78,89,84,88])\n\nstats.ttest_1samp(sample,80)\n\nTtestResult(statistic=1.348399724926488, pvalue=0.21445866072113726, df=8)\n\n\nSince the p-value is greater than 0.05, we fail to reject the null hypothesis. Hence, we cannot conclude that the average score of students has changed."
  },
  {
    "objectID": "insights/StatisticsandProbability/13. t-test.html#two-sample-t-test",
    "href": "insights/StatisticsandProbability/13. t-test.html#two-sample-t-test",
    "title": "T-Test",
    "section": "Two-sample t-test",
    "text": "Two-sample t-test\nA two-sample t-test is used when we take samples from two populations, where both the sample sizes are less than 30, and both the population standard deviations are unknown. Formula:\n\\[t = \\frac{\\overline x_1 - \\overline x_2}{\\sqrt{S_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\]\nWhere \\(x_1\\) and \\(x_2\\) are the sample means\nThe degrees of freedom: \\(df=n_1 + n_2 − 2\\)\nThe pooled variance \\(S_p^2 = \\frac{(n_1 -1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\\)\nA coaching institute has centers in two different cities. It takes a sample of ten students from each center and records their scores, which are as follows:\n\n\n\nCenter A:\n80, 87, 80, 75, 79, 78, 89, 84, 88\n\n\n\n\nCenter B:\n81, 74, 70, 73, 76, 73, 81, 82, 84\n\n\n\nConduct a hypothesis test at a 5% significance level, and verify if there a significant difference in the average scores of the students in these two centers.\n\\(H_0:\\mu_1 = \\mu_2\\)\n\\(H_1:\\mu_1 != \\mu_2\\)\n\na = np.array([80,87,80,75,79,78,89,84,88])\nb = np.array([81,74,70,73,76,73,81,82,84])\n\nstats.ttest_ind(a,b)\n\nTtestResult(statistic=2.1892354788555664, pvalue=0.04374951024120649, df=16.0)\n\n\nWe can conclude that there is a significant difference in the average scores of students in the two centers of the coaching institute since the p-value is less than 0.05"
  },
  {
    "objectID": "insights/StatisticsandProbability/13. t-test.html#two-sample-t-test-for-paired-samples",
    "href": "insights/StatisticsandProbability/13. t-test.html#two-sample-t-test-for-paired-samples",
    "title": "T-Test",
    "section": "Two-sample t-test for paired samples",
    "text": "Two-sample t-test for paired samples\nThis test is used to compare population means from samples that are dependent on each other, that is, sample values are measured twice using the same test group.\n\nA measurement taken at two different times (e.g., pre-test and post-test score with an intervention administered between the two time points)\nA measurement taken under two different conditions (e.g., completing a test under a “control” condition and an “experimental” condition)\n\nThis equation gives the critical value of the test statistic for a paired two-sample t-test:\n\\[t = \\frac{\\overline d}{s/\\sqrt{n}}\\]\nWhere \\(\\overline d\\) is the average of the difference between the elements of the two samples. Both the samples have the same size, \\(n\\).\nStandard deviation of the differences between the elements of the two samples, S = \\(\\sqrt{\\frac{\\sum d^2 -((\\sum d)^2/ n)}{n -1}}\\)\nThe coaching institute is conducting a special program to improve the performance of the students. The scores of the same set of students are compared before and after the special program. Conduct a hypothesis test at a 5% significance level to verify if the scores have improved because of this program.\n\na = np.array([80,87,80,75,79,78,89,84,88])\nb = np.array([81,89,83,81,79,82,90,82,90])\n\nstats.ttest_rel(a,b)\n\nTtestResult(statistic=-2.4473735525455615, pvalue=0.040100656419513776, df=8)\n\n\nWe can conclude, at a 5% significance level, that the average score has improved after the special program was conducted since the p-value is less than 0.05"
  },
  {
    "objectID": "insights/StatisticsandProbability/15. Chi-Square Test for Independence and Goodness of Fit.html",
    "href": "insights/StatisticsandProbability/15. Chi-Square Test for Independence and Goodness of Fit.html",
    "title": "Chi-square test for Independence",
    "section": "",
    "text": "The chi-square test is a nonparametric test for testing the association between two variables. A non-parametric test is one that does not make any assumption about the distribution of the population from which the sample is drawn.\nThe following are some of the characteristics of the chi-square test. + The chi-square test of association is used to test if the frequency of occurrence of one categorical variable is significantly associated with that of another categorical variable.\nThe chi-square test statistic is given by: \n\n$$\\chi^2 = \\sum\\frac {(f_o -f_e)^2}{f_e}$$\n\nwhere, $f_o$ denotes the observed frequencies, $f_e$ denotes the expected frequencies, and $\\chi$ is the test statistic.  \nUsing the chi-square test of association, we can assess if the differences between the frequencies are statistically significant.\n\nA contingency table is a table with frequencies of the variable listed under separate columns. The formula for the degrees of freedom in the chi-square test is given by: df=(r-1)(c-1), where df is the number of degrees of freedom, r is the number of rows in the contingency table, and c is the number of columns in the contingency table.\nThe chi-square test compares the observed values of a set of variables with their expected values. It determines if the differences between the observed values and expected values are due to random chance (like a sampling error), or if these differences are statistically significant. If there are only small differences between the observed and expected values, it may be due to an error in sampling. If there are substantial differences between the two, it may indicate an association between the variables.\n\n\n\n\n\nThe shape of the chi-square distribution for different values of k (degrees of freedom) When the degrees of freedom are few, it looks like an F-distribution. It has only one tail (toward the right). As the degrees of freedom increase, it looks like a normal curve. Also, the increase in the degrees of freedom indicates that the difference between the observed values and expected values could be meaningful and not just due to a sampling error.\n\nExample:\nSuppose there is a city of 1,000,000 residents with four neighborhoods: A, B, C, and D. A random sample of 650 residents of the city is taken and their occupation is recorded as “white collar”, “blue collar”, or “no collar”. The null hypothesis is that each person’s neighborhood of residence is independent of the person’s occupational classification. The data are tabulated as:\n\n\n\nOBSERVED\nA\nB\nC\nD\nRow Total\n\n\n\n\nWhite Collar\n90\n60\n104\n95\n349\n\n\nBlue Collar\n30\n50\n51\n20\n151\n\n\nNo Collar\n30\n40\n45\n35\n150\n\n\nColumn Total\n150\n150\n200\n150\n650\n\n\n\n\nNull hypothesis: \\(H_0\\): Occupation and Neighbourhood of Residence are not related.\nAlternative hypothesis: \\(H_1\\): Occupation and Neighbourhood of Residence are related.\nNumber of variables: Two categorical variables (Occupation and Neighbourhood)\nWhat we are testing: Testing for an association between Occupation and Neighbourhood.\nWe conduct a chi-square test of association based on the preceding characteristics.\nFix the level of significance: α=0.05\n\nMake an expected value table from the totals\nFor each entry calcuate : \\[\\frac{(row\\ total * column\\ total)}{overall\\ total}\\]\nExample: For A neighbourhood 150 * (349/650) must be the expected White collar Job.\n\n\n\nEXPECTED\nA\nB\nC\nD\n\n\n\n\nWhite Collar\n80.54\n80.54\n107.38\n80.54\n\n\nBlue Collar\n34.85\n34.85\n46.46\n34.85\n\n\nNo Collar\n34.62\n34.62\n46.15\n34.62\n\n\n\nEach of the value in the Expected Value table is 5 or higher. May proceed with Chi-Square test.\nCalculate: \\[\\chi^2 = \\sum\\frac {(f_o -f_e)^2}{f_e}\\]\n\\[\\chi^2\\  statistic\\  \\approx\\  24.6\\]\nUnder the null hypothesis, this sum has approximately a chi-squared distribution whose number of degrees of freedom is\ndof = (number of rows-1)(number of columns-1) = (3-1)(4-1) = 6\nFrom chi square distribution table p value less than 0.0005\n\nimport scipy.stats as stats\nimport numpy as np\n\nobservations = np.array([[90,60,104,95],[30,50,51,20],[30,40,45,35]])\nchi2stat, pval, dof, expvalue = stats.chi2_contingency(observations)\n\nprint(f'Chi-Square Statistic: ', chi2stat)\nprint(f'p-value: ', pval)\nprint(f'degrees of freedom: ', dof)\nprint(f'Expected Value: \\n', expvalue)\n\nChi-Square Statistic:  24.5712028585826\np-value:  0.0004098425861096696\ndegrees of freedom:  6\nExpected Value: \n [[ 80.53846154  80.53846154 107.38461538  80.53846154]\n [ 34.84615385  34.84615385  46.46153846  34.84615385]\n [ 34.61538462  34.61538462  46.15384615  34.61538462]]\n\n\np-value turns to be 0.0004 &lt; 0.05. Therefore we reject the null hypothesis. There is a significant association between the Occupation and Neighbourhood of Residence, at a 5% significance level.\n\nChi-Square Goodness of Fit Test:\nA Chi-Square goodness of fit test can be used in a wide variety of settings. Here are a few examples:\n\nWe want to know if a die is fair, so we roll it 50 times and record the number of times it lands on each number.\nWe want to know if an equal number of people come into a shop each day of the week, so we count the number of people who come in each day during a random week.\n\nIt is performed in a similar way.\nA shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, an independent researcher records the number of customers that come into the shop on a given week and finds the following:\n\n\n\nDay\nCustomers\n\n\n\n\nMonday\n50\n\n\nTuesday\n60\n\n\nWednesday\n40\n\n\nThursday\n47\n\n\nFriday\n53\n\n\n\n\\(H_0\\): An equal number of customers come into the shop each day.\n\\(H_1\\): An equal number of customers do not come into the shop each day.\nThere were a total of 250 customers that came into the shop during the week. Thus, if we expected an equal amount to come in each day then the expected value \\(E\\) for each day would be 50.\n\\(Monday: (50-50)^2 / 50 = 0\\)\n\\(Tuesday: (60-50)^2 / 50 = 2\\)\n\\(Wednesday: (40-50)^2 / 50 = 2\\)\n\\(Thursday: (47-50)^2 / 50 = 0.18\\)\n\\(Friday: (53-50)^2 / 50 = 0.18\\)\n\\(\\chi^2 = \\sum \\frac{(Obs-Exp)^2}{Exp} = 0 + 2 + 2 + 0.18 + 0.18 = 4.36\\)\nthe p-value associated with \\(\\chi^2\\) = 4.36 and degrees of freedom n-1 = 5-1 = 4 is 0.359472.\nSince this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html",
    "title": "\nTable of Contents\n",
    "section": "",
    "text": "Table of Contents"
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#the-process",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#the-process",
    "title": "\nTable of Contents\n",
    "section": "The process",
    "text": "The process\nThe basic process of statistical tests is the following : - Stating a Null Hypothesis (most often : “the two values are not different”) - Stating an Alternative Hypothesis (most often : “the two values are different”) - Defining an alpha value, which is a confidence level (most often : 95%). The higher it is, the harder it will be to validate the Alternative Hypothesis, but the more confident we will be if we do validate it. - Depending on data at disposal, we choose the relevant test (Z-test, T-test, etc… More on that later) - The test computes a score, which corresponds to a p-value. - If p-value is below 1-alpha (0.05 if alpha is 95%), we can accept the Alternative Hypothesis (or “reject the Null Hypothesis”). If it is over, we’ll have to stick with the Null Hypothesis (or “fail to reject the Null Hypothesis”).\nThere’s a built-in function for most statistical tests out there. Let’s also build our own function to summarize all the information. All tests we will conduct from now on are based on alpha = 95%.\n::: {#cell-9 .cell _uuid=‘781566a3f1a8b9465fad0818f57c86d9be026780’}\ndef results(p):\n    if(p['p_value']&lt;0.05):p['hypothesis_accepted'] = 'alternative'\n    if(p['p_value']&gt;=0.05):p['hypothesis_accepted'] = 'null'\n\n    df = pd.DataFrame(p, index=[''])\n    cols = ['value1', 'value2', 'score', 'p_value', 'hypothesis_accepted']\n    return df[cols]\n:::"
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-tailed-and-one-tailed",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-tailed-and-one-tailed",
    "title": "\nTable of Contents\n",
    "section": "Two-tailed and One-tailed",
    "text": "Two-tailed and One-tailed\nTwo-tails tests are used to show two values are just “different”. One-tail tests are used to show one value is either “larger” or “lower” than another one. This has an influence on the p-value : in case of one-tail tests, p-value has to be divided by 2.  Most of the functions we’ll use (those from the statweights modules) do that by themselves if we input the right information in the parameters. We’ll have to do it on our own with functions from the scipy module."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#types-of-tests",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#types-of-tests",
    "title": "\nTable of Contents\n",
    "section": "Types of tests",
    "text": "Types of tests\nThere are different types of tests, here are the ones we will cover : - T-tests. Used for small sample sizes (n&lt;30), and when population’s standard deviation is unknown. - Z-tests. Used for large sample sizes (n=&gt;30), and when population’s standard deviation is known. - F-tests. Used for comparing values of more than two variables. - Chi-square. Used for comparing categorical data."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#normal-distribution",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#normal-distribution",
    "title": "\nTable of Contents\n",
    "section": "Normal distribution",
    "text": "Normal distribution\nAlso, most tests - parametric tests - require a population that is normally distributed. It it not the case for SalePrice - which we’ll use for most tests - but we can fix this by log-transforming the variable. Note that to go back to our original scale and understand values vs. our $120 000, we’ll to exponantiate values back.\n::: {#cell-16 .cell _uuid=‘5e7f1cf9faba52350ec923638e9e26b9356764d7’ outputId=‘434155bc-55fd-454b-d9a2-c99dbbe5c315’}\nimport numpy as np\ncity_hall_dataset['SalePrice'] = np.log1p(city_hall_dataset['SalePrice'])\nlogged_budget = np.log1p(120000) #logged $120 000 is 11.695\nlogged_budget\n:::"
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-t-test-two-tailed-means",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-t-test-two-tailed-means",
    "title": "\nTable of Contents\n",
    "section": "One sample T-test | Two-tailed | Means",
    "text": "One sample T-test | Two-tailed | Means\nSo first question we want to ask is : How are our $120 000 situated vs. the average Ames house SalePrice?  In other words, is 120 000 (11.7 logged) any different from the mean SalePrice of the population? To know that from a 25 observations sample, we need to use a One Sample T-Test.\nNull Hypothesis : Mean SalePrice = 11.695  Alternative Hypothesis : Mean SalePrice ≠ 11.695 \n::: {#cell-23 .cell _uuid=‘b114d786b7657bb44b18cd10f3a75b59e8b76e4d’ outputId=‘994270a3-6585-436d-9f14-99dfe11e5c6f’}\np['value1'], p['value2'] = sample['SalePrice'].mean(), logged_budget\np['score'], p['p_value'] = stats.ttest_1samp(sample['SalePrice'], popmean=logged_budget)\nresults(p)\n:::\nSo we know our initial budget is significantely different from the mean SalePrice. From the table above, it unfortunately seems lower."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-t-test-one-tailed-means",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-t-test-one-tailed-means",
    "title": "\nTable of Contents\n",
    "section": "One sample T-test | One-tailed | Means",
    "text": "One sample T-test | One-tailed | Means\nLet’s make sure our budget is lower by running a one-tailed test. Question now is : is 120 000 (11.695 logged) lower than the mean SalePrice of the population?\nNull Hypothesis : Mean SalePrice &lt;= 11.695  Alternative Hypothesis : Mean SalePrice &gt; 11.695 \n::: {#cell-28 .cell _uuid=‘f5a2b2f062c437304675fb7f97cc75be030f7cd8’ outputId=‘96d56b97-7dd3-4cc5-edf0-722874dd5e98’}\np['value1'], p['value2'] = sample['SalePrice'].mean(), logged_budget\np['score'], p['p_value'] = stats.ttest_1samp(sample['SalePrice'], popmean=logged_budget)\np['p_value'] = p['p_value']/2 #one-tailed test (with scipy function), we need to divide p-value by 2 ourselves\nresults(p)\n:::\nUnfortunately it is! We have 95% chance of believing that our starting budget won’t let us buy a house at the average Ames price."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-t-test-two-tailed-means",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-t-test-two-tailed-means",
    "title": "\nTable of Contents\n",
    "section": "Two sample T-test | Two-tailed | Means",
    "text": "Two sample T-test | Two-tailed | Means\nNow that our expectations are lowered, we realize something important : The entire dataset probably contains some big houses fitted for entire families as well as small houses for fewer inhabitants. Prices are probably really different in-between the two types. And we are moving in alone, so we probably don’t need that big of a house. What if we could ask the City Hall to give us a sample for big houses, and a sample for smaller houses? We first could see if there is a significant difference in prices. And then see how our $120 000 are doing against the small houses average SalePrice. We do ask the City Hall, and because they understand it is also for the sake of this tutorial, they accept. They say they’ll split the dataset in two, based on the surface area of the houses. They will give us a sample from the top 50% houses in terms of surface, and another sample from the bottom 50%.\n::: {#cell-32 .cell _uuid=‘0ba8742a710640d9e15b7bb828d707b74c76fa9d’}\nsmaller_houses = city_hall_dataset.sort_values('GrLivArea')[:730].sample(n=25)\nlarger_houses = city_hall_dataset.sort_values('GrLivArea')[730:].sample(n=25)\n:::\nNow we first want to know if the two samples, extracted from two different populations, have significant differences in their average SalePrice.\nNull Hypothesis : SalePrice of smaller houses = SalePrice of larger houses  Alternative Hypothesis : SalePrice of smaller houses ≠ SalePrice of larger houses \n::: {#cell-35 .cell _uuid=‘063769aa739f591274cfb0c9ff3ca02cb3f47929’ outputId=‘fd3651ca-4b3c-44c5-d97d-8ba31b8bf057’}\np['value1'], p['value2'] = smaller_houses['SalePrice'].mean(), larger_houses['SalePrice'].mean()\np['score'], p['p_value'], p['df'] = ttest_ind(smaller_houses['SalePrice'], larger_houses['SalePrice'])\nresults(p)\n:::\nAs expected, the two samples show some significant differences in SalePrice."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-t-test-one-tailed-means",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-t-test-one-tailed-means",
    "title": "\nTable of Contents\n",
    "section": "Two sample T-test | One-tailed | Means",
    "text": "Two sample T-test | One-tailed | Means\nObviously, larger houses have a higher SalePrice. Let’s prove it this with one-tailed test.\nNull Hypothesis : SalePrice of smaller houses &gt;= SalePrice of larger houses  Alternative Hypothesis : SalePrice of smaller houses &lt; SalePrice of larger houses \n::: {#cell-40 .cell _uuid=‘27b5247cac5eb55e0635a066fcac0d9c33785567’ outputId=‘6eb1b4bd-90bb-480b-bd32-023d5dfbd24e’}\np['value1'], p['value2'] = smaller_houses['SalePrice'].mean(), larger_houses['SalePrice'].mean()\np['score'], p['p_value'], p['df'] = ttest_ind(smaller_houses['SalePrice'], larger_houses['SalePrice'], alternative='smaller')\nresults(p)\n:::\nStill as expected, SalePrice is significantly higher for larger houses."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-z-test-one-tailed-means",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-z-test-one-tailed-means",
    "title": "\nTable of Contents\n",
    "section": "Two sample Z-test | One-tailed | Means",
    "text": "Two sample Z-test | One-tailed | Means\nNow that the City Hall has already splitted the population in two, why not ask them for larger samples? We’ll pay a fee but that’s all right, this is fake money.\n::: {#cell-44 .cell _uuid=‘bb09dbc679b48df0861cc3fbbdcce8f7f41c2adb’}\nsmaller_houses = city_hall_dataset.sort_values('GrLivArea')[:730].sample(n=100, random_state=1)\nlarger_houses = city_hall_dataset.sort_values('GrLivArea')[730:].sample(n=100, random_state=1)\n:::\nNull Hypothesis : SalePrice of smaller houses &gt;= SalePrice of larger houses  Alternative Hypothesis : SalePrice of smaller houses &lt; SalePrice of larger houses \n::: {#cell-46 .cell _uuid=‘0378fa5d6d7df2e9f17a953f889e104bec2e0790’ outputId=‘bc5e303a-d6de-466a-d18f-54a0cbc91ed3’}\np['value1'], p['value2'] = smaller_houses['SalePrice'].mean(), larger_houses['SalePrice'].mean()\np['score'], p['p_value'] = ztest(smaller_houses['SalePrice'], larger_houses['SalePrice'], alternative='smaller')\nresults(p)\n:::\nHigher sample sizes show the same results : SalePrice is significantely higher for larger houses."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-z-test-one-tailed-proportions",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#two-sample-z-test-one-tailed-proportions",
    "title": "\nTable of Contents\n",
    "section": "Two sample Z-test | One-tailed | Proportions",
    "text": "Two sample Z-test | One-tailed | Proportions\nInstead of means, we can also run tests on proportions. Is the proportion of houses over $120 000 higher in the larger houses populations than in smaller houses population?\nNull Hypothesis : Proportion of smaller houses with SalePrice over 11.695 &gt;= Proportion of larger houses with SalePrice over 11.695  Alternative Hypothesis : Proportion of smaller houses with SalePrice over 11.695 &lt; Proportion of larger houses with SalePrice over 11.695 \n::: {#cell-51 .cell _uuid=‘6908a85072828db67c52eea41d821caf0b94cc56’ outputId=‘e8750c30-e44e-41e4-e9f2-2cae12d284a0’}\nfrom statsmodels.stats.proportion import *\nA1 = len(smaller_houses[smaller_houses.SalePrice&gt;logged_budget])\nB1 = len(smaller_houses)\nA2 = len(larger_houses[larger_houses.SalePrice&gt;logged_budget])\nB2 = len(larger_houses)\np['value1'], p['value2'] = A1/B1, A2/B2\np['score'], p['p_value'] = proportions_ztest([A1, A2], [B1, B2], alternative='smaller')\nresults(p)\n:::\nLogically, the test shows that the larger houses population has a higher ratio of houses sold over \\$120 000 vs. the smaller houses population."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-z-test-one-tailed-means",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-z-test-one-tailed-means",
    "title": "\nTable of Contents\n",
    "section": "One sample Z-test | One-tailed | Means",
    "text": "One sample Z-test | One-tailed | Means\nSo now let’s see how our $120 000 (11.7 logged) are doing against smaller houses only, based on the 100 observations sample.\nNull Hypothesis : Mean SalePrice of smaller houses =&gt; 11.695  Alternative Hypothesis : Mean SalePrice of smaller houses &lt; 11.695 \n::: {#cell-56 .cell _uuid=‘29042a1ab9f5558a694ba2b537f204d648c9229f’ outputId=‘622aee32-78c1-47fe-879f-2ecdedc566ce’}\np['value1'], p['value2'] = smaller_houses['SalePrice'].mean(), logged_budget\np['score'], p['p_value'] = ztest(smaller_houses['SalePrice'], value=logged_budget, alternative='larger')\nresults(p)\n:::\nThat’s quite depressing : our \\$120 000 do not even beat the average price of smaller houses."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-z-test-one-tailed-proportions",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#one-sample-z-test-one-tailed-proportions",
    "title": "\nTable of Contents\n",
    "section": "One sample Z-test | One-tailed | Proportions",
    "text": "One sample Z-test | One-tailed | Proportions\nOur $120 000 do not seem too far from the average SalePrice of small houses though. Let’s see if at least 25% of houses have a SalePrice in our budget.\nNull Hypothesis : Proportion of smaller houses with SalePrice under 11.695 &lt;= 25%  Alternative Hypothesis : Proportion of smaller houses with SalePrice under 11.695 &gt; 25% \n::: {#cell-61 .cell _uuid=‘d9bcda8bd0df8421f071c5fe0d38f07abde33549’ outputId=‘f5c5da9c-98c6-4678-8627-711b9055b243’}\nfrom statsmodels.stats.proportion import *\nA = len(smaller_houses[smaller_houses.SalePrice&lt;logged_budget])\nB = len(smaller_houses)\np['value1'], p['value2'] = A/B, 0.25\np['score'], p['p_value'] = proportions_ztest(A, B, alternative='larger', value=0.25)\nresults(p)\n:::\nSo at least, now we know we can buy a house among at least 25% of the smaller houses."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#f-test-anova",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#f-test-anova",
    "title": "\nTable of Contents\n",
    "section": "F-test (ANOVA)",
    "text": "F-test (ANOVA)\nThe House Price Dataset has a MSZoning variable, which identifies the general zoning classification of the house. For instance, it lets you know if the house is situated in a residential or a commerical zone. We’ll therefore try to know if there is a significant difference in SalePrice based on the zoning. And then know where we will be more likely to live with our budget. Based on the 100 observations samples of smaller houses, let’s first have an overview of mean SalePrice by zone.\n::: {#cell-65 .cell _uuid=‘5ad3bbf7148ee3ab852e1e86fe31a722a097fcf7’ outputId=‘50c47d05-1ec7-4a14-9888-210d9b99a413’}\nreplacement = {'FV': \"Floating Village Residential\", 'C (all)': \"Commercial\", 'RH': \"Residential High Density\",\n              'RL': \"Residential Low Density\", 'RM': \"Residential Medium Density\"}\nsmaller_houses['MSZoning_FullName'] = smaller_houses['MSZoning'].replace(replacement)\nmean_price_by_zone = smaller_houses.groupby('MSZoning_FullName')['SalePrice'].mean().to_frame()\nmean_price_by_zone\n:::\nTo know if there is a significant difference between these values, we run an ANOVA test. (because there a more than 2 values to compare) The test won’t not able to tell us what attributes are different from the others, but at least we’ll know if there is a difference or not.\nNull Hypothesis : No difference between SalePrice means  Alternative Hypothesis : Difference between SalePrice means \n::: {#cell-68 .cell _uuid=‘336c3d9d49d2b2ceaf514e2fbd3a4356e1787ba8’ outputId=‘a4a4807c-b568-483b-873b-899e62f8436a’}\nsh = smaller_houses.copy()\np['score'], p['p_value'] = stats.f_oneway(sh.loc[sh.MSZoning=='FV', 'SalePrice'], \n               sh.loc[sh.MSZoning=='C (all)', 'SalePrice'],\n               sh.loc[sh.MSZoning=='RH', 'SalePrice'],\n               sh.loc[sh.MSZoning=='RL', 'SalePrice'],\n               sh.loc[sh.MSZoning=='RM', 'SalePrice'],)\nresults(p)[['score', 'p_value', 'hypothesis_accepted']]\n:::\nThere is a difference between SalePrices based on where the house is located. Looking at the Average SalePrice by zone, Commerical Zones and Residential High Density zones seem to be the most affordable for our budget."
  },
  {
    "objectID": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#chi-square-test",
    "href": "insights/StatisticsandProbability/17.Statistical tests (Summarized).html#chi-square-test",
    "title": "\nTable of Contents\n",
    "section": "Chi-square test",
    "text": "Chi-square test\nOne last question we’ll address : can we get a garage? If yes, what type of garage? If not, then we won’t bother saving up for a car, and we’ll try to get a house next to Public Transportion. The dataset contains a categorical variable, GarageType, that will help us answer the question. \n::: {#cell-72 .cell _uuid=‘c143db864b4a24d0587b00099194f21457e22c29’ outputId=‘2811f9f3-d88a-41ec-d59b-b2538a4c40ba’}\nsmaller_houses['GarageType'].fillna('No Garage', inplace=True)\nsmaller_houses['GarageType'].value_counts().to_frame()\n:::\nWe know we can get a house in at least the bottom 25% of smaller houses. We would ideally like to know if distribution of Garage Types among these 25% is different than in the three other quarters We are now friends with the City Hall, so we can ask them one last favor :  Split the smaller houses population in 4 based on surface, and give us a sample of each quarter. Because we working here with categorical data, we’ll run a Chi-Square test.\n::: {#cell-74 .cell _uuid=‘a0462a0bd18442f27a88c5b0e267122e76ab34b7’ outputId=‘56ad970c-fe14-42da-bd74-c4ca45d8a955’}\ncity_hall_dataset['GarageType'].fillna('No Garage', inplace=True)\nsample1 = city_hall_dataset.sort_values('GrLivArea')[:183].sample(n=100)\nsample2 = city_hall_dataset.sort_values('GrLivArea')[183:366].sample(n=100)\nsample3 = city_hall_dataset.sort_values('GrLivArea')[366:549].sample(n=100)\nsample4 = city_hall_dataset.sort_values('GrLivArea')[549:730].sample(n=100)\ndff = pd.concat([\n    sample1['GarageType'].value_counts().to_frame(),\n    sample2['GarageType'].value_counts().to_frame(), \n    sample3['GarageType'].value_counts().to_frame(), \n    sample4['GarageType'].value_counts().to_frame()], \n    axis=1, sort=False)\ndff.columns = ['Sample1 (smallest houses)', 'Sample2', 'Sample3', 'Sample4 (largest houses)']\ndff = dff[:3] #chi-square tests do not work when table contains some 0, we take only the most frequent attributes\ndff \n:::\nNull Hypothesis : No difference between GarageType distribution  Alternative Hypothesis : Difference between GarageType distribution \n::: {#cell-76 .cell _uuid=‘59e533a57eebfb818a880a00c5eb7b668ddfedf7’ outputId=‘188aca50-8077-4402-c7ef-629e6864b641’}\np['score'], p['p_value'], p['ddf'], p['contigency'] = stats.chi2_contingency(dff)\np.pop('contigency')\nresults(p)[['score', 'p_value', 'hypothesis_accepted']]\n:::\nClearly there’s a difference in GarageType distribution according to size of houses. The sample that concerns us, Sample1, has the highest proportion of “No Garage” and “Detached Garage”. We’ll probably have to stick with Public Transportation."
  }
]