[
  {
    "objectID": "insights.html",
    "href": "insights.html",
    "title": "Insights",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPermutation and Combination\n\n\n\n\n\n\n\n\nJan 7, 2025\n\n\nAnushka Dhiman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn, explore, and connect.",
    "section": "",
    "text": "At SOTA Insights, you’ll find:\n\nInsights: Articles and tutorials on SOTA algorithms and architectures, complete with code and visualizations.\nBlogs: Updates on the latest trends, breakthroughs, and technologies in AI.\n\nSOTA Insights is just the beginning of my journey to explore, learn, and contribute to the AI ecosystem. I hope you’ll join me in this mission."
  },
  {
    "objectID": "blog/whats_next_in_ai/index.html",
    "href": "blog/whats_next_in_ai/index.html",
    "title": "What’s next in AI: A Guide to 2025",
    "section": "",
    "text": "While 2024 has been a year of incredible developments in artificial intelligence, and more is to come. From generative AI and large language models (LLMs) to multimodal AI, agentic AI, and quantum AI, we are excited to watch how AI will develop in coming years. Workflows across sectors will be further accelerated by these technologies, bringing about revolutionary change and opening up new opportunities.\nAs AI develops further, scientists are concentrating on improving fundamental ideas, creating new applications, and resolving the shortcomings of existing models.\nThe following key fields of AI research are anticipated to have substantial advancements:"
  },
  {
    "objectID": "blog/whats_next_in_ai/index.html#multimodal-ai",
    "href": "blog/whats_next_in_ai/index.html#multimodal-ai",
    "title": "What’s next in AI: A Guide to 2025",
    "section": "Multimodal AI",
    "text": "Multimodal AI\nMultimodal AI is a type of artificial intelligence that can understand and generate multiple data inputs simultaneously, including text, images, and sound. It is trained on large datasets to learn relationships between different modalities and fuse them effectively. It can perform tasks like image captioning, text-to-image generation, video understanding, human-computer interaction, and robot interaction, offering significant potential for real-world applications.\nHere are some examples,\nChatGPT offers multimodal capabilities, allowing users to interact with the chatbot in various ways. Users can upload images as prompts, which the chatbot uses to generate responses. Voice input is also available for hands-free tasks, and responses can be generated in one of five natural-sounding voices. Additionally, ChatGPT Plus and Enterprise users can generate images from text descriptions directly within the ChatGPT interface with the DALL-E GPT.\nGoogle Gemini is gaining popularity in the AI space, in certain areas due to its fresh data. Gemini integrates with various data sources like Google Flights, Maps, Hotels, Workspace, and YouTube. Its dynamic communication with tools like Maps and Hotels allows for real-time updates on queries related to those topics.\n\nUse Cases:\nPersonalized Nutrition: The USDA’s MyPlate allows individuals to plan their meals based on various factors such as age, sex, weight, height, and physical activity level. Future plans could include additional data like blood test results, health questionnaires, genotype, or microbiome. Important features need to be identified using large clinical studies with advanced feature selection methods. The USDA Nutritional Penotyping study includes over 5000 variables from self-reporting diet questionnaires and blood tests, and millions from high-throughput “omics” analyses. ML analyses of this data can serve two purposes: developing a mechanistic understanding of the relationship between diet and health and developing personalized nutrition models to predict health effects.\nMedical Images: M-LLMs are advanced medical imaging tools that can improve diagnostic accuracy and efficiency in various image modalities like x-rays, MRI scans, CT scans, positron emission tomography scans, ultrasound images, digital pathology slides, and retinal images. These tools provide unique insights into the body’s internal structures, aiding in early disease detection, diagnosis, and monitoring. For example, in radiology, M-LLMs analyze CT and MRI images to identify anomalies and generate automated reports. They also allow annotation and tagging of medical images with keywords, enabling additional analytics applications. In pathology, they interpret tissue sample slides to identify disease markers. In dermatology, they assess skin lesions for early detection of skin cancers.\nAI Personalised Nutritions"
  },
  {
    "objectID": "blog/whats_next_in_ai/index.html#agentic-ai",
    "href": "blog/whats_next_in_ai/index.html#agentic-ai",
    "title": "What’s next in AI: A Guide to 2025",
    "section": "Agentic AI",
    "text": "Agentic AI\nAgentic AI is a form of AI that emphasizes autonomy. This implies that it is capable of making decisions, taking action, and even learning on its own to accomplish particular objectives. It’s similar to having a virtual assistant that can reason, think, and adjust to new conditions without continual guidance.\nThere are four main phases to agentic AI operation:\nPerception: It collects information from its surroundings.\nReasoning: It analyses this information to determine what is happening.\nAction: It uses its comprehension to determine what to do.\nLearning: It gains knowledge from experience and criticism as it develops and changes throughout time.\nBecause of this, agentic AI is extremely independent and capable of managing challenging tasks involving logic, problem-solving, and situational adaptation.\n\nUse Cases:\nHealthcare: AI agents can extract important information from massive volumes of patient and medical data to assist physicians in making more educated decisions about patient care. Doctors can concentrate on building relationships with their patients by automating administrative duties and taking clinical notes during patient consultations.\nAutonomous cars: In autonomous cars, an agentic AI system not only recognizes objects but also forecasts how other drivers and pedestrians will behave. It makes decisions in real time, plans routes, and avoids barriers using this information. Agentic AI systems exhibit a degree of “intelligent agency” that surpasses data processing by fusing perception and action, giving them a special capability.\nReal-Time Threat Detection: Agentic AI systems can detect real-time threats by analyzing video streams to identify anomalies like unauthorized access or unusual movement patterns. They can autonomously recognize trespassers and alert security personnel, integrating object detection and behavioural analysis for accurate and timely interventions."
  },
  {
    "objectID": "blog/whats_next_in_ai/index.html#explainable-ai",
    "href": "blog/whats_next_in_ai/index.html#explainable-ai",
    "title": "What’s next in AI: A Guide to 2025",
    "section": "Explainable AI",
    "text": "Explainable AI\nThe goal of the developing field of explainable AI is to provide intuitive explanations for AI model behaviour. The intention is to assist users comprehend how an AI model generates results or makes predictions. An AI system whose core operations are hidden from the user is known as an AI black box. The AI model doesn’t give any justification for its decisions.\nFor instance, you might not understand why an AI model accurately classifies an image. Scientists and engineers dislike “black boxes.” They’re curious about what’s going on within the box. Explainable AI seeks to reveal mysteries. Explainability strategies aid in demonstrating the rationale behind and possibly the process by which the AI model arrived at its conclusion.\nExplainable AI techniques, such as Grad-CAM, LIME, and occlusion sensitivity, are useful when using models that are not inherently explainable, such as deep learning models. These techniques help users gain confidence in AI decisions and can be useful when comparing the performance of multiple models. The mapping of important features slightly differs between explainability techniques due to the underlying methodology. LIME approximates complex model behaviour using a simpler model, while Grad-CAM uses gradients of classification score to determine the importance of features in the deep learning model. Occlusion sensitivity computes a map of the change in activation when parts of the input are occluded with a mask.\nAs AI technology advances, it is increasingly used to solve real-world problems, and explainability techniques are crucial to ensure trust in the decisions of the model and its functionality.\n\nUse Cases:\nOptimize Clinical Decision Support: The study aimed to improve alert criteria using explainable artificial intelligence (XAI) approaches. Data was extracted from Vanderbilt University Medical Center alerts from 2019 to 2020, and machine learning models were developed to predict user responses. XAI techniques were applied to generate global and local explanations. The final dataset included 2,991–823 firings with 2689 features. The LightGBM model achieved the highest Area under the ROC Curve, identifying 96 helpful suggestions. A total of 278–807 firings (9.3%) could have been eliminated. Some suggestions also revealed workflow and education issues. The study aims to identify improvements in clinical decision support (CDS) and improve quality by identifying scenarios where CDS alerts are not accepted due to workflow, education, or staffing issues."
  },
  {
    "objectID": "blog/whats_next_in_ai/index.html#artificial-general-intelligence",
    "href": "blog/whats_next_in_ai/index.html#artificial-general-intelligence",
    "title": "What’s next in AI: A Guide to 2025",
    "section": "Artificial General Intelligence",
    "text": "Artificial General Intelligence\nWhile different people have different definitions of artificial general intelligence (AGI), the most significant aspects of it have already been accomplished by the most recent generation of sophisticated AI large language models, including ChatGPT, LLaMA, Gemini and Claude. One of the key limitations of generative AI, often highlighted by researchers, is what’s known as ‘AI hallucinations.’ While these systems are remarkable in generating coherent and contextually appropriate content, there are instances where their outputs can be misleading or outright inaccurate.\nIt would need human-level comprehension and reasoning to fully resolve hallucinations. Consider this: In order to prevent hallucinations, a system would have to:\nHave the ability to achieve various goals and perform tasks in various contexts and environments.\nCapable of handling problems and situations different from those anticipated by its creators.\nCapable of generalizing knowledge to transfer it across different problems or contexts.\nIn essence, these qualities are what we refer to as “general intelligence”. However, arbitrary general intelligence is not possible due to realistic resource constraints. Real-world systems may display limited generality but are more efficient at learning certain tasks, making them somewhat biased towards certain goals and environments. It is unlikely that humans manifest a maximal level of general intelligence, even in relation to their evolutionarily adapted goals and environments.\nAn AI would have attained human-like comprehension and reasoning skills if it could actually overcome hallucinations. By then, we would have something that was close to artificial general intelligence (AGI), not merely a more accurate language model.\n\nUse Cases:\nSmart Homes: The Whisper model has revolutionized speech recognition, delivering exceptional accuracy in multilingual recognition, translation, and language identification. This allows for more intuitive interactions between users and smart home systems, bridging the gap between human language and technology. VALL-E, a text-to-speech synthesis model, uses semi-supervised data to produce personalized, high-quality speech, enhancing communication quality and adaptability. These advancements bring us closer to AGI-powered smart homes that are natural, tailored, and responsive.\nSmart Healthcare: AGI technology can be applied to wearable health devices, enabling real-time data processing to detect vital signs anomalies. This technology can prompt interventions and improve patient care. A Smart Healthcare System integrates IoT and AGI, analyzing data for continuous monitoring and disease prediction."
  },
  {
    "objectID": "blog/whats_next_in_ai/index.html#quantum-ai",
    "href": "blog/whats_next_in_ai/index.html#quantum-ai",
    "title": "What’s next in AI: A Guide to 2025",
    "section": "Quantum AI",
    "text": "Quantum AI\nQuantum AI summaries a selective approach to leverage artificial intelligence, specifically machine learning techniques, to enhance the entire workflow encompassing quantum algorithm development, experimental design, the identification of near-optimal parameters, the transpilation of quantum circuits, error correction during execution, as well as the calibration and design of quantum devices.\n\nUse Cases:\nDrug Discoveries: Artificial intelligence (AI) combines enhanced methods to make drug discovery and development more cost-effective. It can improve drug approval rates, lower costs, and speed up patient access to medications. Quantum computing (QC) can further benefit drug approval and profitability. Pharma leaders are adopting AI and QC technologies for various stages of drug development. In the next 5–10 years, AI-QC is expected to become standard in the industry. The focus is shifting to scalable solutions addressing larger pharmaceutical issues. Using AI and QC with data analysis will improve understanding of drug processes, streamline experiments, discover new targets, and address pharmaceutical challenges. Developing effective AI-QC strategies can be complex, requiring thorough knowledge of the industry to broaden its applications across the drug life cycle.\nAI researchers intend to focus on developing fundamental technologies that increase the efficiency, interpretability, and adaptability of AI systems. There will be a lot of innovation in the field, ranging from AGI and self-supervised learning to quantum computing and AI ethics. As these technologies become more prevalent in daily life, there will also be a greater focus on the problem of bringing AI systems closer to human values and societal demands."
  },
  {
    "objectID": "blog/whats_next_in_ai/index.html#references",
    "href": "blog/whats_next_in_ai/index.html#references",
    "title": "What’s next in AI: A Guide to 2025",
    "section": "References:",
    "text": "References:\n\nParr, C.S., Lemay, D.G., Owen, C.L., Woodward-Greene, M.J. and Sun, J., 2021. Multimodal AI to improve agriculture. IT Professional, 23(3), pp.53–57.\nSwinckels L, de Keijzer A, Loos B, Applegate R, Kookal K, Kalenderian E, Bijwaard H, Bruers J. A personalized periodontitis risk based on nonimage electronic dental records by machine learning. Journal of Dentistry 2025;153:105469\nErik Pounds. (2024, October 22). What Is Agentic AI?\nOgbu, D., Agentic AI in Computer Vision Domain-Recent Advances and Prospects.\nXu, F., Uszkoreit, H., Du, Y., Fan, W., Zhao, D. and Zhu, J., 2019. Explainable AI: A brief survey on history, research areas, approaches and challenges. In Natural language processing and Chinese computing: 8th cCF international conference, NLPCC 2019, dunhuang, China, October 9–14, 2019, proceedings, part II 8 (pp. 563–574). Springer International Publishing.\nZhang, H. and Ogasawara, K., 2023. Grad-CAM-based explainable artificial intelligence related to medical text processing. Bioengineering, 10(9), p.1070.\nSalih, Ahmed M., et al. A perspective on explainable artificial intelligence methods: SHAP and LIME.Advanced Intelligent Systems (2024): 2400304.\nSiru Liu, Allison B McCoy, Josh F Peterson, Thomas A Lasko, Dean F Sittig, Scott D Nelson, Jennifer Andrews, Lorraine Patterson, Cheryl M Cobb, David Mulherin, Colleen T Morton, Adam Wright, Leveraging explainable artificial intelligence to optimize clinical decision support, Journal of the American Medical Informatics Association,Volume 31, Issue 4, April 2024, Pages 968–974,\nGoertzel, Ben. Artificial general intelligence: concept, state of the art, and future prospects. Journal of Artificial General Intelligence 5.1 (2014): 1.\nDou, Fei, et al. Towards artificial general intelligence (agi) in the internet of things (iot): Opportunities and challenges. arXiv preprint arXiv:2309.07438 (2023).\nWichert, Andreas. Principles of quantum artificial intelligence: quantum problem solving and machine learning. 2020.\nCova, T., Vitorino, C., Ferreira, M., Nunes, S., Rondon-Villarreal, P., Pais, A. (2022). Artificial Intelligence and Quantum Computing as the Next Pharma Disruptors. arXiv preprint arXiv:2309.07438 (2023). In: Heifetz, A. (eds) Artificial Intelligence in Drug Design. Methods in Molecular Biology, vol 2390. Humana, New York, NY.\n\n\nCitation:\nAnushka Dhiman. (Jan 1, 2025). What’s next in AI: A Guide to 2025. SOTA Insights Blog: https://anushkadhiman.github.io/blog/whats_next_in_ai"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Anushka Dhiman, a Computer Vision Engineer and an AI enthusiast interested in the area of AI driven application and researches. I have experience in developing cutting-edge AI systems, including applications in computer vision, natural language processing, and generative AI. I’ve worked on projects such as AI based monitoring and security system in Video Analytics and Survallience.\nMy journey into AI has been fueled by a passion for continuous learning, exploring groundbreaking technologies, and sharing knowledge to empower others in the field."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html",
    "href": "blog/AIResearchHighlightsIn2024/index.html",
    "title": "AI Research Highlights in 2024",
    "section": "",
    "text": "In 2024, the emerging field of artificial intelligence is developing more quickly than anyone could have imagined. It produced innovations that have changed industries, safeguard lives, and gone beyond our wildest expectations. From artificial intelligence (AI) that produces hyper-realistic content to emotionally intelligent machines, AI is advancing beyond our previous understanding of technology and challenging everything we believed to be true.\nLet’s dive into the most significant researches in AI this year, revealing how they are transforming industries and altering modern life, creation of a sentient artificial intelligence."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#sora",
    "href": "blog/AIResearchHighlightsIn2024/index.html#sora",
    "title": "AI Research Highlights in 2024",
    "section": "Sora",
    "text": "Sora\nIn February 2024, OpenAI introduced Sora, a generative AI model that converts text to video. The model has the ability to simulate the real environment and is trained to produce videos of realistic or imaginative scenes based on text instructions. Sora stands out from other video generation models because it can create exceptional videos a maximum of minute long while adhering to user-provided language instructions.\n\n\nMajor contributions\nTurning visual data into patches: Inspired from LLM, create visual patches which an effective representation for generative models of visual data for highly-scalable and effective representation.\nVideo compression network: This network is used to reduces the dimensionality of visual data and outputs a latent representation that is compressed both temporally and spatially. Moreover, train a corresponding decoder model that maps generated latents back to pixel space.\nSpacetime latent patches: Given a compressed input video, a sequence of spacetime patches is extracted which act as transformer tokens.\nScaling transformers for video generation: A diffusion transformer model is trained by given input noisy patches to predict the clean patches.\nLanguage understanding: Re-captioning technique from DALL·E 330 is used to train text-to-video generation systems. The study also uses GPT to convert user prompts into detailed captions.\n\n\nOutcomes\nPrompting with images and videos: Sora employs a variety of picture and video editing techniques, including as extending videos forward or backward in time, animating static images, and producing flawlessly looping videos.\nImage generation capabilities: Sora generates images by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame with variable sizes up to 2048x2048 resolution.\nEmerging simulation capabilities: Video models show emergent capabilities when trained at scale, allowing Sora to simulate physical world aspects without explicit inductive biases for 3D and objects.\n\n\nDiscussion\nSora’s simulator has limitations, including inaccuracies in modelling basic interactions like glass shattering and incorrect object state changes in eating food, and common failure modes include incoherencies and spontaneous appearances."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#phi-3",
    "href": "blog/AIResearchHighlightsIn2024/index.html#phi-3",
    "title": "AI Research Highlights in 2024",
    "section": "Phi-3",
    "text": "Phi-3\nIn April 2024, Microsoft Introduced Phi-3, the most powerful and economical small language models (SLMs) on the market, superior to models of the same size and next size up in a range of language, reasoning, coding, and math benchmarks.\n\nMajor contributions\nEnhanced Efficiency: Phi-3, despite its smaller parameter size of 3.8 billion, performs relatively as good as larger models, resulting in cost savings for deployment and operation in resource-constrained environments.\nVersatile Applications: Phi-3 is versatile, capable of performing tasks like natural language processing, content creation, data analysis, and automation, reducing administrative burdens and enhancing market research and product development.\nMultimodal Capabilities: Phi-3 Vision’s multimodal capabilities enable efficient text and image processing, expanding its application scope across various industries while maintaining high performance and reduced computational cost.\n\n\nOutcomes\nPractical Applications: Phi-3 is utilized by organizations for content generation, data analysis, and customer support, enabling the creation of marketing copy, product descriptions, and chatbots.\nScalability and Flexibility: The Phi-3 family offers various model variants, including Phi-3-mini, Phi-3-small, and upcoming larger models, ensuring scalability and flexibility in performance and resource requirements.\n\n\nDiscussion\nLimited Factual Knowledge: Phi-3, due to its smaller parameter size, struggles with tasks requiring extensive factual knowledge, hindering its effectiveness in applications requiring deep understanding, as seen in benchmarks like TriviaQA.\nComplexity of Optimal Data Mixture: The complexity of selecting the optimal data mixture for training can impact the model’s generalization capabilities and overall effectiveness across various tasks."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#alphafold-3",
    "href": "blog/AIResearchHighlightsIn2024/index.html#alphafold-3",
    "title": "AI Research Highlights in 2024",
    "section": "AlphaFold 3",
    "text": "AlphaFold 3\nIn May 2024, AlphaFold 3, a revolutionary protein folding model, the results show high-accuracy modelling across biomolecular space possibly within a single unified deep-learning network. It was unveiled in 2021 by 2024 Nobel Prize in Chemistry winner Co-founder and CEO of Google DeepMind and Isomorphic Labs Sir Demis Hassabis, and Google DeepMind Director Dr. John Jumper by Demis Hassabis, a groundbreaking AI system that predicts the 3D structure of proteins from their amino acid sequences. Their contributions to science have been widely praised and recognized for their work.\n\nMajor contributions\nSubstantial evolution of the AF2 architecture and training procedure: Achieved by both to accommodate more general chemical structures and to improve the data efficiency of learning.\nImproved evoformer: The system reduces the amount of multiple-sequence alignment (MSA) processing by replacing the AF2 evoformer with the simpler pairformer module.\nSpacetime latent patches: Given a compressed input video, a sequence of spacetime patches is extracted which act as transformer tokens.\nA diffusion-based module: It directly predicts the raw atom coordinates with a diffusion module, replacing the AF2 structure module that operated on amino-acid-specific frames and side-chain torsion angles.\n\n\nOutcomes\nSignificantly higher accuracy for protein-ligand interactions as compared to the most advanced docking.\nMuch better accuracy for protein-nucleic acid interactions than nucleic acid-specific predictors, and significantly better accuracy for antibody-antigen prediction than AlphaFold-Multimer v.2.3.\n\n\nDiscussion\nThe AF3 model demonstrates the ability to accurately predict the structure of various biomolecular systems in a unified framework. Although there are still challenges, it shows strong coverage and generalization for all interactions.\nAF3 demonstrates that deep-learning frameworks can reduce data requirements and enhance data impact. Structural modelling will improve due to deep learning and experimental structure determination improvements."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#veo-and-imagen-3",
    "href": "blog/AIResearchHighlightsIn2024/index.html#veo-and-imagen-3",
    "title": "AI Research Highlights in 2024",
    "section": "Veo and Imagen 3",
    "text": "Veo and Imagen 3\nIn May 2024, Google’s DeepMind introduced Veo and Imagen 3, a most advanced video generation model and highest quality text-to-image model of their research.\nImagen 3 generated image with an input prompt.\n\nMajor contributions\nModel Architecture and Dataset: Veo uses advanced architectures for video generation, focusing on natural language and visual semantics. It generates coherent high-definition videos from text or image prompts. Imagen 3, a refined model, enhances photorealistic image generation by integrating improvements in detail and artifact reduction, likely based on diffusion models. Veo and Imagen 3 are trained on large-scale datasets containing real-world videos and AI-generated imagery with detailed captions, enabling to generate images and videos across various artistic genres and produce more accurate outputs.\n\n\nOutcomes\nFunctionality: Veo is a advanced video generation model that can produce high-definition videos from text or image prompts, supports various cinematic styles, and understands natural language and visual semantics. It can generate coherent video clips from real and AI-generated images, ensuring responsible content use. And, Imagen 3, an upgraded version of Google’s text-to-image generator, offers advanced editing capabilities, mask-based editing, and customization options for creating detailed, lifelike images from simple text prompts. It can produce images across various styles, including photorealism, impressionism, and anime.\n\n\nDiscussion\nVeo has advanced capabilities but struggles with maintaining consistency in complex scenes, leading to discrepancies in detail and realism. Additionally, Veo may overpromise its capabilities, particularly in producing hyper-realistic animations. Imagen 3 faces challenges in achieving realistic outputs due to hallucinations."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#llama-3",
    "href": "blog/AIResearchHighlightsIn2024/index.html#llama-3",
    "title": "AI Research Highlights in 2024",
    "section": "LLAMA 3",
    "text": "LLAMA 3\nIn July 2024, Meta released Llama3, Herd of Models, that natively support multilinguality, coding, reasoning, and tool usage. This largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens.\n\nMajor contributions\nLarger Dataset and Training: The model was trained on a dataset that is seven times larger than that of Llama 2, encompassing over 15 trillion tokens.\nIncreased Context Length: Llama 3 has doubled the context length from 4K to 8K tokens for both its 8 billion (8B) and 70 billion (70B) parameter models. This allows for improved handling of longer documents and complex queries.\nModel architecture: Llama 3 keeps the decoder-only transformer design but adds a number of improvements, like grouped query attention (GQA) and a tokenizer with a 128K token vocabulary, to improve inference performance and efficiency.\nTrust and Safety Features: Meta has integrated new safety tools such as Llama Guard and Code Shield to filter insecure code outputs, emphasizing responsible AI deployment and usage.\n\n\nOutcomes\nPerformance Metrics: Llama 3 performs with impressive output speeds, with the 8 billion parameter model reaching 117.5 tokens per second and a lower latency of 0.34 seconds, enhancing user experience.\nUsability and Flexibility: Llama 3 is highly adaptable and cost-efficient, suitable for various applications like coding and creative writing. Its configurations allow for fine-tuning, making it suitable for high-throughput tasks.\n\n\nDiscussion\nTechnical and Resource Challenges: Llama 3, with its 405 billion parameters, requires significant computational resources, making it impractical for regular users without hardware capabilities. Despite being designed for speed, larger models may experience latency issues and maintain coherence, affecting user experience.\nUsability and Integration Challenges: Llama 3’s current version has limited multimodal capabilities, primarily focusing on text, which restricts its applicability in multimodal environments. Additionally, its English-centric dataset may result in suboptimal multilingual performance, hindering global applications."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#sam-2",
    "href": "blog/AIResearchHighlightsIn2024/index.html#sam-2",
    "title": "AI Research Highlights in 2024",
    "section": "SAM 2",
    "text": "SAM 2\nIn July 2024, Meta announced the Segment Anything Model 2 (SAM 2), the next generation of the Meta Segment Anything Model, supporting object segmentation in videos and images.\nSAM 2 model, a promptable segmentation in images and videos\n\nMajor contributions\nUnified Architecture: SAM 2 introduces a unified model architecture that integrates image and video segmentation, simplifying workflows and allowing users to work seamlessly across various visual media.\nEnhanced Neural Network Design: The model’s refined neural network architecture enhances segmentation, improving accuracy in complex scenes with occlusions or similar colors and textures.\nZero-Shot Learning: SAM 2’s zero-shot learning feature enables it to segment new objects without explicit retraining, making it versatile and adaptable to diverse visual domains.\n\n\nOutcomes\nImproved Performance Metrics: SAM 2 has set new performance benchmarks, enhancing image segmentation accuracy and reducing user interaction time compared to its predecessor, SAM.\nPromptable Segmentation: In video allows users to specify objects of interest through clicks, bounding boxes, or masks, enhancing user interaction and precision in model output.\nConsistency Across Frames: The model ensures consistent object tracking and segmentation across video frames, crucial for high-quality animation and visual effects applications.\n\n\nDiscussion\nComplexity of Video Segmentation: Video segmentation is complex due to varying object motion, occlusion, lighting changes, and lower quality, which can complicate tracking and segmentation, especially in videos.\nUser Interaction Requirements: SAM 2 is designed for visual segmentation, but still requires user interaction for refinement. Automation could improve mask quality verification and error correction."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#claudes-sonnet",
    "href": "blog/AIResearchHighlightsIn2024/index.html#claudes-sonnet",
    "title": "AI Research Highlights in 2024",
    "section": "Claude’s Sonnet",
    "text": "Claude’s Sonnet\nClaude 3.5 Sonnet is a highly proficient model that excels in understanding complex queries and nuanced language, ensuring high-quality, relatable content for a global audience.\n\nMajor contributions\nModel Architecture: Claude 3.5 Sonnet uses a Generative Pre-trained Transformer (GPT) architecture to predict the next word in a sequence of text, enabling effective applications in coding, writing, and visual data interpretation.\nDataset and Training: Claude 3.5 Sonnet, a model trained on vast datasets, enhances natural language understanding by generating human-like text. It supports a large context window of 200,000 tokens and incorporates advanced vision capabilities for image recognition and data visualization.\n\n\nOutcomes\nVisual Processing Capabilities: Claude 3.5 Sonnet excels in image analysis, interpreting visual data with high accuracy, and effectively integrates visual and textual data for comprehensive analyses in industries like retail and finance.\nCoding and Software Development: Claude 3.5 Sonnet enhances coding proficiency and productivity by enabling independent code writing, editing, and execution, while facilitating quick prototyping for app development.\nPerformance Metrics: Claude 3.5 Sonnet has demonstrated superior performance in benchmarks like HumanEval for coding and GPQA for reasoning tasks, showcasing its advanced capabilities.\n\n\nDiscussion\nPerformance Limitations: Claude 3.5 Sonnet, despite improving its language understanding, can misinterpret complex queries and have high error rates in coding tasks, requiring users to debug or refine the code.\nTechnical Challenges: Claude 3.5 Sonnet faces technical challenges, including token limitations and integration issues, affecting processing speed and efficiency for complex tasks and existing workflows."
  },
  {
    "objectID": "blog/AIResearchHighlightsIn2024/index.html#gemini-2",
    "href": "blog/AIResearchHighlightsIn2024/index.html#gemini-2",
    "title": "AI Research Highlights in 2024",
    "section": "Gemini 2",
    "text": "Gemini 2\nGoogle’s most recent AI model was unveiled as a major development in the business’s AI capabilities. It is built for what Google calls the “agentic era,” highlighting the model’s capacity to integrate multimodal functions and carry out activities independently on behalf of users.\n\nMajor contributions\nMultimodal Understanding: Gemini 2.0 is a versatile model that can process and generate various types of data, including text, images, audio, and code, making it more versatile than previous models.\nEnhanced Performance with Low Latency: Gemini 2.0 Flash’s experimental model enhances performance with improved response times and time to first token (TTFT), improving user experience in real-time applications.\nAgentic Capabilities: The design enhances agentic capabilities, allowing the model to comprehend context, follow complex instructions, and make autonomous decisions based on user prompts, making it a crucial proactive AI assistant.\n\n\nOutcomes\nBenchmark Performance: Gemini 2.0 Flash outperforms in language understanding, visual and multimedia understanding, and coding ability, with a score of 70.7% on the MMMU benchmark. However, it shows slightly lower performance in coding tasks.\nAgentic Performance: Gemini 2.0 demonstrated its agentic performance with a 51.8% score on SWE-bench Verified, demonstrating its capability in real-world tasks like software engineering challenges.\nApplications Across Various Fields: Gemini 2.0’s versatility allows its use in various fields such as research, business, education, and creative arts, accelerating scientific discovery, automating tasks, and improving decision-making.\n\n\nDiscussion\nGemini 2.0 has faced criticism for generating biased answers and historically inaccurate images, leading to backlash and calls for improvements in performance and accuracy. It also struggles with providing clear, accurate responses to complex topics, potentially causing user frustration and diminishing trust in its capabilities.\nAs we approach 2025, the development of AI presents both exciting opportunities and significant risks. While focusing optimistically on how AI could transform industries, improve decision-making, and solve global problems. Ethicists emphasize responsible AI development, balancing innovation with oversight, to serve humanity’s best interests. The trajectory of AI depends on navigating competing visions and developing technology and governance structures."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWhat’s next in AI: A Guide to 2025\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nAnushka Dhiman\n\n\n\n\n\n\n\n\n\n\n\n\nAI Research Highlights in 2024\n\n\n\n\n\n\n\n\nDec 25, 2024\n\n\nAnushka Dhiman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html",
    "title": "Permutation and Combination",
    "section": "",
    "text": "The study of permutations and combinations is concerned with determining the number of different ways of arranging and selecting objects out of a given number of objects, without actually listing them. There are some basic counting techniques which will be useful in determining the number of different ways of arranging or selecting objects. The two basic counting principles are given below:"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#overview",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#overview",
    "title": "Permutation and Combination",
    "section": "",
    "text": "The study of permutations and combinations is concerned with determining the number of different ways of arranging and selecting objects out of a given number of objects, without actually listing them. There are some basic counting techniques which will be useful in determining the number of different ways of arranging or selecting objects. The two basic counting principles are given below:"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#fundamental-principle-of-counting",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#fundamental-principle-of-counting",
    "title": "Permutation and Combination",
    "section": "Fundamental principle of counting",
    "text": "Fundamental principle of counting\n\nMultiplication principle\nSuppose an event E can occur in m different ways and associated with each way of occurring of E, another event F can occur in n different ways, then the total number of occurrence of the two events in the given order is m × n .\nFor example, if you roll a six-sided dice and a two sided coin, then there are 6 × 2 = 12 possible outcomes.\nTherefore, this principle can be use in everyday life when you need to calculate the number of possibilities that exist when combining choices.\n\n# The total number of possible combinations (dice, coin) can be calculated using the fundamental principle of counting.\n\n# Number of choices for dice and coin\ndice_choices = 6 # 1, 2, 3, 4, 5, 6\ncoin_choices = 2  # H, T\n\n# Total combinations using the Multiplication Principle of Counting\ntotal_combinations = dice_choices * coin_choices\n\nprint(f\"Total number of possible combinations: {total_combinations}\")\n\nTotal number of possible combinations: 12\n\n\n\n\nAddition principle\nIf an event E can occur in m ways and another event F can occur in n ways, and suppose that both can not occur together, then E or F can occur in m + n ways.\n\n# The total number of possible combinations (dice, coin) can be calculated using addition principle of counting.\n\n# Number of choices for dice and coin\ndice_choices = 6 # 1, 2, 3, 4, 5, 6\ncoin_choices = 2  # H, T\n\n# Total combinations using the Addition Principle of Counting\ntotal_combinations = dice_choices + coin_choices\n\nprint(f\"Total number of possible combinations: {total_combinations}\")\n\nTotal number of possible combinations: 8"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#permutations",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#permutations",
    "title": "Permutation and Combination",
    "section": "Permutations",
    "text": "Permutations\nA permutation is an arrangement of objects in a definite order.\n\nPermutation of n different objects:\nThe number of permutations of n objects taken all at a time, denoted by the symbol \\(^nP_n\\) , is given by\nwhere \\({n!}\\) = n(n – 1) (n – 2) … 3.2.1, read as factorial n, or n factorial.\nThere are basically two types of permutation:\n\n\nWhen repetition of objects is allowed\nIn this case, you can finds all possible arrangements of a set of objects. And here, the order of the objects is important.\nThe number of permutations of n things taken all at a time, when repetion of objects is allowed is \\(n^n\\).\nThe number of permutations of n objects, taken r at a time, when repetition of objects is allowed, is \\(n^r\\).\n\n\nWhen repetition of objects is not allowed\nIn this case, you cannot select the same item multiple times while arranging a set of items. Essentially, each item can only be used once in the arrangement, meaning the number of available choices decreases with each selection made.\nThe number of permutations of n objects taken r at a time, where 0 &lt; r ≤ n, denoted by \\(^nP_r\\) , is given by\n\\[ ^nP_r = \\frac{n!}{(n-r)!} \\]\nwhere:\n- \\(n\\) is the number of elements in the set.\n- \\(r\\) is the number of elements taken together.\n\n\nPermutations when the objects are not distinct: they are identical\nIn this case, when objects are identical, many arrangements will appear the same, so you need to divide by the number of ways to arrange the identical objects among themselves.\nThe number of permutations of n objects of which r1 are of one kind, r2 are of second kind, …, rk are of kth kind and the rest if any, are of different kinds is\n\\[ \\frac{n!}{(r1!*r2!*r3!..rk)!} \\]"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#combinations",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#combinations",
    "title": "Permutation and Combination",
    "section": "Combinations",
    "text": "Combinations\nThis method is used to calculate the total number of outcomes when order doesn’t matter. Essentially, we are not interested in arranging rather in selecting r objects from given n objects.\nThere are also two types of combinations:\n\nCombinations without Repetition\nIn this case, you can only select each item once when choosing a group from a set. For instance, picking lottery numbers, where each number drawn cannot be repeated.\nThe formula for calculating the number of combinations from \\(n\\) elements taken \\(r\\) at a time is given by:\n\\[^n C_r = \\frac{n!}{(n-r)! r!}\\]\n\n\nCombinations with Repetition\nIn this case, when selecting items from a set, you can choose the same item multiple times. For instance, selecting flavors for ice cream cones, where you could choose two scoops of chocolate.\nIf we choose a set of r items from n types of items, where repetition is allowed and the number items we are choosing from is essentially unlimited, the number of selections possible is given by:\n\\[^n C_r = \\frac{(r+n-1)!}{(n-1)! r!}\\]\nLet’s explore some examples to better understand these concepts."
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#permutation-and-combination-in-python",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#permutation-and-combination-in-python",
    "title": "Permutation and Combination",
    "section": "Permutation and Combination in Python",
    "text": "Permutation and Combination in Python\nThese methods can be found in itertools package.\n\nPermutation\nWe can use itertools module to generate permutations of objects efficiently. Here’s how to use itertools for the three types of permutations that we’ve discussed lately:\n1. Permutations of 𝑛 Different Objects (No Repetition)\nFor a set of 𝑛 distinct objects, itertools.permutations() can generate all possible permutations of those objects. This is for the case where you want to arrange all n different objects without repetition.\n\nimport itertools\n\n# List of distinct objects\nobjects = ['A', 'B', 'C']\n\n# Generate all permutations of the objects\npermutations = list(itertools.permutations(objects))\n\n# Output the permutations and the total number of permutations\nprint(\"Permutations of distinct objects:\", permutations)\nprint(\"Total number of permutations:\", len(permutations))\n\nPermutations of distinct objects: [('A', 'B', 'C'), ('A', 'C', 'B'), ('B', 'A', 'C'), ('B', 'C', 'A'), ('C', 'A', 'B'), ('C', 'B', 'A')]\nTotal number of permutations: 6\n\n\n2. Permutations When Repetition of Objects is Allowed\nIn this case, itertools.product() can be used to simulate the scenario where repetition is allowed. The result will generate all possible combinations where the same object can appear multiple times.\n\n# Generate all permutations of length 2, allowing repetition\npermutations_with_repetition = list(itertools.product(objects, repeat=2))\n\n# Output the permutations and the total number of permutations\nprint(\"Permutations with repetition allowed:\", permutations_with_repetition)\nprint(\"Total number of permutations:\", len(permutations_with_repetition))\n\nPermutations with repetition allowed: [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]\nTotal number of permutations: 9\n\n\nIf you want to generate permutations for selecting and arranging a specific number of elements from a set.\nPermutaion taking 2 elements together from a set of 3 elements:\n\\[ ^3P_2 = \\frac{3!}{(3-2)!} = \\frac{3!}{1!} = \\frac{6}{1} = 6 \\]\nPermutation taking 3 elements together from a set of 3 elements:\n\\[ ^3P_3 = \\frac{3!}{(3-3)!} = \\frac{3!}{0!} = \\frac{6}{0} = 6 \\]\n\nchar_set = ['A', 'B', 'C'] # define a charater set of 3 elements\n\npermutations1 = list(itertools.permutations(char_set, 2)) # permutations taking two elements\npermutations2 = list(itertools.permutations(char_set, 3)) # permutations taking three elements\n\n# Output the permutations taking two elements and taking three elements\nprint(\"Permutations of 2 elements from 3 elements:\", permutations1)\nprint(\"Permutations of 3 elements from 3 elements:\", permutations2)\n\nPermutations of 2 elements from 3 elements: [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]\nPermutations of 3 elements from 3 elements: [('A', 'B', 'C'), ('A', 'C', 'B'), ('B', 'A', 'C'), ('B', 'C', 'A'), ('C', 'A', 'B'), ('C', 'B', 'A')]\n\n\n3. Permutations When the Objects are Not Distinct\nFor permutations with non-distinct objects, itertools.permutations will still generate all permutations, but you need to account for repeated objects manually by removing duplicates after generating the permutations.\n\nimport itertools\n\n# List of objects, some of which are repeated\nobjects = ['A', 'A', 'B']\n\n# Generate all permutations and remove duplicates by converting to a set\ndistinct_permutations = set(itertools.permutations(objects))\n\n# Output the distinct permutations and the total number of distinct permutations\nprint(\"Distinct permutations of non-distinct objects:\", distinct_permutations)\nprint(\"Total number of distinct permutations:\", len(distinct_permutations))\n\nDistinct permutations of non-distinct objects: {('B', 'A', 'A'), ('A', 'B', 'A'), ('A', 'A', 'B')}\nTotal number of distinct permutations: 3\n\n\nNow, let’s take a look at combinations\nCombinations without Repetition\nCombination taking 2 elements together from a set of 3 elements:\n\\[ ^3C_2 = \\frac{3!}{(3-2)!2!} = \\frac{3!}{1!*2!} = \\frac{6}{2} = 3 \\]\nCombination taking 3 elements together from a set of 3 elements:\n\\[ ^3C_3 = \\frac{3!}{(3-3)!3!} = \\frac{3!}{0!*3!} = \\frac{6}{6} = 1 \\]\n\n# import combinations using itertools package\nfrom itertools import combinations \n\ncombination1 = list(combinations(char_set, 2)) # combination taking two elements\n\n# Output the combination taking two elements and the total number of combinations\nprint(\"Combinations of 2 elements from 3 elements:\", combination1)\nprint(\"Total number of combinations of 2 elements from 3 elements:\", len(combination1))\n\nCombinations of 2 elements from 3 elements: [('A', 'B'), ('A', 'C'), ('B', 'C')]\nTotal number of combinations of 2 elements from 3 elements: 3\n\n\n\ncombination2 = list(combinations(char_set, 3)) # combination taking three elements\n\n# Output the combination taking three elements and the total number of combinations\nprint(\"Combinations of 2 elements from 3 elements:\", combination2)\nprint(\"Total number of combinations of 3 elements from 3 elements:\", len(combination2))\n\nCombinations of 2 elements from 3 elements: [('A', 'B', 'C')]\nTotal number of combinations of 3 elements from 3 elements: 1\n\n\nCombinations with Repetiton\nThis function generates all possible combinations of \\(r\\) elements from a given iterable, allowing elements to be selected multiple times. It’s useful when repetitions are allowed in the selection process.\nCombination taking 2 elements together from a set of 3 elements:\n\\[ ^3C_2 = \\frac{(2+3-1)!}{(3-1)!*2!} = \\frac{4!}{2!*2!} = \\frac{24}{2*2} = 6 \\]\nCombination taking 3 elements together from a set of 3 elements:\n\\[ ^3C_3 = \\frac{(3+3-1)!}{(3-1)!*3!} = \\frac{5!}{2!*3!} = \\frac{120}{2*6} = 10 \\]\n\nfrom itertools import combinations_with_replacement \n\ncombination_with_replacement1 = list(combinations_with_replacement(char_set, 2)) #combination taking two elements with replacement\n\n# Output the combination taking three elements and the total number of combinations\nprint(\"Combinations of 2 elements from 3 elements:\", combination_with_replacement1)\nprint(\"Total number of combinations of 2 elements from 3 elements:\", len(combination_with_replacement1))\n\nCombinations of 2 elements from 3 elements: [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]\nTotal number of combinations of 3 elements from 3 elements: 6\n\n\n\ncombination_with_replacement2 = list(combinations_with_replacement(char_set, 3)) #combination taking three elements with replacement\n\n# Output the combination taking three elements and the total number of combinations\nprint(\"Combinations of 3 elements from 3 elements:\", combination_with_replacement2)\nprint(\"Total number of combinations of 3 elements from 3 elements:\", len(combination_with_replacement2))\n\nCombinations of 3 elements from 3 elements: [('A', 'A', 'A'), ('A', 'A', 'B'), ('A', 'A', 'C'), ('A', 'B', 'B'), ('A', 'B', 'C'), ('A', 'C', 'C'), ('B', 'B', 'B'), ('B', 'B', 'C'), ('B', 'C', 'C'), ('C', 'C', 'C')]\nTotal number of combinations of 3 elements from 3 elements: 10"
  },
  {
    "objectID": "insights/StatisticsandProbability/01. Permutation and Combinations.html#conclusion",
    "href": "insights/StatisticsandProbability/01. Permutation and Combinations.html#conclusion",
    "title": "Permutation and Combination",
    "section": "Conclusion",
    "text": "Conclusion\nThis article explores combinatorics, specifically permutations and combinations, implementing it using python. It provides mathematical theory and python implementation in solving broader problems.\nPermutations and Combinations have been crucial in fields ranging from cryptography to the optimization of AI algorithms.\nFor instance, in machine learning is feature selection in model training.\n\nPermutations: It can be used to generate different sets of features to test their performance, optimizing the model’s accuracy.\nCombinations: It can be used to determine the number of ways features can be selected from a larger set, helping in reducing model complexity and improving interpretability.\n\nI’d love any feedback you may have. Feel free to reach out and follow SOTA Insights for more such articles!"
  }
]